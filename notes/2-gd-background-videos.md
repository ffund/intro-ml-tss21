---
title: Background - videos on convexity, directional derivatives, gradient descent	
author: Fraida Fund
---

Last week, we said that one of the "ingredients" in the "recipe" for supervised machine learning is a *training algorithm*, which we use to find model parameters that minimize the loss function. This is fundamentally an *optimization* problem, and in our discussion we will assume some background knowledge of the foundational math concepts for optimization.

If you have not encountered these concepts before - convexity, directional derivatives, gradient descent - please watch these short videos before our lecture (note: subtitles are available). Ask any questions you have on our Q&A site.

### [Objective function: convexity](https://www.youtube.com/watch?v=LJAaNR1PbsQ)

<iframe width="640" height="385" src="https://www.youtube.com/embed/LJAaNR1PbsQ" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

### [Objective function: differentiability, the first order](https://www.youtube.com/watch?v=a4v2cKn8pvw)

<iframe width="640" height="385" src="https://www.youtube.com/embed/a4v2cKn8pvw" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

### [Objective function: gradient and descent](https://www.youtube.com/watch?v=fJj0UCQBbHc)

<iframe width="640" height="385" src="https://www.youtube.com/embed/fJj0UCQBbHc" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>