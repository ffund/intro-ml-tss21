<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Fraida Fund" />
  <title>Simple linear regression - extended derivation of OLS solution</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <link rel="stylesheet" href="../style/pandoc.css" />
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Simple linear regression - extended derivation of OLS solution</h1>
<p class="author">Fraida Fund</p>
</header>
<h2 id="set-up">Set up</h2>
<p>We assume a linear model</p>
<p><span class="math display">\[\hat{y_i} = w_0 + w_1 x_i\]</span></p>
<p>Given the (convex) loss function</p>
<p><span class="math display">\[ MSE(w_0, w_1) = \frac{1}{n} \sum_{i=1}^n [y_i - (w_0 + w_1 x_i) ]^2 \]</span></p>
<p>to find the minimum, we take the derivative and set it equal to zero:</p>
<p><span class="math display">\[ \frac{\partial MSE}{\partial w_0} = 0, \frac{\partial MSE}{\partial w_1} = 0\]</span></p>
<h2 id="solution-for-intercept-w_0">Solution for intercept <span class="math inline">\(w_0\)</span></h2>
<p>First, let’s solve for the intercept <span class="math inline">\(w_0\)</span>. Using the chain rule, power rule:</p>
<p><span class="math display">\[ 
\frac{\partial MSE}{\partial w_0} = \frac{1}{n}\sum_{i=1}^n  (2)[y_i - (w_0 + w_1 x_i) ] (-1) = -\frac{2}{n} \sum_{i=1}^n [y_i - (w_0 + w_1 x_i)] 
\]</span></p>
<p>(We can then drop the constant factor when we set this expression equal to <span class="math inline">\(0\)</span>.)</p>
<p>Then, setting <span class="math inline">\(\frac{\partial MSE}{\partial w_0}=0\)</span> is equivalent to setting the sum of residuals to zero:</p>
<p><span class="math display">\[ \sum_{i=1}^n e_i  = 0\]</span></p>
<p>(where <span class="math inline">\(e_i\)</span> is the residual term for sample <span class="math inline">\(i\)</span>).</p>
<h2 id="solution-for-slope-w_1">Solution for slope <span class="math inline">\(w_1\)</span></h2>
<p>Next, we work on the slope:</p>
<p><span class="math display">\[ \frac{\partial MSE}{\partial w_1} = \frac{1}{n}\sum_{i=1}^n  2[y_i - (w_0 + w_1 x_i)](-x_i)\]</span></p>
<p><span class="math display">\[  \implies -\frac{2}{n} \sum_{i=1}^n x_i [y_i - (w_0 + w_1 x_i)]  = 0\]</span></p>
<p>Again, we can drop the constant factor. Then, this is equivalent to:</p>
<p><span class="math display">\[  \sum_{i=1}^n x_i e_i  = 0\]</span></p>
<p>(where <span class="math inline">\(e_i\)</span> is the residual term for sample <span class="math inline">\(i\)</span>).</p>
<h2 id="solving-two-equations-for-two-unknowns">Solving two equations for two unknowns</h2>
<p>From setting the <span class="math inline">\(\frac{\partial MSE}{\partial w_0}=0\)</span> and <span class="math inline">\(\frac{\partial MSE}{\partial w_1}=0\)</span> we end up with two equations involving the residuals:</p>
<p><span class="math display">\[  \sum_{i=1}^n e_i  = 0,  \sum_{i=1}^n x_i e_i  = 0\]</span></p>
<p>where</p>
<p><span class="math display">\[ e_i = y_i - (w_0 + w_1 x_i) \]</span></p>
<p>We can expand <span class="math inline">\(\sum_{i=1}^n e_i = 0\)</span> into</p>
<p><span class="math display">\[  \sum_{i=1}^n  y_i = n w_0 + \sum_{i=1}^n x_i w_1 \]</span></p>
<p>then divide by <span class="math inline">\(n\)</span>, and we find the intercept</p>
<p><span class="math display">\[w_0 = \frac{1}{n} \sum_{i=1}^n y_i - w_1 \frac{1}{n} \sum_{i=1}^n x_i \]</span></p>
<p>i.e.</p>
<p><span class="math display">\[w_0^* = \bar{y} - w_1 \bar{x}\]</span></p>
<p>where <span class="math inline">\(\bar{x}, \bar{y}\)</span> are the sample means of <span class="math inline">\(x, y\)</span>.</p>
<p>To solve for <span class="math inline">\(w_1\)</span>, expand <span class="math inline">\(\sum_{i=1}^n x_i e_i = 0\)</span> into</p>
<p><span class="math display">\[  \sum_{i=1}^n x_i y_i = \sum_{i=1}^n  x_i w_0 + \sum_{i=1}^n x_i^2 w_1 \]</span></p>
<p>and multiply by <span class="math inline">\(n\)</span>.</p>
<p><span class="math display">\[  n \sum_{i=1}^n x_i y_i = n \sum_{i=1}^n  x_i w_0 + n \sum_{i=1}^n x_i^2 w_1 \]</span></p>
<p>Also, multiply the “expanded” version of <span class="math inline">\(\sum_{i=1}^n e_i = 0\)</span>,</p>
<p><span class="math display">\[  \sum_{i=1}^n  y_i = n w_0 + \sum_{i=1}^n x_i w_1 \]</span></p>
<p>by <span class="math inline">\(\sum x_i\)</span>, to get</p>
<p><span class="math display">\[  \sum_{i=1}^n x_i \sum_{i=1}^n  y_i = n \sum_{i=1}^n x_i w_0 + (\sum_{i=1}^n x_i)^2 w_1 \]</span></p>
<p>Now, we can subtract to get</p>
<p><span class="math display">\[  n \sum_{i=1}^n x_i y_i - \sum_{i=1}^n x_i \sum_{i=1}^n  y_i = n \sum_{i=1}^n x_i^2 w_1  - (\sum_{i=1}^n x_i)^2 w_1 \]</span></p>
<p><span class="math display">\[ = w_1 \left( n \sum_{i=1}^n x_i^2 - (\sum_{i=1}^n x_i)^2 \right) \]</span></p>
<p>and solve for <span class="math inline">\(w_1^*\)</span>:</p>
<p><span class="math display">\[ w_1^*  = \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y}) }{\sum_{i=1}^n (x_i - \bar{x})^2}\]</span></p>
</body>
</html>
