<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="author" content="Fraida Fund">
  <title>Ensemble methods</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="reveal.js-master/dist/reset.css">
  <link rel="stylesheet" href="reveal.js-master/dist/reveal.css">
  <style>
    .reveal .sourceCode {  /* see #7635 */
      overflow: visible;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="reveal.js-master/dist/theme/white.css" id="theme">
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <h1 class="title">Ensemble methods</h1>
  <p class="author">Fraida Fund</p>
</section>

<section class="slide level3">

<aside class="notes">
<p><strong>Math prerequisites for this lecture</strong>: You should know
about:</p>
<ul>
<li>Variance of a random variable</li>
<li>Independence of random variables</li>
<li>Variance of sum of random variables</li>
</ul>
</aside>
</section>
<section>
<section id="ensemble-methods" class="title-slide slide level2">
<h2>Ensemble methods</h2>

</section>
<section id="recap-decision-trees" class="slide level3">
<h3>Recap: decision trees</h3>
<ul>
<li>Let trees grow deep - low bias, high variance</li>
<li>Don’t let trees get deep: low variance, high bias</li>
</ul>
</section>
<section id="ensemble-methods---the-idea" class="slide level3">
<h3>Ensemble methods - the idea</h3>
<p>Combine multiple <strong>weak learners</strong> - having either high
bias or high variance - to create an <strong>ensemble</strong> with
better prediction</p>
</section>
<section id="ensemble-methods---types-1" class="slide level3">
<h3>Ensemble methods - types (1)</h3>
<ul>
<li>Combine multiple learners with high <strong>variance</strong> in a
way that reduces their variance</li>
<li>Combine multiple learners with high <strong>bias</strong> in a way
that reduces their bias</li>
</ul>
</section>
<section id="ensemble-methods---types-2" class="slide level3">
<h3>Ensemble methods - types (2)</h3>
<ul>
<li><strong>Parallel</strong>: build base estimators
<em>independently</em> and then average their predictions. Combined
estimator is usually better than any single base estimator because its
<em>variance</em> is reduced.</li>
<li><strong>Sequential</strong>: (boosting) build base estimators
<em>sequentially</em> and each one tries to reduce the <em>bias</em> of
the combined estimator.</li>
</ul>
</section></section>
<section>
<section id="bagging" class="title-slide slide level2">
<h2>Bagging</h2>

</section>
<section id="bagging---background" class="slide level3">
<h3>Bagging - background</h3>
<ul>
<li>Designed for, and most often applied to, decision trees</li>
<li>Name comes from <strong>bootstrap aggregation</strong></li>
</ul>
</section>
<section id="bootstrapping" class="slide level3">
<h3>Bootstrapping</h3>
<ul>
<li>Basic idea: Sampling <strong>with replacement</strong></li>
<li>Each “bootstrap training set” is <em>same size</em> as full training
set, and is created by sampling with replacement</li>
<li>Some samples will appear more than once, some samples not at
all</li>
</ul>
</section>
<section id="bootstrap-aggregation" class="slide level3">
<h3>Bootstrap aggregation</h3>
<ul>
<li>Create multiple versions <span class="math inline">\(1, \ldots,
B\)</span> of training set with bootstrap</li>
<li>Independently train a model on each bootstrap training set:
calculate <span class="math inline">\(\hat{f}_1(x) \ldots,
\hat{f}_B(x)\)</span></li>
<li>Combine output of models by voting (classification) or averaging
(regression):</li>
</ul>
<p><span class="math display">\[\hat{f}_{bag}(x) = \frac{1}{B}
\sum_{b=1}^B \hat{f}_b (x)\]</span></p>
</section>
<section id="bagging-trees" class="slide level3">
<h3>Bagging trees</h3>
<ul>
<li>Construct <span class="math inline">\(B\)</span> trees using <span
class="math inline">\(B\)</span> bootstrapped training sets</li>
<li>Let the trees grow deep, no pruning</li>
<li>Each individual tree has low bias, high variance</li>
<li>Average the prediction of the trees to reduce variance (if
independent!)</li>
</ul>
</section>
<section id="variance-reduction-rule" class="slide level3">
<h3>Variance reduction rule</h3>
<p><span class="math display">\[\begin{align}
\mathrm{Var}(\bar X)
&amp;= \mathrm{Var}\!\left(\frac{1}{n}\sum_{i=1}^n X_i\right) \\
&amp;= \frac{1}{n^2}\,\mathrm{Var}\!\left(\sum_{i=1}^n X_i\right) \\
&amp;= \frac{1}{n^2}\left(\sum_{i=1}^n \mathrm{Var}(X_i) +
2\sum_{i&lt;j}\mathrm{Cov}(X_i,X_j)\right) \\
&amp;= \frac{1}{n^2}\cdot n  \mathrm{Var}(X_i) \quad \text{(if $X_i$
i.i.d.)} \\
&amp;= \frac{1}{n}\,\mathrm{Var}(X_i).
\end{align}\]</span></p>
<aside class="notes">
<p>where:</p>
<ol type="1">
<li>uses definition of the sample mean: <span class="math inline">\(\bar
X = \tfrac{1}{n}\sum_{i=1}^n X_i\)</span>.</li>
<li>uses the scaling rule: <span class="math inline">\(\mathrm{Var}(aY)
= a^2\,\mathrm{Var}(Y)\)</span> with <span
class="math inline">\(a=\tfrac{1}{n}\)</span>.</li>
<li>uses variance of sum (+ symmetry of covariance): <span
class="math display">\[\mathrm{Var}(\sum_i X_i)= \sum_i
\mathrm{Var}(X_i)+  \sum_{\substack{j=1\\ j\neq i}}
\mathrm{Cov}(X_i,X_j) = \sum_i
\mathrm{Var}(X_i)+2\sum_{i&lt;j}\mathrm{Cov}(X_i,X_j)\]</span></li>
<li>because independence <span class="math inline">\(\Rightarrow
\mathrm{Cov}(X_i,X_j)=0\)</span> for <span class="math inline">\(i\neq
j\)</span>, and iid means <span
class="math inline">\(\mathrm{Var}(X_i)\)</span> is same for all <span
class="math inline">\(i\)</span>.</li>
</ol>
</aside>
</section>
<section id="variance-reduction-rule-general" class="slide level3">
<h3>Variance reduction rule (general)</h3>
<p>For <span class="math inline">\(n\)</span> i.i.d. random variables,
the variance of their mean decreases with <span
class="math inline">\(n\)</span>:</p>
<table>
<colgroup>
<col style="width: 44%" />
<col style="width: 55%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th><strong>General RV</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>The average</td>
<td><span class="math inline">\(\bar{X} = \frac{1}{n}\sum_{i=1}^n
X_i\)</span></td>
</tr>
<tr class="even">
<td>Variance with independence</td>
<td><span class="math inline">\(\mathrm{Var}(\bar{X}) =
\frac{1}{n}\mathrm{Var}(X_i)\)</span></td>
</tr>
</tbody>
</table>
</section>
<section id="variance-reduction-rule-bagged-trees" class="slide level3">
<h3>Variance reduction rule (bagged trees)</h3>
<p>For <span class="math inline">\(B\)</span>
<strong>independent</strong> bagged trees, variance decreases with <span
class="math inline">\(B\)</span>:</p>
<table>
<colgroup>
<col style="width: 44%" />
<col style="width: 55%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th><strong>Bagged Trees</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>The average</td>
<td><span class="math inline">\(\hat{f}_{bag}(x) =
\frac{1}{B}\sum_{b=1}^B \hat{f}_b(x)\)</span></td>
</tr>
<tr class="even">
<td>Variance with independence</td>
<td><span class="math inline">\(\mathrm{Var}(\hat{f}_{bag}(x)) =
\frac{1}{B}\mathrm{Var}(\hat{f}_b(x))\)</span></td>
</tr>
</tbody>
</table>
<p>…but, they are not really independent!</p>
</section>
<section id="correlated-trees" class="slide level3">
<h3>Correlated trees</h3>
<p>Problem: trees produced by bagging are highly correlated, and:</p>
<p><span class="math display">\[
\mathrm{Var}(\bar{X}) = \frac{\sigma^2}{n}\left[1 + (n-1)\rho\right]
\]</span></p>
<p>where <span class="math inline">\(\rho = \mathrm{Corr}(X_i, X_j)
= \frac{\mathrm{Cov}(X_i,
X_j)}{\sqrt{\mathrm{Var}(X_i)\,\mathrm{Var}(X_j)}}\)</span></p>
<aside class="notes">
<ul>
<li>Imagine there is one feature that is strong predictor, several
moderate predictors</li>
<li>Most/all trees will split on this feature</li>
<li>Averaging correlated quantities does not reduce variance as
much.</li>
</ul>
</aside>
</section>
<section id="random-forests" class="slide level3">
<h3>Random forests</h3>
<p>Grow many decorrelated trees:</p>
<ul>
<li><strong>Bootstrap</strong>: grow each tree with bootstrap resampled
data set.</li>
<li><strong>Split-variable randomization</strong>: Force each split to
consider <em>only</em> a subset of <span
class="math inline">\(m\)</span> of the <span
class="math inline">\(p\)</span> predictors.</li>
</ul>
<p>Typically <span class="math inline">\(m = \frac{p}{3}\)</span> but
this should be considered a tuning parameter.</p>
</section>
<section id="bagged-trees-illustration" class="slide level3">
<h3>Bagged trees illustration</h3>
<figure>
<img data-src="../images/5-ensemble-trees.png" style="width:90.0%"
alt="Identical data, bootstrapped data, and bootstrapped data with split variable randomization." />
<figcaption aria-hidden="true">Identical data, bootstrapped data, and
bootstrapped data with split variable randomization.</figcaption>
</figure>
</section>
<section id="a-note-on-computation" class="slide level3">
<h3>A note on computation</h3>
<ul>
<li>Bagged trees and random forests can be fitted in parallel on many
cores!</li>
<li>Each tree is built independently of the others</li>
</ul>
</section></section>
<section>
<section id="boosting" class="title-slide slide level2">
<h2>Boosting</h2>

</section>
<section id="boosting---training" class="slide level3">
<h3>Boosting - training</h3>
<p><strong>Iteratively</strong> build a succession of models:</p>
<ul>
<li>Train a weak model. Typically a very shallow tree.</li>
<li>In training set for <span class="math inline">\(b\)</span>th model,
focus on errors made by <span class="math inline">\(b-1\)</span>th
model.</li>
<li>Use (weighted) model output</li>
<li>Reduces bias <em>and</em> variance!</li>
</ul>
</section>
<section id="adaboost-adaptive-boosting" class="slide level3">
<h3>AdaBoost (Adaptive Boosting)</h3>
<p>Adjust <em>weights</em> so that each successive model focuses on more
“difficult” samples.</p>
<p>Consider a binary classification problem, where <span
class="math inline">\(y_i\in \{-1, +1\} \forall i\)</span>.</p>
</section>
<section id="adaboost-algorithm" class="slide level3">
<h3>AdaBoost algorithm</h3>
<ol type="1">
<li>Let <span class="math inline">\(w_i = \frac{1}{N}\)</span> for all
<span class="math inline">\(i\)</span> in training set.</li>
<li>For <span class="math inline">\(m=1,\ldots,M\)</span>, repeat:</li>
</ol>
</section>
<section id="adaboost-algorithm-inner-loop" class="slide level3">
<h3>AdaBoost algorithm (inner loop)</h3>
<ul>
<li><p>Fit weak learner <span class="math inline">\(\hat{f}^m\)</span>,
on training data with sample weights <span
class="math inline">\(w_i\)</span>.</p></li>
<li><p>Compute weighted error <span
class="math inline">\(err_m\)</span>:</p>
<p><span class="math display">\[err_m = \frac{\sum_{i=1}^N w_i 1(y_i
\neq \hat{f}^m(x_i))}{\sum_{i=1}^N w_i}\]</span></p></li>
<li><p>Compute coefficient <span class="math inline">\(\alpha_m = \log
\left( \frac{1-err_m}{err_m} \right)\)</span></p></li>
<li><p>Update weights: <span class="math inline">\(w_i \leftarrow w_i
e^{\alpha_m 1(y_i \neq \hat{f}^m(x_i))}\)</span> (for misclassified
samples, scale weight by <span
class="math inline">\(e^{\alpha_m}\)</span>)</p></li>
</ul>
</section>
<section id="adaboost-algorithm-final-step" class="slide level3">
<h3>AdaBoost algorithm (final step)</h3>
<ol start="3" type="1">
<li><p>Output final ensemble model:</p>
<p><span class="math display">\[f_M(x) = \sum_{m=1}^{M} \alpha_m
\hat{f}^{m}(x),  \quad  \hat{y}(x) =
\mathrm{sign}[f_M(x)]\]</span></p></li>
</ol>
<!-- 

### Boosting - algorithm for regression tree (1)

1. Let $\hat{f}(x)=0$ and $r_i = y_i$ for all $i$ in training set.
2. For $b=1,\ldots,B$, repeat:


::: notes

The idea is: we fit trees to the residuals, not the outcome $y$. 

:::


### Boosting - algorithm for regression tree (inner loop)

* Fit a tree $\hat{f}^b$ with $d$ splits ($d+1$ leaf nodes) on training data $(X,r)$.
* Update $\hat{f}$ with a *shrunken* version of new tree:
  $$\hat{f}(x) \leftarrow \hat{f}(x) + \lambda \hat{f}^b(x)$$
* Update residuals:
  $$r_i \leftarrow r_i - \lambda \hat{f}^b(x)$$


### Boosting - algorithm for regression tree (final step)

3. Output boosted model:
$$\hat{f}(x) = \sum_{b=1}^B \lambda \hat{f}^b(x)$$

### Boosting - algorithm for regression tree (tuning)

Tuning parameters to select by CV:

* Number of trees $B$
* Shrinkage parameter $\lambda$, controls *learning rate*
* $d$, number of splits in each tree. ( $d=1 \rightarrow$ tree is called a *stump* )

-->
</section>
<section id="gradient-boosting" class="slide level3">
<h3>Gradient Boosting</h3>
<ul>
<li><p>General goal of boosting: find the model at each stage that
minimizes loss function on ensemble (computationally
difficult!)</p></li>
<li><p>AdaBoost interpretation (discovered years later): Gradient
descent algorithm that minimizes exponential loss function.</p></li>
<li><p>Gradient boosting: works for any differentiable loss function. At
each stage, find the local gradient of loss function, and take steps in
direction of steepest descent.</p></li>
</ul>
<aside class="notes">
<p>Gradient boosting can be viewed as <em>functional gradient
descent</em>. We have a differentiable loss function</p>
<p><span class="math display">\[L(y, f(x)),\]</span></p>
<p>(exponential loss function, in the case of AdaBoost), and an additive
model:</p>
<p><span class="math display">\[f_M(x) = \sum_{m=1}^{M} \alpha_m
\hat{f}^m(x),\]</span></p>
<p>where each weak learner <span
class="math inline">\(\hat{f}^m(x)\)</span> acts like a basis
function.</p>
<p>Each iteration performs an update:</p>
<p><span class="math display">\[f_m(x) = f_{m-1}(x) + \alpha_m
\hat{f}^m(x).\]</span></p>
<p>where at each step:</p>
<ul>
<li><span class="math inline">\(\hat{f}^m(x)\)</span> - the weak learner
- is chosen to align with the direction of the negative gradient of the
loss,</li>
<li><span class="math inline">\(\alpha_m\)</span> - the coefficient -
determines the step size in that direction.</li>
</ul>
<!--
---

In AdaBoost, we aim to minimize the exponential loss function:

$$L(f) = \sum_{i=1}^N \exp(-y_i f(x_i))$$

The derivative of the loss with respect to the model output is:

$$\frac{\partial L}{\partial f(x_i)} = -y_i \exp(-y_i f_{m-1}(x_i))$$

Therefore, the negative gradient is

$$r_i = y_i \exp(-y_i f_{m-1}(x_i))$$

Minimizing the weighted classification error is equivalent to choosing $\hat{f}^m(x)$ that is most correlated with the negative gradient direction:

$$\hat{f}^m(x) \approx \operatorname{sign}(r_i)$$

The next weak learner $\hat{f}^m(x)$ is chosen to minimize the weighted classification error:

$$err_m = \frac{\sum_i w_i \mathbf{1}[y_i \neq h_m(x_i)]}{\sum_i w_i}$$

where the sample weights are **proportional to the magnitude of the gradient term** in $r_i$.:

$$w_i = \exp(-y_i f_{m-1}(x_i))$$


The inner product:

$$\sum_i r_i\, h_m(x_i)$$

represents the alignment between the weak learner's predictions and the negative gradient of the loss, and our $\hat{f}^m(x)$ maximizes this.

In summary: Selecting (from among candidate weak learners) the one that minimizes the weighted error is **equivalent** to selecting the learner that best aligns with the negative gradient of the exponential loss function.

-->
</aside>
</section></section>
<section id="summary-of-selected-ensemble-methods"
class="title-slide slide level2">
<h2>Summary of (selected) ensemble methods</h2>
<ul>
<li>Can use a single estimator that has poor performance</li>
<li>Combining the output of multiple estimators into a single
prediction: better predictive accuracy, less interpretability</li>
<li>Also more expensive to fit</li>
</ul>
</section>
    </div>
  </div>

  <script src="reveal.js-master/dist/reveal.js"></script>

  <!-- reveal.js plugins -->
  <script src="reveal.js-master/plugin/notes/notes.js"></script>
  <script src="reveal.js-master/plugin/search/search.js"></script>
  <script src="reveal.js-master/plugin/zoom/zoom.js"></script>
  <script src="reveal.js-master/plugin/math/math.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
        // Display controls in the bottom right corner
        controls: true,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: true,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'bottom-right',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: false,

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: false,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: true,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'default',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: true,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'slide',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'fade',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        math: {
          mathjax: '/usr/share/javascript/mathjax/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [
          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    </body>
</html>
