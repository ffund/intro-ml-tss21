<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="author" content="Fraida Fund">
  <title>Working with Data</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="reveal.js-master/dist/reset.css">
  <link rel="stylesheet" href="reveal.js-master/dist/reveal.css">
  <style>
    .reveal .sourceCode {  /* see #7635 */
      overflow: visible;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="reveal.js-master/dist/theme/white.css" id="theme">
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <h1 class="title">Working with Data</h1>
  <p class="author">Fraida Fund</p>
</section>

<section id="garbage-in-garbage-out" class="title-slide slide level2">
<h2>Garbage in, garbage out</h2>
<aside class="notes">
<p>Any machine learning project has to start with ‚Äúgood‚Äù‚Äù data.</p>
<p>There is a ‚Äúgarbage in, garbage out‚Äù rule: If you use ‚Äúgarbage‚Äù to
train a machine learning model, you will only get ‚Äúgarbage‚Äù out.</p>
<p>And: Since you are evaluating on the same data, you might not even
realize it is ‚Äúgarbage‚Äù at first! You may not realize until the model is
already deployed in production! This is the absolute worst case
scenario, as illustrated below‚Ä¶</p>
</aside>
</section>

<section id="model-training-vs-evaluation-vs-deployment-1"
class="title-slide slide level2">
<h2>Model training vs evaluation vs deployment (1)</h2>
<figure>
<img data-src="../images/1-lifecycle-data.png" style="width:60.0%"
alt="The lifecycle of an ML model" />
<figcaption aria-hidden="true">The lifecycle of an ML model</figcaption>
</figure>
<aside class="notes">
<p>We want to understand how the model will behave in
<em>deployment</em> as early as possible (before investing too much
time, effort, money in a model that won‚Äôt do well).</p>
</aside>
</section>

<section id="model-training-vs-evaluation-vs-deployment-2"
class="title-slide slide level2">
<h2>Model training vs evaluation vs deployment (2)</h2>
<table>
<colgroup>
<col style="width: 27%" />
<col style="width: 30%" />
<col style="width: 30%" />
<col style="width: 12%" />
</colgroup>
<thead>
<tr class="header">
<th>Training Accuracy</th>
<th>Evaluation Accuracy</th>
<th>Deployment Accuracy</th>
<th>Outcome</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0.95</td>
<td>0.93</td>
<td>0.91</td>
<td>ü§©</td>
</tr>
<tr class="even">
<td>0.55</td>
<td>0.52</td>
<td>N/A</td>
<td>üòê</td>
</tr>
<tr class="odd">
<td>0.95</td>
<td>0.93</td>
<td>0.51</td>
<td>üò±</td>
</tr>
</tbody>
</table>
<aside class="notes">
<ul>
<li>Best case: Model does well in evaluation, and deployment</li>
<li>Second best case: Model does poorly in evaluation, is not
deployed</li>
<li>Worst case: Model does well in evaluation, poorly in deployment
(‚Äúoverly optimistic evaluation‚Äù)</li>
</ul>
</aside>
</section>

<section id="working-with-data-two-stages"
class="title-slide slide level2">
<h2>Working with data: two stages</h2>
<ol type="1">
<li>Can I use this data?</li>
<li>How should I process this data?</li>
</ol>
</section>

<section>
<section id="stage-1-can-i-use-this-data"
class="title-slide slide level2">
<h2>Stage 1: Can I use this data?</h2>

</section>
<section id="ethical-and-legal-concerns" class="slide level3">
<h3>Ethical and legal concerns</h3>
<ul>
<li>Bias/fairness</li>
<li>Privacy</li>
<li>Consent</li>
<li>Copyright</li>
</ul>
<aside class="notes">
<p>Some examples of data ethics failures:</p>
<ul>
<li>You are going to train a machine learning model to classify
photographs of skin lesions as ‚Äúcancer‚Äù or ‚Äúnot cancer‚Äù. You have a
dataset of photographs with diagnoses from a hospital in a primarily
white neighborhood. Your model is likely to underperform on photographs
of darker skin tones because of the selection bias in the data.</li>
<li>You train a model to allocate health care resources based on a
historical dataset of health care use. But because some groups
historically used less health care due to lack of access or
socioeconomic factors, despite being equally or more ill, the model
underestimates their needs. This data has a social/historical bias,
which you are perpetuating in the model.</li>
<li>You train a model to identify ‚Äúoffensive‚Äù posts on social media. You
are using a dataset of real posts that were labeled as ‚Äúoffensive‚Äù or
‚Äúnot offensive‚Äù by a team of human annotators. But the human annotators
were more likely to label language from African American English as
offensive, so your model disproportionately removes posts by Black
users.</li>
<li>To generate a large dataset for model training, you extract
text-image pairs from a public web archive. However, this web archive
includes materials that should never have been online, such as private
medical records, which end up in the dataset and raise serious privacy
risks.</li>
<li>You create an account on an online dating platform, which allows you
to view and scrape other users‚Äô profiles. However, this data includes
deeply personal details - like sexual preferences - which users did not
consent to have extracted, aggregated, or redistributed.</li>
<li>To train a generative image model, you scrape millions of images
from the public web. In response to some prompts, your model outputs
original training images - which may be copyrighted artworks or contain
trademarks - almost exactly.</li>
</ul>
<p>Citations/further reading:</p>
<ul>
<li>Marin Benƒçeviƒá, Marija Habijan, Irena Galiƒá, Danilo Babin,
Aleksandra Pi≈æurica, Understanding skin color bias in deep
learning-based skin lesion segmentation, Computer Methods and Programs
in Biomedicine, Volume 245, 2024, <a
href="https://doi.org/10.1016/j.cmpb.2024.108044">https://doi.org/10.1016/j.cmpb.2024.108044</a></li>
<li>Ziad Obermeyer, Brian Powers, Christine Vogeli, Sendhil
Mullainathan, Dissecting racial bias in an algorithm used to manage the
health of populations, Science, Volume 366, Issue 6464, 2019,
pp.¬†447‚Äì453, <a
href="https://doi.org/10.1126/science.aax2342">https://doi.org/10.1126/science.aax2342</a></li>
<li>Shirin Ghaffary, The algorithms that detect hate speech online are
biased against black people, Vox, August 15, 2019, <a
href="https://www.vox.com/recode/2019/8/15/20806384/social-media-hate-speech-bias-black-african-american-facebook-twitter">https://www.vox.com/recode/2019/8/15/20806384/social-media-hate-speech-bias-black-african-american-facebook-twitter</a></li>
<li>Maarten Sap, Dallas Card, Saadia Gabriel, Yejin Choi, Noah A. Smith,
The Risk of Racial Bias in Hate Speech Detection, Proceedings of the
57th Annual Meeting of the Association for Computational Linguistics,
2019, pages 1668‚Äì1678, <a
href="https://doi.org/10.18653/v1/P19-1163">https://doi.org/10.18653/v1/P19-1163</a></li>
<li>Benj Edwards, Artist finds private medical record photos in popular
AI training data set, Ars Technica, September 21, 2022, <a
href="https://arstechnica.com/information-technology/2022/09/artist-finds-private-medical-record-photos-in-popular-ai-training-data-set/">https://arstechnica.com/information-technology/2022/09/artist-finds-private-medical-record-photos-in-popular-ai-training-data-set/</a></li>
<li>Joseph Cox, ‚Äú70,000 OkCupid Users Just Had Their Data Published,‚Äù
Vice, May 12, 2016. <a
href="https://www.vice.com/en/article/70000-okcupid-users-just-had-their-data-published/">https://www.vice.com/en/article/70000-okcupid-users-just-had-their-data-published/</a>
<!-- * Timothy B. Lee, Stable Diffusion copyright lawsuits could be a legal earthquake for AI, Ars Technica, April 3, 2023, [https://arstechnica.com/tech-policy/2023/04/stable-diffusion-copyright-lawsuits-could-be-a-legal-earthquake-for-ai/](https://arstechnica.com/tech-policy/2023/04/stable-diffusion-copyright-lawsuits-could-be-a-legal-earthquake-for-ai/) --></li>
</ul>
<p><strong>Takeaway</strong>: Consider possible sources of bias in the
data you are using. Be aware of how the data was collected and its
provenance/lineage. Ensure that appropriate consent was obtained, and
verify that the data was not collected in violation of service terms,
privacy expectations, or copyright protections.</p>
</aside>
</section>
<section id="representativeness-concerns" class="slide level3">
<h3>Representativeness concerns</h3>
<p>Compared to where your model will be used, is the data similar
in:</p>
<ul>
<li>Population</li>
<li>Context/setting</li>
<li>Time period</li>
<li>Label balance</li>
</ul>
<aside class="notes">
<p>Some examples of representativeness failures:</p>
<ul>
<li>You train a cardiovascular risk prediction model on data from
middle-aged adults in high-income countries. When the same model is
applied to younger populations in low- and middle-income countries, it
systematically underestimated risk, because disease patterns, diet, and
access to care differed significantly.</li>
<li>You train a model to identify crop disease based on a dataset of
labeled plant photographs. When applied to real photographs ‚Äúin the
wild‚Äù, your model fails:</li>
</ul>
<figure>
<img data-src="../images/PlantDoc_Examples.png" style="width:70.0%"
alt="When the context is not representative." />
<figcaption aria-hidden="true">When the context is not
representative.</figcaption>
</figure>
<ul>
<li>You train a malware detection model using samples of malware and
non-malicious software collected over a six year period. Initially, it
performs very well. However, as you deploy it years later, the model‚Äôs
accuracy declines significantly because new malware variants - evolving
beyond the original training distribution - are not accurately
detected.</li>
<li>You want to train a model to detect AI-generated text, for potential
integration into learning platforms like Brightspace. To create a
labeled data set, you grab a sample of Wikipedia articles (for
human-written samples), and then you prompt a LLM to generate a similar
number Wikipedia-style articles on a wide range of topics. Your model
‚Äúlearns‚Äù that text samples are AI-generated approximately half the time,
even though the true prevalence of AI-generated text might be much lower
- or much higher - in real student work.</li>
</ul>
<p>Citations/further reading:</p>
<ul>
<li>Ralph B. D‚ÄôAgostino, Scott Grundy, Lisa M. Sullivan, Peter Wilson,
Validation of the Framingham Coronary Heart Disease prediction scores:
results of a multiple ethnic groups investigation, JAMA, Volume 286,
Issue 2, 2001, pp.¬†180‚Äì187, <a
href="https://jamanetwork.com/journals/jama/fullarticle/193997">https://jamanetwork.com/journals/jama/fullarticle/193997</a></li>
<li>Davinder Singh, Naman Jain, Pranjali Jain, Pratik Kayal, Sudhakar
Kumawat, Nipun Batra, PlantDoc: A Dataset for Visual Plant Disease
Detection, Proceedings of the 7th ACM IKDD CoDS and 25th COMAD (CoDS
COMAD 2020), pp.¬†249‚Äì253, 2020, <a
href="https://doi.org/10.1145/3371158.3371196">https://doi.org/10.1145/3371158.3371196</a></li>
<li>C. Galen, R. Steele, Evaluating Performance Maintenance and
Deterioration Over Time of Machine Learning-based Malware Detection
Models on the EMBER PE Dataset, 2020 Seventh International Conference on
Social Networks Analysis, Management and Security (SNAMS), Paris,
France, 2020, pp.¬†1‚Äì7, <a
href="https://doi.org/10.1109/SNAMS52053.2020.9336538">https://doi.org/10.1109/SNAMS52053.2020.9336538</a></li>
</ul>
<p><strong>Takeaway</strong>: Ensure that your dataset reflects the
real-world conditions where your model will be deployed. Check whether
the population, context, time period, and class balance in the data
match the deployment setting, and be careful when combining data from
multiple sources into what‚Äôs known as a ‚ÄúFrankenstein‚Äù dataset.</p>
<!-- 
* **Data is not representative of your target situation**. For example, you are training a model to predict the spread of infectious disease for a NYC-based health startup, but you are using data from another country.
* **Data or situation changes over time**. For example, imagine you train a machine learning model to classify loan applications. However, if the economy changes, applicants that were previously considered credit-worthy might not be anymore despite having the same income, as the lender becomes more risk-averse. Similarly, if wages increase across the board, the income standard for a loan would increase.
-->
</aside>
</section>
<section id="predictive-feature-concerns" class="slide level3">
<h3>Predictive feature concerns</h3>
<aside class="notes">
<p>Some examples of predictive feature failures:</p>
<ul>
<li>You train a model to predict criminality based on facial features.
There is nothing inherently predictive in facial structure. Later, it
turns out that the apparent signal comes from differences in image
sources (mugshots vs.¬†ID photos).</li>
<li>You attempt to build an ‚ÄúAI lie detector‚Äù that classifies deception
based on micro-expressions in facial video. Human lie detection is
barely above chance, and there are no reliable facial markers of
deception, so the dataset has no true predictive features to learn
from.</li>
<li>You train a model to classify people as entrepreneurs from facial
images scraped from Crunchbase. While the model achieves high reported
accuracy, there is no causal or biologically plausible relationship
between facial morphology and occupational choices. The model exploits
dataset artifacts.</li>
</ul>
<p>Citations/further reading - many more examples in:</p>
<ul>
<li>Mel Andrews, Andrew Smart, Abeba Birhane, The reanimation of
pseudoscience in machine learning and its ethical repercussions, Nature
Machine Intelligence, Volume 5, Issue 9, 2024, Article 101027, <a
href="https://doi.org/10.1016/j.patter.2024.101027">https://doi.org/10.1016/j.patter.2024.101027</a></li>
</ul>
<p><strong>Takeaway</strong>: A model cannot succeed if the dataset
lacks true predictive features. (Be careful - the model may seem to be
predictive only because of a data leakage problem, like some of the
examples noted above!)</p>
</aside>
</section>
<section id="other-data-quality-concerns" class="slide level3">
<h3>Other data quality concerns</h3>
<ul>
<li>Label error</li>
<li>Other data entry error</li>
<li>Inconsistent units/formats</li>
<li>Missing data</li>
</ul>
<aside class="notes">
<p>Some examples of data quality failures:</p>
<ul>
<li>You train a COVID-19 diagnostic model using a chest X-ray dataset
that was compiled from case studies published in the early weeks of the
pandemic. Later reviews find that some samples are mislabeled, and are
actually other types of pneumonia, especially samples that were reported
before a reliable diagnostic test for COVID-19 was widely
available.</li>
<li>You train a ride-hailing demand prediction model on GPS data from
mobile phones. Some trips contain corrupted coordinates (e.g., location
recorded in the middle of the ocean, or a misplaced decimal point
showing a car moving 3,000 km in a minute). The model treats these
entries as real trips, introducing noise and spurious patterns.</li>
<li>You combine clinical datasets from multiple hospitals, but one site
records lab results in mg/dL while another uses mmol/L. Your model makes
nonsensical predictions.<br />
</li>
<li>You train a predictive model using historical returns of currently
active mutual funds. Since funds that failed or were closed are missing
from the data, the model learns on only successful funds, so it
overestimates expected returns. (This is known as survivorship
bias.)</li>
</ul>
<p>Citations/further reading:</p>
<ul>
<li>Curtis G. Northcutt. Pervasive Label Errors in ML Datasets
Destabilize Benchmarks. Blog post, March 29, 2021. <a
href="https://l7.curtisnorthcutt.com/label-errors">https://l7.curtisnorthcutt.com/label-errors</a></li>
<li>Curtis G. Northcutt, Anish Athalye, Jonas Mueller. Pervasive Label
Errors in Test Sets Destabilize Machine Learning Benchmarks. In Advances
in Neural Information Processing Systems (NeurIPS 2021). <a
href="https://neurips.cc/virtual/2021/47102">https://neurips.cc/virtual/2021/47102</a></li>
<li>Brown, S. J., Goetzmann, W. N., Ibbotson, R. G., &amp; Ross, S. A.
Survivorship bias in performance studies. The Review of Financial
Studies, 5(4), 553‚Äì580 (1992). <a
href="https://doi.org/10.1093/rfs/5.4.553">https://doi.org/10.1093/rfs/5.4.553</a></li>
</ul>
<p><strong>Takeaway</strong>: Problems in the data can undermine the
predictive ability of the models trained on it.</p>
</aside>
</section>
<section id="should-i-use-the-data-illustration" class="slide level3">
<h3>Should I use the data? illustration</h3>
<aside class="notes">
<figure>
<img data-src="../images/1-working-with-data-flowchart.png"
style="width:60.0%" alt="Flowchart for determining suitability." />
<figcaption aria-hidden="true">Flowchart for determining
suitability.</figcaption>
</figure>
</aside>
</section></section>
<section>
<section id="stage-2-how-do-i-process-the-data"
class="title-slide slide level2">
<h2>Stage 2: How do I process the data?</h2>
<ul>
<li>Select features, target</li>
<li>Split data (avoid data leakage)</li>
<li>Convert to numeric types</li>
<li>Create ‚Äútransformed‚Äù features</li>
<li>Explore data, make and check assumptions</li>
<li>Handle missing data</li>
</ul>
</section>
<section id="select-features" class="slide level3">
<h3>Select features</h3>
<p>Include features if they are:</p>
<ul>
<li>Plausibly predictive</li>
<li>Available at inference time</li>
<li>Don‚Äôt have other data leakage issues</li>
</ul>
<aside class="notes">
<p>We must <em>exclude</em> fields in the data that have no plausible
relationship to the target variable:</p>
<ul>
<li>A machine learning model will find ‚Äúpatterns‚Äù even if the feature
data is not really related to the target variable! It will find
‚Äúspurious‚Äù relationships. That can potentially be much worse than if
there was no ML model at all.</li>
<li>In many cases, there will be fields in the data that you shouldn‚Äôt
use, e.g.¬†you won‚Äôt use someone‚Äôs phone number or home address to
predict their Intro ML course grade.</li>
<li>Always exclude fields that are purely identifiers (e.g.¬†a numeric
identifier for a sample, for a respondent).</li>
</ul>
<p>We also must <em>exclude</em> fields in the data that are not going
to be available to our model when it is deployed for ‚Äúreal‚Äù:</p>
<ul>
<li>Example: You build a model to help decide which applicants should be
approved for loans. In the training dataset, you accidentally include
fields such as ‚Äúnumber of late payments,‚Äù which are only known after the
loan has already been issued. By training on them, the model looks
impressively accurate during evaluation, but in deployment those
features are unavailable, so the model cannot make real-world
predictions.</li>
</ul>
<p>Data leakage: TBD</p>
</aside>
</section>
<section id="select-target" class="slide level3">
<h3>Select target</h3>
<p>Target should be:</p>
<ul>
<li>measureable</li>
<li>available</li>
<li>correct</li>
</ul>
<aside class="notes">
<p>If the exact thing we want to predict is measurable and available to
us in the data, it will be a <em>direct</em> target variable. Sometimes,
however, the thing we want to predict is not measurable or available. In
this case, we may need to use a <em>proxy</em> variable that <em>is</em>
measurable and available, and is closely related to the thing we want to
predict. (The results will only be as good as the relationship between
the thing we want to predict, and the proxy!)</p>
<p>Example: You want to predict a patient‚Äôs illness severity in order to
allocate extra care resources. But severity itself is not directly
measurable in your dataset. Instead, you use past healthcare costs as a
proxy variable, assuming that sicker patients generate higher costs. (In
practice, this systematically underestimates the needs of disadvantaged
groups who historically have had less access to care. The model appears
accurate against the proxy, but fails against the true target.)</p>
<p>Citation: Ziad Obermeyer, Brian Powers, Christine Vogeli, Sendhil
Mullainathan, Dissecting racial bias in an algorithm used to manage the
health of populations, Science, Volume 366, Issue 6464, 2019,
pp.¬†447‚Äì453, <a
href="https://doi.org/10.1126/science.aax2342">https://doi.org/10.1126/science.aax2342</a></p>
<p>We mentioned label error when we talked about data quality. Since it
is expensive to get labeled data, it‚Äôs not uncommon for labels to be
either machine-generated, or added by humans who spend very little time
on each sample.</p>
</aside>
</section>
<section id="splitting-data" class="slide level3">
<h3>Splitting data</h3>
<aside class="notes">
<p>If we split data incorrectly, we can introduce <em>data leakage</em>,
which causes an <em>overly optimistic evaluation</em> - we said earlier
how important it is to avoid that!</p>
<p>Data leakage happens when information that would not be available in
the real deployment setting ‚Äúleaks‚Äù into the training or evaluation
process, and makes the evaluation much ‚Äúeasier‚Äù than the real deployment
task.</p>
</aside>
</section>
<section id="types-of-data-leakage-1" class="slide level3">
<h3>Types of data leakage (1)</h3>
<p>No independent test set:</p>
<ul>
<li>no test set at all!</li>
<li>random split of non-independent samples</li>
<li>pre-processing uses entire data</li>
<li>model selection uses test set</li>
</ul>
<aside class="notes">
<p>If there is no test set, if the test set is not really independent of
the training set, or if the test set becomes ‚Äúcontaminated‚Äù by using it
during model development, then the evaluation task (‚Äúmake predictions on
data that isn‚Äôt really new!‚Äù) is easier than the real deployment task
(‚Äúmake predictions on actually new data‚Äù).</p>
<p>To mitigate this, we split the data into training and test sets, and
<strong>don‚Äôt look at the test set again</strong> until final model
evaluation. But, a random split only makes an independent test set if
the samples are independent. It doesn‚Äôt work if:</p>
<ul>
<li>there are duplicates in the data</li>
<li>there are multiple samples from the same respondent</li>
<li>there is some kind of temporal relationship</li>
<li>etc.</li>
</ul>
<p>so in these cases, we need to split the data in a way that preserves
the independence of the test set. (We‚Äôll revisit in Week 4.)</p>
<p>And, if we use the test data - for pre-processing, for model
selection, or for model training - we ‚Äúcontaminate‚Äù it and then we no
longer have an independent test set for model evaluation</p>
<p>See <a
href="https://www.cell.com/patterns/pdfExtended/S2666-3899(23)00159-9">Leakage
and the reproducibility crisis in machine learning-based
science</a>.</p>
<p>Example: Suppose you are training a model to predict bicycle traffic
on the Brooklyn Bridge. Your dataset includes features such as ‚Äútime of
day‚Äù (in 15-minute intervals), ‚Äúday of week‚Äù, ‚Äútemperature‚Äù,
‚Äúprecipitation‚Äù, etc.</p>
<ul>
<li>In model development and evaluation, you split the data randomly
into training and test sets at the row level. That means the training
set might include traffic counts for 7:00-7:15 AM and 7:30-7:45 AM on
June 10th, while the test set includes 7:15-7:30 on the same data.
Because the test rows are sandwiched between training rows, the model
evaluation appears very accurate ‚Äî it‚Äôs essentially interpolating
between adjacent points.</li>
<li>When deployed to predict traffic on unseen future days, this
advantage disappears, and performance is much worse than the evaluation
would suggest.</li>
</ul>
</aside>
</section>
<section id="types-of-data-leakage-2" class="slide level3">
<h3>Types of data leakage (2)</h3>
<p>Inappropriate features</p>
<ul>
<li>feature not available at inference time</li>
<li>feature is a proxy for target variable in data, but not in
deployment</li>
</ul>
<aside class="notes">
<p>Example: Suppose you are training a model to predict whether a
patient has hypertension. You are using a dataset of patient medical
info, using ‚Äúhas a hypertension diagnosis‚Äù as the target variable on
which to train the model.</p>
<ul>
<li>In model development and evaluation, you use ‚Äúcurrent medications‚Äù
as a feature. For patients who have already been diagnosed with
hypertension, this may include drugs to lower blood pressure!</li>
<li>When the model is deployed, it will have to make predictions given
medical history <em>before a hypertension diagnosis is made</em>.</li>
</ul>
<p>Example: Suppose you are training a model to predict whether a
profile photo on LinkedIn is AI-generated. You prepare a dataset with a
bunch of AI-generated photos of headshots, and ‚Äúreal‚Äù faces cropped from
candid photographs found online.</p>
<ul>
<li>In model development and evaluation, the model may ‚Äúlearn‚Äù that a
headshot-style photo is AI-generated, and a cropped candid photo is not.
It will appear to have high accuracy on the evaluation data.</li>
<li>When the model is deployed, this relationship between ‚Äúphoto style‚Äù
and target variable will not exist, since virtually all of the photos
will be headshot-style. The model will have very poor performance.</li>
</ul>
<p>(Knowing which features are valid often requires domain
knowledge‚Ä¶)</p>
<p>For example‚Ä¶.</p>
</aside>
</section>
<section id="covid-19-chest-radiography" class="slide level3">
<h3>COVID-19 chest radiography</h3>
<ul>
<li><strong>Problem</strong>: diagnose COVID-19 from chest radiography
images</li>
<li><strong>Input</strong>: image of chest X-ray (or other
radiography)</li>
<li><strong>Target variable</strong>: COVID or no COVID</li>
</ul>
</section>
<section id="covid-19-chest-radiography-2" class="slide level3">
<h3>COVID-19 chest radiography (2)</h3>
<figure>
<img data-src="../images/1-covid-xrays.png" style="width:60.0%"
alt="Neural networks can classify the source dataset of these chest X-ray images, even without lungs! Source" />
<figcaption aria-hidden="true">Neural networks can classify the source
dataset of these chest X-ray images, even <em>without lungs</em>! <a
href="https://arxiv.org/abs/2004.12823">Source</a></figcaption>
</figure>
<aside class="notes">
<p>Between January and October 2020, more than 2000 papers were
published that claimed to use machine learning to diagnose COVID-19
patients based on chest X-rays or other radiography. But a later <a
href="https://www.nature.com/articles/s42256-021-00307-0">review</a>
found that ‚Äúnone of the models identified are of potential clinical use
due to methodological flaws and/or underlying biases‚Äù.</p>
<p>To train these models, people used an emerging COVID-19 chest X-ray
dataset, along with one or more existing chest X-ray dataset, for
example a pre-existing dataset used to try and classify viral
vs.¬†bacterial pneumonia.</p>
<p>The problem is that the chest X-rays for each dataset were so
‚Äúdistinctive‚Äù to that dataset, that a neural network could be trained
with high accuracy to classify an image into its source dataset, even
without the lungs showing!</p>
</aside>
</section>
<section id="covid-19-chest-radiography-2-1" class="slide level3">
<h3>COVID-19 chest radiography (2)</h3>
<p>Findings:</p>
<ul>
<li>some non-COVID datasets were pediatric images, COVID images were
adult</li>
<li>there were dataset-level differences in patient positioning</li>
<li>many COVID images came from screenshots of published papers, which
often had text, arrows, or other annotations over the images. (Some
non-COVID images did, too.)</li>
</ul>
</section>
<section id="covid-19-chest-radiography-3" class="slide level3">
<h3>COVID-19 chest radiography (3)</h3>
<figure>
<img data-src="../images/1-covid-xrays-saliency.png" style="width:90.0%"
alt="Saliency map showing the ‚Äúimportant‚Äù pixels for classification. Source" />
<figcaption aria-hidden="true">Saliency map showing the ‚Äúimportant‚Äù
pixels for classification. <a
href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7523163/">Source</a></figcaption>
</figure>
<aside class="notes">
<p>These findings are based on techniques like</p>
<ul>
<li>saliency maps, where the model is made to highlight the part of the
image (the pixels) that it considered most relevant to its
decision.</li>
<li>using generative models and asking it to take a COVID-negative X-ray
and make it positive (or v.v.)</li>
</ul>
<p>Many of the findings are not easy to understand without domain
knowledge (e.g.¬†knowing what part of the X-ray <em>should</em> be
important and what part should not be.) For example: should the
diaphragm area be helpful?</p>
</aside>
<!-- 


https://www.nature.com/articles/s41559-023-02162-1  Data leakage jeopardizes ecological applications of machine learning

https://www.nature.com/articles/s41467-024-46150-w Data leakage inflates prediction performance in connectome-based machine learning models

https://www.nature.com/articles/s41592-024-02362-y Guiding questions to avoid data leakage in biological machine learning applications


-->
</section>
<section id="signs-of-potential-data-leakage-after-training"
class="slide level3">
<h3>Signs of potential data leakage (after training)</h3>
<ul>
<li>Performance is ‚Äútoo good to be true‚Äù</li>
<li>Unexpected behavior of model (e.g.¬†learns from a feature that
shouldn‚Äôt help)</li>
</ul>
</section>
<section id="detecting-data-leakage" class="slide level3">
<h3>Detecting data leakage</h3>
<ul>
<li>Exploratory data analysis</li>
<li>Study the data before, during, and after you use it!</li>
<li>Explainable ML methods</li>
<li>Early testing in production</li>
</ul>
<!-- 
‚Ä¢ Convert to numeric types
‚Ä¢ Create ‚Äútransformed‚Äù features
‚Ä¢ Explore data, make and check assumptions
‚Ä¢ Handle missing data

-->
</section>
<section id="convert-to-numeric-types" class="slide level3">
<h3>Convert to numeric types</h3>
<ul>
<li>fix ‚Äúreading in the data‚Äù issues</li>
<li>ordinal and one-hot encoding of categorical data</li>
<li>image data to raw pixels</li>
<li>text to ‚Äúbag of words‚Äù or other representation</li>
<li>audio to frequency domain (or image of frequency domain)
features</li>
</ul>
</section>
<section id="create-transformed-features" class="slide level3">
<h3>Create ‚Äútransformed‚Äù features</h3>
</section>
<section id="make-and-check-assumptions" class="slide level3">
<h3>Make and check assumptions</h3>
<aside class="notes">
<p>It‚Äôs always a good idea to ‚Äúsanity check‚Äù your data - before you look
at it, think about what you expect to see. Then check to make sure your
expectations are realized.</p>
<p>Look at plots of data, summary statistics, etc. and consider general
trends.</p>
</aside>
</section>
<section id="example-author-citation-data-1" class="slide level3">
<h3>Example: author citation data (1)</h3>
<p>Data analysis: use PubMed, and identify the year of first publication
for the 100,000 most cited authors.</p>
<aside class="notes">
<p>What are our expectations about what this should look like?</p>
</aside>
</section>
<section id="example-author-citation-data-2" class="slide level3">
<h3>Example: author citation data (2)</h3>
<figure>
<img data-src="../images/1-pubmed-authors.png" style="width:50.0%"
alt="Does this look reasonable?" />
<figcaption aria-hidden="true">Does this look reasonable?</figcaption>
</figure>
<aside class="notes">
<p>We can think of many potential explanations for this pattern, even
though it is actually a data artifact.</p>
<p>The true explanation: in 2002, PubMed started using full first names
in authors instead of just initials. The same author is represented in
the dataset as a ‚Äúnew‚Äù author with a first date of publication in
2002.</p>
</aside>
</section>
<section id="example-author-citation-data-3" class="slide level3">
<h3>Example: author citation data (3)</h3>
<figure>
<img data-src="../images/1-pubmed-authors2.png" style="width:50.0%"
alt="The real distribution, after name unification. Example via Steven Skiena @ Stony Brook U." />
<figcaption aria-hidden="true">The real distribution, after name
unification. Example via <a
href="https://www3.cs.stonybrook.edu/~skiena/519/">Steven Skiena @ Stony
Brook U</a>.</figcaption>
</figure>
<aside class="notes">
<p>How <em>should</em> you handle unreasonable values, data that does
not match expectations, or ‚Äúoutliers‚Äù? It depends!</p>
<ul>
<li>e.g.¬†suppose in a dataset of voter information, some have impossible
year of birth - would make the voter over 120 years old. (The reason:
Voters with no known DOB, who registered before DOB was required, are
often encoded with a January 1900 DOB.)</li>
<li><strong>not</strong> a good idea to just remove outliers unless you
are sure they are a data entry error or otherwise not a ‚Äútrue‚Äù
value.</li>
<li>Even if an outlier is due to some sort of error, if you remove them,
you may skew the dataset (as in the 1/1/1900 voters example).</li>
</ul>
<p>Consider the possibility of:</p>
<ul>
<li>Different units, time zones, etc. in different rows</li>
<li>Same value represented several different ways (e.g.¬†names,
dates)</li>
<li>Missing data encoded as zero</li>
</ul>
</aside>
</section>
<section id="handle-missing-data" class="slide level3">
<h3>Handle missing data</h3>
<p>Missing data can appear as:</p>
<ul>
<li>Rows that have <code>NaN</code> values</li>
<li>Rows that have other values encoding ‚Äúmissing‚Äù (-1, 0, 100‚Ä¶)</li>
<li>Rows that are <em>not there</em> but should be</li>
</ul>
<aside class="notes">
<!-- 
* Example: NYC taxi tip data, NYS thruway data
-->
<p>Some practical examples of ‚Äúrows that should be there, but are not‚Äù
-</p>
<!-- 
* Twitter API terms of use don't allow researchers to share tweets directly, only message IDs (except for limited distribution, e.g. by email). To reproduce the dataset, you use the Twitter API to download messages using their IDs. But, tweets that have been removed are not available - the distribution of removed tweets is not flat! (For example: you might end up with a dataset that has offensive posts but few "obvious" offensive posts.) -->
<ul>
<li>A dataset of Tweets following Hurricane Sandy makes it looks like
Manhattan was the hub of the disaster, because of power blackouts and
limited cell service in the most affected areas. <a
href="https://hbr.org/2013/04/the-hidden-biases-in-big-data">Source</a></li>
<li>The City of Boston released a smartphone app that uses accelerometer
and GPS data to detect potholes and report them automatically. But, low
income and older residents are less likely to have smartphones, so this
dataset presents a skewed view of where potholes are. <a
href="https://hbr.org/2013/04/the-hidden-biases-in-big-data">Source</a></li>
</ul>
</aside>
</section>
<section id="types-of-missingness" class="slide level3 cell markdown">
<h3>Types of ‚Äúmissingness‚Äù</h3>
<ul>
<li>not related to anything of interest</li>
<li>correlated with observed features</li>
<li>correlated with measure of interest</li>
</ul>
<aside class="notes">
<p>These are often referred to using this standard terminology (which
can be confusing):</p>
<ul>
<li>Missing <em>completely</em> at random: equal probability of being
missing for every sample.</li>
<li>Missing at random: samples with <span class="math inline">\(x =
X\)</span> (for some feature, value <span
class="math inline">\(X\)</span>) more likely to be missing.</li>
<li>Missing not at random: some values of target variable <span
class="math inline">\(y\)</span>, more likely to be missing.</li>
</ul>
<p>For example, suppose we want to survey students about their course
load and their stress levels. in order to predict stress levels in
future students and better advise them about course registration -</p>
<ul>
<li>MCAR: a pile of survey forms is accidentally thrown out. Losing this
data doesn‚Äôt have any systematic impact, beyond the less of the
data.</li>
<li>MAR: last-semester students are less likely to fill out the survey
than first-semester students, because they don‚Äôt feel like they‚Äôll be
around long enough to benefit from the results. Losing this data means
that our end result may be biased, or less accurate, for students in
their last semester.</li>
<li>MNAR: students who are stressed out are less likely to fill out the
survey. Losing this data is likely to have a (bad) systematic
effect.</li>
</ul>
</aside>
</section>
<section id="handling-missing-data" class="slide level3">
<h3>Handling missing data</h3>
<p>How should you handle little bits of missing data? It always depends
on the data and the circumstances. Some possibilities include:</p>
<ul>
<li>omit the row (or column)</li>
<li>fill back/forward (ordered rows)</li>
<li>fill with mean, median, max, mode‚Ä¶</li>
</ul>
<aside class="notes">
<p>You generally have to know why the data is missing, to understand the
best way to handle it. If imputing a value, we want it to be <em>as
close as possible to the true (unknown) value</em>.</p>
<p>During the data preparation stage, it‚Äôs important not to
‚Äúcontaminate‚Äù the test set - any cleaning that uses statistics of the
data (mean, max, etc.) must use the statistics of the training set
only.</p>
<p>For example: if imputing missing values with the mean of a feature,
we would use the mean of the feature in the training set only.</p>
</aside>
<!-- 

Biological reasoning

Physical reasoning

Social reasoning

Practical reasoning (see HellaSwag)

Object and individual tracking

Non sequiturs

failed gemini prompts:

* generate a photorealistic image of a pencil in a glass of water
* Two days ago, I found two cute little bunnies in my backyard. Yesterday, I found two dead little bunnies in my backyard and I buried them. What will I find in my backyard tomorrow? Answer in 5 words or less.


https://www.technologyreview.com/2020/08/22/1007539/gpt3-openai-language-generator-artificial-intelligence-ai-opinion/






Mis-labeled examples:

https://www.surgehq.ai/blog/30-percent-of-googles-reddit-emotions-dataset-is-mislabeled

> Yay, cold McDonald's. My favorite. ‚Äì mislabeled as LOVE




-->
<!-- 
### Example: anomalous voting data (1)

![Data like this was widely (wrongly) used as evidence of anomaly in the 2020 U.S. Presidential election.](../images/1-election2020.png){ width=30% }

::: notes

What are our assumptions about election night data, and how are they violated here? 

We expect that per-candidate vote totals (computed by multiplying total votes and vote share) should increase as more votes are counted, but never decrease.

What are possible explanations?

:::

### Example: anomalous voting data (2)

![Process by which data is collected by Edison and AP.](../images/1-election2020-process.png){ width=75% }

::: notes

This anomaly makes a lot of sense as a correction of a data entry or duplicate entry error. 

How Edison/AP collects the data for their Election Night feed:

* There are "stringers" (temporary reporters) at various elections offices who call results into their phone center
* They have people who look at official government websites for new results that they manually enter into the system
* They have people who monitor results sent by fax from counties and cities

all working as fast as they can! Data entry and duplicate entry errors are not only likely, they are almost guaranteed. When they are corrected, vote totals may decrease.

Source: [AP](https://web.archive.org/web/20210410214207/https://www.ap.org/en-us/topics/politics/elections/counting-the-vote), [Edison](http://www.edisonresearch.com/wp-content/uploads/2020/10/Web-Entry-Team-Handout-2020.pdf)

:::

-->
</section></section>
<section id="recap-working-with-data" class="title-slide slide level2">
<h2>Recap: Working with data</h2>

</section>
    </div>
  </div>

  <script src="reveal.js-master/dist/reveal.js"></script>

  <!-- reveal.js plugins -->
  <script src="reveal.js-master/plugin/notes/notes.js"></script>
  <script src="reveal.js-master/plugin/search/search.js"></script>
  <script src="reveal.js-master/plugin/zoom/zoom.js"></script>
  <script src="reveal.js-master/plugin/math/math.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
        // Display controls in the bottom right corner
        controls: true,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: true,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'bottom-right',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: false,

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: false,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: true,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'default',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: true,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'slide',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'fade',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        math: {
          mathjax: '/usr/share/javascript/mathjax/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [
          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    </body>
</html>
