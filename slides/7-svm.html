<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="author" content="Fraida Fund">
  <title>Support vector machines</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="reveal.js-master/dist/reset.css">
  <link rel="stylesheet" href="reveal.js-master/dist/reveal.css">
  <style>
    .reveal .sourceCode {  /* see #7635 */
      overflow: visible;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="reveal.js-master/dist/theme/white.css" id="theme">
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <h1 class="title">Support vector machines</h1>
  <p class="author">Fraida Fund</p>
</section>

<section class="slide level3">

<aside class="notes">
<p><strong>Math prerequisites for this lecture</strong>: Constrained
optimization (Appendix C in in Boyd and Vandenberghe).</p>
</aside>
<!--

## In this lecture

* Maximal margin classifier
* Support vector classifier
* Solving constrained optimization to find coefficients

-->
</section>
<section>
<section id="maximal-margin-classifier"
class="title-slide slide level2">
<h2>Maximal margin classifier</h2>

</section>
<section id="binary-classification-problem" class="slide level3">
<h3>Binary classification problem</h3>
<ul>
<li><span class="math inline">\(n\)</span> training samples, each with
<span class="math inline">\(p\)</span> features <span
class="math inline">\(\mathbf{x}_1, \ldots, \mathbf{x}_n \in
\mathbb{R}^p\)</span></li>
<li>Class labels <span class="math inline">\(y_1, \ldots, y_n \in \{-1,
1\}\)</span></li>
</ul>
</section>
<section id="linear-separability" class="slide level3">
<h3>Linear separability</h3>
<p>The problem is <strong>perfectly linearly separable</strong> if there
exists a <strong>separating hyperplane</strong> <span
class="math inline">\(H_i\)</span> such that</p>
<ul>
<li>all <span class="math inline">\(\mathbf{x} \in C_i\)</span> lie on
its positive side, and</li>
<li>all <span class="math inline">\(\mathbf{x} \in C_j, j \neq
i\)</span> lie on its negative side.</li>
</ul>
<aside class="notes">
<p>In the binary classification case: The data are linearly separable if
we can find a hyperplane that places all <span
class="math inline">\(y=1\)</span> points on one side and all <span
class="math inline">\(y=-1\)</span> on the other.</p>
</aside>
</section>
<section id="separating-hyperplane-1" class="slide level3">
<h3>Separating hyperplane (1)</h3>
<p>The separating hyperplane has the property that for all <span
class="math inline">\(i=1,\ldots,n\)</span>,</p>
<p><span class="math display">\[w_0 + \sum_{j=1}^p w_j x_{ij} &gt; 0
\text{ if } y_i = 1\]</span></p>
<p><span class="math display">\[w_0 + \sum_{j=1}^p w_j x_{ij} &lt; 0
\text{ if } y_i = -1\]</span></p>
</section>
<section id="separating-hyperplane-2" class="slide level3">
<h3>Separating hyperplane (2)</h3>
<p>Equivalently:</p>
<p><span class="math display">\[\begin{equation}
y_i \left(w_0 + \sum_{j=1}^p w_j x_{ij} \right) &gt; 0
\label{eq:sep-no-margin}
\end{equation}\]</span></p>
<aside class="notes">
<p>(we mention this compact form because we will use it in our
formulation of the classifier.)</p>
</aside>
</section>
<section id="using-the-hyperplane-to-classify" class="slide level3">
<h3>Using the hyperplane to classify</h3>
<p>Then, we can classify a new sample <span
class="math inline">\(\mathbf{x}\)</span> using the sign of</p>
<p><span class="math display">\[z = w_0 + \sum_{j=1}^p w_j
x_{j}\]</span></p>
<p>and we can use the magnitude of <span
class="math inline">\(z\)</span> to determine how confident we are about
our classification. (Larger <span class="math inline">\(z\)</span> =
farther from hyperplane = more confident about classification.)</p>
</section>
<section id="which-separating-hyperplane-is-best" class="slide level3">
<h3>Which separating hyperplane is best?</h3>
<figure>
<img data-src="../images/6-many-hyperplanes.png" style="width:35.0%"
alt="If the data is linearly separable, there are many separating hyperplanes." />
<figcaption aria-hidden="true">If the data is linearly separable, there
are many separating hyperplanes.</figcaption>
</figure>
<aside class="notes">
<p>We said there will be infinitely many separating hyperplanes, if
there is one:</p>
<ul>
<li>Previously, with the logistic regression, we found the maximum
likelihood classifier: the hyperplane that maximizes the probability of
these particular observations.</li>
<li>This time, we’ll find a different one.</li>
</ul>
</aside>
</section>
<section id="margin" class="slide level3">
<h3>Margin</h3>
<p>For any “candidate” hyperplane,</p>
<ul>
<li>Compute distance from each sample to separating hyperplane.</li>
<li>Smallest distance among all samples is called the
<strong>margin</strong>.</li>
</ul>
<aside class="notes">
<figure>
<img data-src="../images/6-mm-margin.png" style="width:35.0%"
alt="For this hyperplane, bold lines show the smallest distance (tie among several samples)." />
<figcaption aria-hidden="true">For this hyperplane, bold lines show the
smallest distance (tie among several samples).</figcaption>
</figure>
</aside>
</section>
<section id="classifier-that-maximizes-the-margin" class="slide level3">
<h3>Classifier that maximizes the margin</h3>
<ul>
<li>Among all separating hyperplanes, choose the one with the largest
margin!</li>
<li>Find the widest “slab” we can fit between the two classes; use the
midline of this “slab”.</li>
</ul>
<aside class="notes">
<figure>
<img data-src="../images/6-mm-classifier.png" style="width:35.0%"
alt="Maximal margin classifier. Width of the “slab” is 2x the margin." />
<figcaption aria-hidden="true">Maximal margin classifier. Width of the
“slab” is 2x the margin.</figcaption>
</figure>
</aside>
</section>
<section id="support-vectors" class="slide level3">
<h3>Support vectors</h3>
<ul>
<li>Points that lie on the border of maximal margin hyperplane are
<strong>support vectors</strong></li>
<li>They “support” the maximal margin hyperplane: if these points move,
then the maximal margin hyperplane moves</li>
<li>Maximal margin hyperplane is not affected by movement of any other
point, as long as it doesn’t cross borders!</li>
</ul>
<aside class="notes">
<figure>
<img data-src="../images/6-mm.png" style="width:80.0%"
alt="Maximal margin classifier (left) is not affected by movement of a point that is not a support vector (middle) but the hyperplane and/or margin are affected by movement of a support vector (right)." />
<figcaption aria-hidden="true">Maximal margin classifier (left) is not
affected by movement of a point that is not a support vector (middle)
but the hyperplane and/or margin are affected by movement of a support
vector (right).</figcaption>
</figure>
</aside>
</section>
<section id="constructing-the-maximal-margin-classifier"
class="slide level3">
<h3>Constructing the maximal margin classifier</h3>
<p>To construct this classifier, we will set up a <em>constrained
optimization</em> problem with:</p>
<ul>
<li>an objective</li>
<li>one or more constraints to satisfy</li>
</ul>
<p>What should the objective/constraints be in this scenario?</p>
</section>
<section id="constructing-the-maximal-margin-classifier-1"
class="slide level3">
<h3>Constructing the maximal margin classifier (1)</h3>
<p><span class="math display">\[\begin{equation}
\operatorname*{maximize}_{\mathbf{w}, \gamma} \gamma
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
\text{subject to: }\sum_{j=1}^{p} w_j^2 = 1
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
\text{and } y_i \left(w_0 + \sum_{j=1}^{p} w_j x_{ij} \right) \geq
\gamma, \forall i
\end{equation}\]</span></p>
<aside class="notes">
<hr />
<p>The constraint</p>
<p><span class="math display">\[y_i \left(w_0 + \sum_{j=1}^{p} w_j
x_{ij} \right) \geq \gamma, \forall i \]</span></p>
<p>guarantees that each observation is on the correct side of the
hyperplane <em>and</em> on the correct side of the margin, if margin
<span class="math inline">\(\gamma\)</span> is positive. (This is
analogous to Equation <span
class="math inline">\(\ref{eq:sep-no-margin}\)</span>, but we have added
a margin.)</p>
<p>The constraint</p>
<p><span class="math display">\[\text{and } \sum_{j=1}^{p} w_j^2 =
1\]</span></p>
<p>is not really a constraint: if a separating hyperplane is defined by
<span class="math inline">\(w_0 + \sum_{j=1}^{p} w_j x_{ij} =
0\)</span>, then for any <span class="math inline">\(k \neq 0\)</span>,
<span class="math inline">\(k\left( w_0 + \sum_{j=1}^{p} w_j
x_{ij}\right) = 0\)</span> is also a separating hyperplane.</p>
<p>This “constraint” just scales <span
class="math inline">\(\mathbf{w}\)</span> so that distance from <span
class="math inline">\(i\)</span>th sample to the hyperplane is given by
<span class="math inline">\(y_i \left(w_0 + \sum_{j=1}^{p} w_j x_{ij}
\right)\)</span>. This is what make the previous constraint
meaningful!</p>
<figure>
<img data-src="../images/6-mm-simple.png" style="width:35.0%"
alt="Maximal margin classifier." />
<figcaption aria-hidden="true">Maximal margin classifier.</figcaption>
</figure>
</aside>
</section>
<section id="constructing-the-maximal-margin-classifier-2"
class="slide level3">
<h3>Constructing the maximal margin classifier (2)</h3>
<p>The constraints ensure that</p>
<ul>
<li>Each observation is on the correct side of the hyperplane, and</li>
<li>at least <span class="math inline">\(\gamma\)</span> away from the
hyperplane</li>
</ul>
<p>and <span class="math inline">\(\gamma\)</span> is maximized.</p>
</section>
<section id="problems-with-mm-classifier-1" class="slide level3">
<h3>Problems with MM classifier (1)</h3>
<figure>
<img data-src="../images/6-mm-no-solution.png" style="width:25.0%"
alt="When data is not linearly separable, optimization problem has no solution with \gamma &gt;0." />
<figcaption aria-hidden="true">When data is not linearly separable,
optimization problem has no solution with <span
class="math inline">\(\gamma &gt;0\)</span>.</figcaption>
</figure>
</section>
<section id="problems-with-mm-classifier-2" class="slide level3">
<h3>Problems with MM classifier (2)</h3>
<figure>
<img data-src="../images/6-mm-not-robust.png" style="width:60.0%"
alt="The classifier is not robust - one new observation can dramatically shift the hyperplane." />
<figcaption aria-hidden="true">The classifier is not robust - one new
observation can dramatically shift the hyperplane.</figcaption>
</figure>
</section></section>
<section>
<section id="support-vector-classifier"
class="title-slide slide level2">
<h2>Support vector classifier</h2>

</section>
<section id="basic-idea" class="slide level3">
<h3>Basic idea</h3>
<ul>
<li>Generalization of MM classifier to non-separable case</li>
<li>Use a hyperplane that <em>almost</em> separates the data</li>
<li>“Soft margin”</li>
</ul>
</section>
<section id="constructing-the-support-vector-classifier"
class="slide level3">
<h3>Constructing the support vector classifier</h3>
<p><span class="math display">\[\begin{equation}
\operatorname*{maximize}_{\mathbf{w}, \mathbf{\epsilon}, \gamma} \gamma
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
\text{subject to: } \sum_{j=1}^{p} w_j^2 = 1
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
y_i \left(w_0 + \sum_{j=1}^{p} w_j x_{ij} \right) \geq
\gamma(1-\epsilon_i), \quad \forall i
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
\epsilon_i \geq 0, \quad \forall i, \quad \sum_{i=1}^n \epsilon_i \leq K
\end{equation}\]</span></p>
<aside class="notes">
<figure>
<img data-src="../images/6-svc.png" style="width:40.0%"
alt="Support vector classifier. Note: the blue arrows show y_i \gamma \epsilon_i." />
<figcaption aria-hidden="true">Support vector classifier. Note: the blue
arrows show <span class="math inline">\(y_i \gamma
\epsilon_i\)</span>.</figcaption>
</figure>
<p><span class="math inline">\(K\)</span> is a non-negative tuning
parameter.</p>
<p><strong>Slack variable</strong> <span
class="math inline">\(\epsilon_i\)</span> determines where a point
lies:</p>
<ul>
<li>If <span class="math inline">\(\epsilon_i = 0\)</span>, point is on
the correct side of margin</li>
<li>If <span class="math inline">\(\epsilon_i &gt; 0\)</span>, point has
<em>violated</em> the margin (wrong side of margin)</li>
<li>If <span class="math inline">\(\epsilon_i &gt; 1\)</span>, point is
on wrong side of hyperplane and is misclassified</li>
</ul>
<p><span class="math inline">\(K\)</span> is the <strong>budget</strong>
that determines the number and severity of margin violations we will
tolerate.</p>
<ul>
<li><span class="math inline">\(K=0 \rightarrow\)</span> same as MM
classifier</li>
<li><span class="math inline">\(K &gt; 0\)</span>, no more than <span
class="math inline">\(K\)</span> observations may be on wrong side of
hyperplane</li>
<li>As <span class="math inline">\(K\)</span> increases, more violations
allowed, margin can be wider</li>
</ul>
</aside>
</section>
<section id="support-vector" class="slide level3">
<h3>Support vector</h3>
<p>For a support vector classifier, the only points that affect the
classifier are:</p>
<ul>
<li>Points that lie on the margin boundary</li>
<li>Points that violate margin</li>
</ul>
<p>These are the <em>support vectors</em>.</p>
</section>
<section id="illustration-of-effect-of-k" class="slide level3">
<h3>Illustration of effect of <span
class="math inline">\(K\)</span></h3>
<figure>
<img data-src="../images/6-svc-c.png" style="width:45.0%"
alt="The margin shrinks as K decreases." />
<figcaption aria-hidden="true">The margin shrinks as <span
class="math inline">\(K\)</span> decreases.</figcaption>
</figure>
<!--
![ISL Fig. 9.7: Margin shrinks as $C$ decreases.](../images/9.7.svg){ width=50% }
-->
</section>
<section id="k-controls-bias-variance-tradeoff" class="slide level3">
<h3><span class="math inline">\(K\)</span> controls bias-variance
tradeoff</h3>
<ul>
<li>Wide margin (<span class="math inline">\(K\)</span> large): many
support vectors, low variance, high bias.</li>
<li>Narrow margin (<span class="math inline">\(K\)</span> small): few
support vectors, high variance, low bias.</li>
</ul>
<aside class="notes">
<p><strong>Terminology note</strong>: In ISL and in the first part of
these notes, meaning of constant is opposite its meaning in Python
<code>sklearn</code>:</p>
<ul>
<li>ISL and these notes: Large <span class="math inline">\(K\)</span>,
wide margin.</li>
<li>Python <code>sklearn</code>: Large <span
class="math inline">\(C\)</span>, small margin.</li>
</ul>
</aside>
</section></section>
<section>
<section id="solution" class="title-slide slide level2">
<h2>Solution</h2>

</section>
<section id="problem-formulation---original" class="slide level3">
<h3>Problem formulation - original</h3>
<p><span class="math display">\[
\begin{aligned}
\operatorname*{maximize}_{\mathbf{w}, \mathbf{\epsilon}, \gamma} \quad
&amp;  \gamma \\
\text{subject to} \quad &amp;\sum_{j=1}^{p} w_j^2 = 1 \\
&amp; y_i \left(w_0 + \sum_{j=1}^{p} w_j x_{ij} \right) \geq
\gamma(1-\epsilon_i), \quad \forall i \\
&amp;  \epsilon_i \geq 0, \quad \forall i \\
&amp; \sum_{i=1}^n \epsilon_i \leq K
\end{aligned}
\]</span></p>
</section>
<section id="problem-formulation---equivalent" class="slide level3">
<h3>Problem formulation - equivalent</h3>
<aside class="notes">
<p>Remember that scaling <span class="math inline">\(\mathbf{w}\)</span>
doesn’t change the separating hyperplane. If we scale so that the margin
boundaries are at <span class="math inline">\(+1\)</span> and <span
class="math inline">\(-1\)</span>, then <span
class="math inline">\(\gamma = \frac{1}{||w||}\)</span>, and we can
formulate the equivalent minimization problem:</p>
</aside>
<p><span class="math display">\[
\begin{aligned}
\operatorname*{minimize}_{\mathbf{w}, \mathbf{\epsilon}} \quad &amp;
\sum_{j=1}^{p} w_j^2 \\
\text{subject to} \quad &amp; y_i \left(w_0 + \sum_{j=1}^{p} w_j x_{ij}
\right) \geq 1-\epsilon_i, \forall i \\
&amp; \epsilon_i \geq 0, \quad \forall i \\
&amp; \sum_{i=1}^n \epsilon_i \leq K \\
\end{aligned}
\]</span></p>
</section>
<section id="problem-formulation---equivalent-2" class="slide level3">
<h3>Problem formulation - equivalent (2)</h3>
<aside class="notes">
<p>Next, move the “budget” into the objective function (easier to
compute):</p>
</aside>
<p><span class="math display">\[
\begin{aligned}
\operatorname*{minimize}_{\mathbf{w}, \mathbf{\epsilon}} \quad &amp;
\frac{1}{2} \sum_{j=1}^p w_j^2  + C \sum_{i=1}^n \epsilon_i  \\
\text{subject to} \quad &amp; y_i(w_0 + \sum_{j=1}^p w_j x_{ij}) \geq
1-\epsilon_i, \quad \forall i \\
&amp; \epsilon_i \geq 0, \quad \forall i
\end{aligned}
\]</span></p>
</section>
<section id="background-constrained-optimization" class="slide level3">
<h3>Background: constrained optimization</h3>
<p>Basic formulation of contrained optimization problem:</p>
<ul>
<li><strong>Objective</strong>: Minimize <span
class="math inline">\(f(x)\)</span></li>
<li><strong>Constraint(s)</strong>: subject to <span
class="math inline">\(g(x)\leq 0\)</span></li>
</ul>
<aside class="notes">
<p>Find <span class="math inline">\({x}^{*}\)</span> that satisfies
<span class="math inline">\(g({x}^{*}) \leq 0\)</span> and, for any
other <span class="math inline">\(x\)</span> that satisfies <span
class="math inline">\(g(x) \leq 0\)</span>, <span
class="math inline">\(f(x) \geq f({x}^{*})\)</span>.</p>
</aside>
</section>
<section id="background-illustration" class="slide level3">
<h3>Background: Illustration</h3>
<figure>
<img data-src="../images/6-constrained-optimization.png"
style="width:80.0%"
alt="Minimizing objective function, without (left) and with (right) a constraint." />
<figcaption aria-hidden="true">Minimizing objective function, without
(left) and with (right) a constraint.</figcaption>
</figure>
</section>
<section id="background-solving-with-lagrangian-1" class="slide level3">
<h3>Background: Solving with Lagrangian (1)</h3>
<p>To solve, we form the Lagrangian:</p>
<p><span class="math display">\[
L(x, \lambda) = f(x) + \lambda_1 g_1(x) + \dots + \lambda_m g_m(x)
\]</span></p>
<p>where each <span class="math inline">\(\lambda \geq 0\)</span> is a
<em>Lagrange multiplier</em>.</p>
<aside class="notes">
<p>The <span class="math inline">\(\lambda g(x)\)</span> terms “pull”
solution toward feasible set, away from non-feasible set.</p>
</aside>
</section>
<section id="background-solving-with-lagrangian-2" class="slide level3">
<h3>Background: Solving with Lagrangian (2)</h3>
<p>Then, to solve, we use joint optimization over <span
class="math inline">\(x\)</span> and <span
class="math inline">\(\lambda\)</span>:</p>
<p><span class="math display">\[\operatorname*{minimize}_{x}
\operatorname*{maximize}_{\lambda \geq 0 } f(x) + \lambda
g(x)\]</span></p>
<p>over <span class="math inline">\(x\)</span> and <span
class="math inline">\(\lambda\)</span>.</p>
<aside class="notes">
<p>(“Solve” in the usual way for convex function: taking partial
derivative of <span class="math inline">\(L(x,\lambda)\)</span> with
respect to each argument, and setting to zero. The solution to the
original function will be a saddle point in the Lagrangian.)</p>
<ul>
<li>We minimize over <span class="math inline">\(x\)</span> (the
solution we want)</li>
<li>We maximize over <span class="math inline">\(\lambda\)</span>
(penalty for violating the constraint)</li>
</ul>
</aside>
</section>
<section id="background-solving-with-lagrangian-3" class="slide level3">
<h3>Background: Solving with Lagrangian (3)</h3>
<p><span class="math display">\[\operatorname*{minimize}_{x}
\operatorname*{maximize}_{\lambda \geq 0 } f(x) + \lambda
g(x)\]</span></p>
<p>Suppose that for the <span class="math inline">\(x\)</span> that
minimizes <span class="math inline">\(f(x)\)</span>, <span
class="math inline">\(g(x) \leq 0\)</span> (i.e. <strong><span
class="math inline">\(x\)</span> is in the feasible set</strong>.)</p>
<p>If <span class="math inline">\(g(x) &lt; 0\)</span> (constraint is
not active),</p>
<ul>
<li>to maximize: we want <span class="math inline">\(\lambda =
0\)</span></li>
<li>to minimize: we’ll minimize <span
class="math inline">\(f(x)\)</span>, <span class="math inline">\(\lambda
g(x) = 0\)</span></li>
</ul>
</section>
<section id="background-solving-with-lagrangian-4" class="slide level3">
<h3>Background: Solving with Lagrangian (4)</h3>
<p><span class="math display">\[\operatorname*{minimize}_{x}
\operatorname*{maximize}_{\lambda \geq 0 } f(x) + \lambda
g(x)\]</span></p>
<p>Suppose that for the <span class="math inline">\(x\)</span> that
minimizes <span class="math inline">\(f(x)\)</span>, <span
class="math inline">\(g(x) &gt; 0\)</span> (<strong><span
class="math inline">\(x\)</span> is not in the feasible
set.</strong>)</p>
<ul>
<li>to maximize: we want <span class="math inline">\(\lambda &gt;
0\)</span></li>
<li>to minimize: we want small <span class="math inline">\(g(x)\)</span>
and <span class="math inline">\(f(x)\)</span>.</li>
</ul>
<aside class="notes">
<p>In this case, the “pull” between</p>
<ul>
<li>the <span class="math inline">\(x\)</span> that minimizes <span
class="math inline">\(f(x)\)</span></li>
<li>and the <span class="math inline">\(\lambda g(x)\)</span> which
pulls toward the feasible set,</li>
</ul>
<p>ends up making the constraint “tight”. We will use the <span
class="math inline">\(x\)</span> on the edge of the feasible set (<span
class="math inline">\(g(x) = 0\)</span>, constraint is active) for which
<span class="math inline">\(f(x)\)</span> is smallest.</p>
<p>This is called complementary slackness: for every constraint, <span
class="math inline">\(\lambda g(x) = 0\)</span>, either because <span
class="math inline">\(\lambda = 0\)</span> (inactive constraint) or
<span class="math inline">\(g(x) = 0\)</span> (active constraint).</p>
</aside>
</section>
<section id="background-activeinactive-constraint" class="slide level3">
<h3>Background: Active/inactive constraint</h3>
<figure>
<img data-src="../images/6-active-inactive.png" style="width:80.0%"
alt="Optimization with inactive, active constraint." />
<figcaption aria-hidden="true">Optimization with inactive, active
constraint.</figcaption>
</figure>
</section>
<section id="background-primal-and-dual-formulation"
class="slide level3">
<h3>Background: Primal and dual formulation</h3>
<p>Under the right conditions, the solution to the <em>primal</em>
problem:</p>
<p><span class="math display">\[\operatorname*{minimize}_{x}
\operatorname*{maximize}_{\lambda \geq 0 } L(x, \lambda) \]</span></p>
<p>is the same as the solution to the <em>dual</em> problem:</p>
<p><span class="math display">\[\operatorname*{maximize}_{\lambda \geq 0
} \operatorname*{minimize}_{x} L(x, \lambda) \]</span></p>
</section>
<section id="problem-formulation---lagrangian-primal"
class="slide level3">
<h3>Problem formulation - Lagrangian primal</h3>
<aside class="notes">
<p>Back to our SVC problem - let’s form the Lagrangian (introducing
<span class="math inline">\(\alpha_i\)</span> multipliers for the
contraint on the margin violation and <span
class="math inline">\(\mu_i\)</span> multipliers for the non-negativity
constraint on slack variables):</p>
</aside>
<p><span class="math display">\[
\begin{aligned}
\operatorname*{minimize}_{\mathbf{w}, \mathbf{\epsilon}}
\operatorname*{maximize}_{\alpha_i \geq 0, \mu_i \geq 0, \forall i }
\quad &amp; \frac{1}{2} \sum_{j=1}^p w_j^2   \\
&amp; + C \sum_{i=1}^n \epsilon_i \\
&amp; - \sum_{i=1}^n  \alpha_i \left[ y_i(w_0 + \sum_{j=1}^p w_j x_{ij})
- (1-\epsilon_i) \right]  \\
&amp; - \sum_{i=1}^n \mu_i \epsilon_i
\end{aligned}
\]</span></p>
<aside class="notes">
<p>This is the <em>primal</em> problem.</p>
</aside>
</section>
<section id="problem-formulation---lagrangian-dual"
class="slide level3">
<h3>Problem formulation - Lagrangian dual</h3>
<aside class="notes">
<p>The equivalent <em>dual</em> problem:</p>
</aside>
<p><span class="math display">\[
\begin{aligned}
\operatorname*{maximize}_{\alpha_i \geq 0, \mu_i \geq 0, \forall i }
\operatorname*{minimize}_{\mathbf{w}, \mathbf{\epsilon}}  \quad &amp;
\frac{1}{2} \sum_{j=1}^p w_j^2   \\
&amp; + C \sum_{i=1}^n \epsilon_i \\
&amp; - \sum_{i=1}^n  \alpha_i \left[ y_i(w_0 + \sum_{j=1}^p w_j x_{ij})
- (1-\epsilon_i) \right]  \\
&amp; - \sum_{i=1}^n \mu_i \epsilon_i
\end{aligned}
\]</span></p>
<aside class="notes">
<p>To solve, we take partial derivatives of the Lagrangian with respect
to <span class="math inline">\(\mathbf{w}\)</span> and <span
class="math inline">\(\mathbf{\epsilon}\)</span>, and set them to
zero.</p>
</aside>
</section>
<section id="partial-derivative-with-respect-to-mathbfw"
class="slide level3">
<h3>Partial derivative with respect to <span
class="math inline">\(\mathbf{w}\)</span></h3>
<p>Optimal coefficients for <span
class="math inline">\(j=1,\ldots,p\)</span> are:</p>
<p><span class="math display">\[\mathbf{w} = \sum_{i=1}^n {\alpha_i} y_i
\mathbf{x}_{i}\]</span></p>
<p>where <span class="math inline">\({\alpha_i}\)</span> come from the
solution to the dual problem.</p>
<aside class="notes">
<hr />
<p>Only two parts of the Lagrangian <span
class="math inline">\(L\)</span> depend on <span
class="math inline">\(\mathbf{w}\)</span>:</p>
<p><span class="math display">\[
\frac{1}{2} \sum_{j=1}^{p} w_j^2
\qquad \text{and} \qquad
- \sum_{i=1}^{n} \alpha_i y_i \sum_{j=1}^{p} w_j x_{ij}
\]</span></p>
<p>Differentiate:</p>
<p><span class="math display">\[
\frac{\partial}{\partial \mathbf{w}}
\left( \frac{1}{2} \sum_{j=1}^{p} w_j^2 \right)
= \mathbf{w}
\]</span></p>
<p><span class="math display">\[
\frac{\partial}{\partial \mathbf{w}}
\left( - \sum_{i=1}^{n} \alpha_i y_i \sum_{j=1}^{p} w_j x_{ij} \right)
= - \sum_{i=1}^{n} \alpha_i y_i \mathbf{x}_i
\]</span></p>
<p>Set derivative to zero:</p>
<p><span class="math display">\[
\mathbf{w} - \sum_{i=1}^{n} \alpha_i y_i \mathbf{x}_i = 0
\]</span></p>
<p>Therefore:</p>
<p><span class="math display">\[
\boxed{
\mathbf{w}
=
\sum_{i=1}^{n} \alpha_i y_i \mathbf{x}_i
}
\]</span></p>
</aside>
</section>
<section id="partial-derivative-with-respect-to-epsilon_i"
class="slide level3">
<h3>Partial derivative with respect to <span
class="math inline">\(\epsilon_i\)</span></h3>
<p>We find that</p>
<p><span class="math display">\[0 \le \alpha_i \le C\]</span></p>
<aside class="notes">
<hr />
<p>Terms involving <span class="math inline">\(\epsilon_i\)</span> in
the Lagrangian:</p>
<p><span class="math display">\[
C \epsilon_i + \alpha_i \epsilon_i - \mu_i \epsilon_i
= (C - \alpha_i - \mu_i)\epsilon_i
\]</span></p>
<p>Differentiate:</p>
<p><span class="math display">\[
\frac{\partial L}{\partial \epsilon_i}
= C - \alpha_i - \mu_i
\]</span></p>
<p>Set equal to zero:</p>
<p><span class="math display">\[
C - \alpha_i - \mu_i = 0
\]</span></p>
<p>Therefore:</p>
<p><span class="math display">\[
\alpha_i = C - \mu_i
\]</span></p>
<p>Because <span class="math inline">\(\mu_i \ge 0\)</span>, we get:</p>
<p><span class="math display">\[
\boxed{
0 \le \alpha_i \le C
}
\]</span></p>
</aside>
</section>
<section id="substituting-into-the-lagrangian" class="slide level3">
<h3>Substituting into the Lagrangian</h3>
<p><span class="math display">\[
\begin{aligned}
\operatorname*{maximize}_{\alpha_i \geq 0, \forall i }  \quad
&amp;  \sum_{i=1}^n \alpha_i  - \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n
\alpha_i \alpha_j y_i y_j \mathbf{x}_i^T \mathbf{x}_j \\
\text{subject to} \quad &amp; \sum_{i=1}^n \alpha_i y_i  = 0 \\
&amp; 0 \leq \alpha_i \leq C, \quad \forall i
\end{aligned}
\]</span></p>
<aside class="notes">
<p>After some algebra, the remaining optimization depends only on <span
class="math inline">\(\alpha\)</span>. Note that <span
class="math inline">\(\alpha\)</span> is non-zero only when the
constraint on margin violation is active - only for support vectors.</p>
</aside>
</section>
<section id="solution-1" class="slide level3">
<h3>Solution</h3>
<p>Once we solve for <span class="math inline">\(\alpha\)</span>,
optimal coefficients for <span
class="math inline">\(j=1,\ldots,p\)</span> are:</p>
<p><span class="math display">\[
\mathbf{w} = \sum_{i=1}^n \alpha_i\, y_i\, \mathbf{x}_i
\]</span></p>
<p>and we solve <span class="math inline">\(w_0 = y_i - \sum_{j=1}^p w_j
x_{ij}\)</span> using any sample <span class="math inline">\(i\)</span>
where <span class="math inline">\(\alpha_i &gt; 0\)</span>, i.e. any
support vector.</p>
</section>
<section id="why-solve-dual-problem" class="slide level3">
<h3>Why solve dual problem?</h3>
<p>For high-dimension problems (many features), dual problem can be much
faster to solve than primal problem:</p>
<ul>
<li>Primal problem: optimize over <span
class="math inline">\(p+1\)</span> coefficients.</li>
<li>Dual problem: optimize over <span class="math inline">\(n\)</span>
dual variables, but there are only as many non-zero ones as there are
support vectors.</li>
</ul>
<p>But mainly: the kernel trick, which we’ll discuss next, works for the
dual formulation, because the data only appears inside inner product
<span class="math inline">\(\mathbf{x}_i^T \mathbf{x}_j\)</span>!</p>
</section>
<section id="loss-function" class="slide level3">
<h3>Loss function</h3>
<p>This problem is equivalent to minimizing hinge loss:</p>
<p><span class="math display">\[\operatorname*{minimize}_{\mathbf{w}}
\left( \sum_{i=1}^n \max  \big(0,\, 1 - y_i z_i\big) + \frac{1}{C}
\sum_{j=1}^p w_j^2 \right)\]</span></p>
<p>where <span class="math inline">\(z_i = w_0 + \sum_{j=1}^{p} w_j
x_{ij}\)</span>.</p>
<aside class="notes">
<p>For a labeled observation <span class="math inline">\((\mathbf{x}_i,
y_i)\)</span> with <span class="math inline">\(y_i \in \{-1,
1\}\)</span>, let</p>
<p><span class="math display">\[z_i = w_0 + \sum_{j=1}^{p} w_j
x_{ij}\]</span></p>
<p>The hinge loss for this observation is <span
class="math inline">\(\max \left(0,\, 1 - y_i z_i \right)\)</span>.</p>
<table>
<colgroup>
<col style="width: 22%" />
<col style="width: 44%" />
<col style="width: 33%" />
</colgroup>
<thead>
<tr class="header">
<th>Value of <span class="math inline">\(y_i z_i\)</span></th>
<th>Interpretation</th>
<th>Hinge loss: <span class="math inline">\(\max(0,\,1 - y_i
z_i)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(&gt; 1\)</span></td>
<td>Correct and outside the margin</td>
<td><span class="math inline">\(0\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(= 1\)</span></td>
<td>Right on the margin</td>
<td><span class="math inline">\(0\)</span></td>
</tr>
<tr class="odd">
<td>Between <span class="math inline">\(0\)</span> and <span
class="math inline">\(1\)</span></td>
<td>Correct but inside the margin</td>
<td><span class="math inline">\(1 - y_i z_i\)</span> (greater than <span
class="math inline">\(0\)</span>)</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\le 0\)</span></td>
<td>Misclassified</td>
<td><span class="math inline">\(1 - y_i z_i\)</span> (greater than <span
class="math inline">\(0\)</span>)</td>
</tr>
</tbody>
</table>
<p>Hinge loss penalizes points only when they are inside the margin or
misclassified!</p>
</aside>
</section>
<section id="compared-to-logistic-regression" class="slide level3">
<h3>Compared to logistic regression</h3>
<ul>
<li><strong>Hinge loss</strong>: zero for points that are correct and
outside margin.</li>
<li><strong>Logistic regression loss</strong>: small but non-zero loss
for points far from separating hyperplane.</li>
</ul>
</section>
<section id="relationship-between-svm-and-other-models"
class="slide level3">
<h3>Relationship between SVM and other models</h3>
<ul>
<li>Like a logistic regression - linear classifier, separating
hyperplane is <span class="math inline">\(w_0 + \sum_{j=1}^p w_j x_{ij}
= 0\)</span></li>
<li>Like a weighted KNN - predicted label is weighted average of labels
for support vectors, with weights proportional to “similarity” of test
sample and support vector.</li>
</ul>
</section>
<section id="correlation-interpretation-1" class="slide level3">
<h3>Correlation interpretation (1)</h3>
<p>Given a new sample <span class="math inline">\(\mathbf{x}\)</span> to
classify, compute</p>
<p><span class="math display">\[
\begin{aligned}
z(\mathbf{x})
&amp;= w_0 + \sum_{j=1}^p w_j x_j \\[6pt]
&amp;= w_0 + \sum_{i=1}^n \alpha_i y_i \left( \sum_{j=1}^p x_{ij} x_j
\right) \\[6pt]
&amp;= w_0 + \sum_{i=1}^n \alpha_i y_i \, (\mathbf{x}_i^\top \mathbf{x})
\end{aligned}
\]</span></p>
<p>Measures inner product (a kind of “correlation”) between new sample
and each support vector.</p>
</section>
<section id="correlation-interpretation-2" class="slide level3">
<h3>Correlation interpretation (2)</h3>
<p>Classifier output (assuming -1,1 labels):</p>
<p><span class="math display">\[\hat{y}(\mathbf{x}) = \text{sign}
({z}(\mathbf{x}))\]</span></p>
<p>Predicted label is weighted average of labels for support vectors,
with weights proportional to “correlation” of test sample and support
vector.</p>
<!--
### Hinge loss vs. logistic regression


![ISL 9.12. Hinge loss is zero for points on correct side of margin.](../images/9.12.svg){ width=50% }
-->
</section></section>
    </div>
  </div>

  <script src="reveal.js-master/dist/reveal.js"></script>

  <!-- reveal.js plugins -->
  <script src="reveal.js-master/plugin/notes/notes.js"></script>
  <script src="reveal.js-master/plugin/search/search.js"></script>
  <script src="reveal.js-master/plugin/zoom/zoom.js"></script>
  <script src="reveal.js-master/plugin/math/math.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
        // Display controls in the bottom right corner
        controls: true,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: true,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'bottom-right',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: false,

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: false,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: true,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'default',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: true,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'slide',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'fade',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        math: {
          mathjax: '/usr/share/javascript/mathjax/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [
          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    </body>
</html>
