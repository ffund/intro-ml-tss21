<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="author" content="Fraida Fund">
  <title>K Nearest Neighbor</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="reveal.js-master/dist/reset.css">
  <link rel="stylesheet" href="reveal.js-master/dist/reveal.css">
  <style>
    .reveal .sourceCode {  /* see #7635 */
      overflow: visible;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="reveal.js-master/dist/theme/white.css" id="theme">
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <h1 class="title">K Nearest Neighbor</h1>
  <p class="author">Fraida Fund</p>
</section>

<section class="slide level3">

<aside class="notes">
<p><strong>Math prerequisites for this lecture</strong>: You should know
about</p>
<ul>
<li>probabilities, conditional probabilities, expectation of a random
variable</li>
<li>norm of a vector (Section I, Chapter 3 in Boyd and
Vandenberghe)</li>
<li>complexity of algorithms and especially of vector and matrix
operations (Appendix B in Boyd and Vandenberghe, also the complexity
part of Section I, Chapter 1 and Section II, Chapter 5)</li>
</ul>
</aside>
</section>
<section id="in-this-lecture" class="title-slide slide level2">
<h2>In this lecture</h2>
<ul>
<li>Parametric vs. non-parametric models</li>
<li>Nearest neighbor</li>
<li>Model choices</li>
<li>Bias and variance of KNN</li>
<li>The Curse of Dimensionality</li>
</ul>
</section>

<section>
<section id="parametric-vs.-non-parametric-models"
class="title-slide slide level2">
<h2>Parametric vs. non-parametric models</h2>

</section>
<section id="so-far" class="slide level3">
<h3>So far…</h3>
<p>All of our models have looked like</p>
<p><span class="math display">\[\hat{y} = f(x,w) = w_0 + w_1 x_1 +
\cdots + w_d x_d\]</span></p>
<p>A model class is more flexible if <span class="math inline">\(f(x,
w)\)</span> can represent more possible functions.</p>
<aside class="notes">
<ul>
<li>Some possibilities for <span class="math inline">\(f(x, w) = w_0 +
w_1 x_1\)</span></li>
<li>More possibilities for <span class="math inline">\(f(x, w) = w_0 +
w_1 x_1 + w_2 x_2\)</span></li>
<li>Even more possibilities for <span class="math inline">\(f(x, w) =
w_0 + w_1 x_1 + w_2 x_2 + w_3 x_3\)</span></li>
</ul>
<p>But until now, we had to “know” how to add flexibility - for example,
by adding interaction terms or other basis functions.</p>
<p>A way to get more flexible models is with a
<strong>non-parametric</strong> approach, where we don’t a priori impose
the functional form or a fixed number of parameters that the model
should learn.</p>
</aside>
</section>
<section id="parametric-models" class="slide level3">
<h3>Parametric models</h3>
<ul>
<li>A particular model class is assumed (e.g. linear)</li>
<li>Number of parameters fixed in advance</li>
</ul>
<aside class="notes">
<p>Note: even if you use K-fold CV to try different candidate models
with different number of parameters, you are still defining each
candidate model in advance.</p>
</aside>
</section>
<section id="non-parametric-models" class="slide level3">
<h3>Non-parametric models</h3>
<ul>
<li>Minimal assumptions about model class</li>
<li>Model structure determined by data</li>
</ul>
</section></section>
<section>
<section id="nearest-neighbor" class="title-slide slide level2">
<h2>Nearest neighbor</h2>
<aside class="notes">
<ul>
<li>A kind of non-parametric model.</li>
<li>Basic idea: Find labeled samples that are “similar” to the new
sample, and use their labels to make prediction for the new sample.</li>
</ul>
<p>We previously spoke about the inductive bias of the linear models -
we assumed that the target variable can be modeled as a linear
combination of features or transformed versions of features. What is the
inductive bias here?</p>
</aside>
</section>
<section id="nn" class="slide level3">
<h3>1-NN</h3>
<ul>
<li>Given training data <span class="math inline">\((\mathbf{x}_{1},
y_{1}), \ldots, (\mathbf{x}_{n}, y_{n})\)</span></li>
<li>And a new sample <span
class="math inline">\(\mathbf{x}_{0}\)</span></li>
<li>Find the sample in the training data <span
class="math inline">\(\mathbf{x}_{i&#39;}\)</span> with the least
distance to <span class="math inline">\(\mathbf{x}_{0}\)</span>.</li>
</ul>
<p><span class="math display">\[ i&#39; =
\operatorname*{argmin}_{i=1,\ldots,n}  d(\mathbf{x}_i,
\mathbf{x}_0)\]</span></p>
<ul>
<li>Let <span class="math inline">\(\hat{y}_0 = y_{i&#39;}\)</span></li>
</ul>
<aside class="notes">
<figure>
<img data-src="../images/5-1nn-example.png" style="width:40.0%"
alt="The nearest neighbor to the orange test point is the one that has the smallest distance in the feature space." />
<figcaption aria-hidden="true">The nearest neighbor to the orange test
point is the one that has the smallest distance in the feature
space.</figcaption>
</figure>
</aside>
</section>
<section id="nn---runtime" class="slide level3">
<h3>1-NN - runtime</h3>
<ul>
<li>Training: just store data</li>
<li>Inference: need to
<ul>
<li>compute distance to each of <span class="math inline">\(n\)</span>
points</li>
<li>distance metric typically scales with <span
class="math inline">\(d\)</span></li>
</ul></li>
</ul>
<aside class="notes">
<p>The runtime for predicting <em>each</em> test point is <span
class="math inline">\(O(nd)\)</span>. How does this compare to linear
regression or logistic regression?</p>
<p>There, the training time could be long, but <em>inference</em> was
only <span class="math inline">\(O(d)\)</span>. We can tolerate a long
training time more easily than a long inference time.</p>
</aside>
</section>
<section id="nn---approximation-as-a-step-function"
class="slide level3">
<h3>1NN - approximation as a step function</h3>
<aside class="notes">
<figure>
<img data-src="../images/6-knn-step.png" style="width:40.0%"
alt="The nearest neighbor model approximates the true function using steps." />
<figcaption aria-hidden="true">The nearest neighbor model approximates
the true function using steps.</figcaption>
</figure>
</aside>
</section>
<section id="nn---decision-boundaries" class="slide level3">
<h3>1NN - decision boundaries</h3>
<figure>
<img data-src="../images/knn-decision-boundaries.png"
style="width:60.0%"
alt="1NN decision boundaries - Nearest neighborhoods for each point of the training data set." />
<figcaption aria-hidden="true">1NN decision boundaries - Nearest
neighborhoods for each point of the training data set.</figcaption>
</figure>
<aside class="notes">
<p>Note that there will be zero error on training set (unless there are
training data points that have identical feature values, but different
labels).</p>
</aside>
</section>
<section id="k-nearest-neighbors" class="slide level3">
<h3>K nearest neighbors</h3>
<p>Instead of 1 closest sample, we find <span
class="math inline">\(K\)</span>: Let <span
class="math inline">\(N_0\)</span> be the set of <span
class="math inline">\(K\)</span> training points that are closest to
<span class="math inline">\(\mathbf{x}_0\)</span>.</p>
<p>How do we use this set for (1) classification? (2) regression?</p>
<aside class="notes">
<figure>
<img data-src="../images/5-3nn-line.png" style="width:30.0%"
alt="3 nearest neighbors." />
<figcaption aria-hidden="true">3 nearest neighbors.</figcaption>
</figure>
</aside>
</section>
<section id="knn-for-classification" class="slide level3">
<h3>KNN for classification</h3>
<aside class="notes">
<p>Idea: Estimate conditional probability for a class as fraction of
points among neighbors with the class label.</p>
<p>Remember: Let <span class="math inline">\(N_0\)</span> be the set of
<span class="math inline">\(K\)</span> training points that are closest
to <span class="math inline">\(\mathbf{x}_0\)</span>.</p>
<p>Then, we can estimate the per-class conditional probability given the
sample <span class="math inline">\(x_0\)</span>.</p>
</aside>
<p>For each class <span class="math inline">\(m \in M\)</span>:</p>
<p><span class="math display">\[ P(y=m | \mathbf{x_0} ) = \frac{1}{K}
\sum_{ (\mathbf{x}_i, y_i) \in N_0} I(y_i = m) \]</span></p>
<p>where <span class="math inline">\(I(y_i = m)\)</span> is 1 if <span
class="math inline">\((\mathbf{x}_i, y_i) \in N_0\)</span> is a member
of class <span class="math inline">\(m\)</span>, 0 otherwise.</p>
<aside class="notes">
<ul>
<li>We can then select the class with the highest probability.</li>
<li>Practically: select the most frequent class among the
neighbors.</li>
</ul>
</aside>
</section>
<section id="knn-for-regression" class="slide level3">
<h3>KNN for regression</h3>
<p>Idea: Use the the combined label of the K nearest neighbors. For
example, we can take their mean:</p>
<p><span class="math display">\[\hat{y}_{0} = \frac{1}{K} \sum_{
(\mathbf{x}_i, y_i) \in N_0} y_i \]</span></p>
<aside class="notes">
<figure>
<img data-src="../images/5-3nn-regression.png" style="width:40.0%"
alt="Example of a regression using median vote of the 3NN. The “true function” is shown in black, the orange dashed line shows the prediction of the regression model." />
<figcaption aria-hidden="true">Example of a regression using median vote
of the 3NN. The “true function” is shown in black, the orange dashed
line shows the prediction of the regression model.</figcaption>
</figure>
</aside>
</section></section>
<section>
<section id="model-choices" class="title-slide slide level2">
<h2>Model choices</h2>
<aside class="notes">
<p>We’re letting the data dictate the form of the solution, but we still
need to make many model choices:</p>
<ul>
<li>What value of <span class="math inline">\(K\)</span>?</li>
<li>What distance measure?</li>
<li>How to combine <span class="math inline">\(K\)</span> labels into
prediction?</li>
</ul>
<p>Question: does larger K mean “more flexible” or “less flexible”
model?</p>
</aside>
</section>
<section id="what-value-of-k-illustration-1nn" class="slide level3">
<h3>What value of K? Illustration (1NN)</h3>
<figure>
<img data-src="../images/knnDemo-1nn.png" style="width:50.0%"
alt="1NN" />
<figcaption aria-hidden="true">1NN</figcaption>
</figure>
</section>
<section id="what-value-of-k-illustration-2nn" class="slide level3">
<h3>What value of K? Illustration (2NN)</h3>
<figure>
<img data-src="../images/knnDemo-2nn.png" style="width:50.0%"
alt="2NN" />
<figcaption aria-hidden="true">2NN</figcaption>
</figure>
</section>
<section id="what-value-of-k-illustration-3nn" class="slide level3">
<h3>What value of K? Illustration (3NN)</h3>
<figure>
<img data-src="../images/knnDemo-3nn.png" style="width:50.0%"
alt="3NN" />
<figcaption aria-hidden="true">3NN</figcaption>
</figure>
</section>
<section id="what-value-of-k-illustration-9nn" class="slide level3">
<h3>What value of K? Illustration (9NN)</h3>
<figure>
<img data-src="../images/knnDemo-9nn.png" style="width:50.0%"
alt="9NN" />
<figcaption aria-hidden="true">9NN</figcaption>
</figure>
</section>
<section id="what-value-of-k-1" class="slide level3">
<h3>What value of K? (1)</h3>
<ul>
<li>In general: larger K, less complex model</li>
<li><span class="math inline">\(K\)</span> can be selected by CV.</li>
<li>Often cited “rule of thumb”: use <span
class="math inline">\(K=\sqrt{N}\)</span></li>
</ul>
</section>
<section id="what-value-of-k-2" class="slide level3">
<h3>What value of K? (2)</h3>
<ul>
<li>Alternative to fixed <span class="math inline">\(K\)</span>:
Radius-based neighbor learning.</li>
<li>A fixed radius <span class="math inline">\(r\)</span> is specified,
can be selected by CV.</li>
<li>Number of neighbors depends on local density of points.</li>
</ul>
<aside class="notes">
<figure>
<img data-src="../images/5-radius-based.png" style="width:60.0%"
alt="Comparison of KNN (top) and radius-based neighbor learning (bottom) when the density of training points in the feature space is uneven." />
<figcaption aria-hidden="true">Comparison of KNN (top) and radius-based
neighbor learning (bottom) when the density of training points in the
feature space is uneven.</figcaption>
</figure>
</aside>
</section>
<section id="what-distance-measure-1" class="slide level3">
<h3>What distance measure? (1)</h3>
<p>Some popular choices: for two vectors <span
class="math inline">\(a_i, b_i\)</span>,</p>
<ul>
<li>Euclidean (L2): <span class="math inline">\(\sqrt{\sum_{i=1}^d(a_i -
b_i)^2}\)</span></li>
<li>Manhattan (L1): <span class="math inline">\(\sum_{i=1}^d |a_i -
b_i|\)</span></li>
</ul>
<aside class="notes">
<p>(L2 distance prefers many medium-sized disagreements to one big
one.)</p>
<p>There are many more choices - for example, look at the <a
href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html">distance
metrics implemented in sklearn</a>.</p>
<p>Problems with the basic distance metrics:</p>
<ul>
<li>When features have different scale/range, need to standardize</li>
<li>KNN implicitly weights all features equally: this is a problem if
you have features that are not relevant for the target variable!</li>
<li>For images: pixel-wise distance doesn’t necessarily equate to
perceptual similarity</li>
</ul>
</aside>
</section>
<section id="distance-measure---standardization-1" class="slide level3">
<h3>Distance measure - standardization (1)</h3>
<figure>
<img data-src="../images/6-knn-standardize-1.png" style="width:40.0%"
alt="Without standardization, via https://stats.stackexchange.com/a/287439/. The x2 feature dominates the distance measure." />
<figcaption aria-hidden="true">Without standardization, via <a
href="https://stats.stackexchange.com/a/287439/">https://stats.stackexchange.com/a/287439/</a>.
The x2 feature dominates the distance measure.</figcaption>
</figure>
</section>
<section id="distance-measure---standardization-2" class="slide level3">
<h3>Distance measure - standardization (2)</h3>
<figure>
<img data-src="../images/6-knn-standardize-2.png" style="width:40.0%"
alt="With standardization, via https://stats.stackexchange.com/a/287439/" />
<figcaption aria-hidden="true">With standardization, via <a
href="https://stats.stackexchange.com/a/287439/">https://stats.stackexchange.com/a/287439/</a></figcaption>
</figure>
<blockquote>

</blockquote>
</section>
<section id="distance-measure---equal-weighted-features"
class="slide level3">
<h3>Distance measure - equal weighted features</h3>
<aside class="notes">
<p>Suppose you are trying to predict a student’s course grade using
their previous GPA and how far they live from the NYU campus:</p>
<figure>
<img data-src="../images/5-irrelevant.png" style="width:40.0%"
alt="The training point circled in orange is the nearest neighbor, but it’s not the most similar according to the only feature that matters (previous GPA). All features are weighted equally in the distance metric, regardless of their importance." />
<figcaption aria-hidden="true">The training point circled in orange is
the nearest neighbor, but it’s not the most similar according to the
only feature that matters (previous GPA). All features are weighted
equally in the distance metric, regardless of their
importance.</figcaption>
</figure>
</aside>
<p>Alternative to equal weighted features: assign feature weights</p>
<p><span class="math display">\[d(\mathbf{a, b}) = \left(  \sum_{i=1}^k
( w_i | a_i - b_i | ) ^q \right) ^{\frac{1}{q}}\]</span></p>
<aside class="notes">
<p>But then we need a way to learn feature weights!</p>
<p>With L1 regularization, we had a data-driven way to do feature
selection. The nearest neighbor method doesn’t have any “built-in” way
to do feature weighting or feature selection as part of the training
process, so we need to do it ourselves as part of the pre-processing
steps.</p>
<p>We’ll go back to this at the end.</p>
</aside>
</section>
<section id="distance-measure---perceptual-distance"
class="slide level3">
<h3>Distance measure - perceptual distance</h3>
<figure>
<img data-src="../images/knn-samenorm.png" style="width:60.0%"
alt="An original image (left) and three other images next to it that are all equally far away from it based on L2 pixel distance. Image via CS321n." />
<figcaption aria-hidden="true">An original image (left) and three other
images next to it that are all equally far away from it based on L2
pixel distance. Image via <a
href="https://cs231n.github.io/classification/#nn">CS321n</a>.</figcaption>
</figure>
<aside class="notes">
<p>This is a little more difficult to overcome. In practice, KNN is
often just not very useful for image data.</p>
</aside>
</section>
<section id="how-to-combine-labels-into-prediction"
class="slide level3">
<h3>How to combine labels into prediction?</h3>
<ul>
<li><strong>Basic voting</strong>: use mode of neighbors for
classification, mean or median for regression.</li>
<li><strong>Distance-weighted</strong>: weight of vote inversely
proportional to distance from the query point. (“More similar” training
points count more.)</li>
</ul>
<aside class="notes">
<figure>
<img data-src="../images/5-weight-by-distance.png" style="width:40.0%"
alt="In this example, the red training point will get a bigger “vote” in the class label because it is closest to the test point. The point can be classified as red, even though 2 of the 3 neighbors are blue." />
<figcaption aria-hidden="true">In this example, the red training point
will get a bigger “vote” in the class label because it is closest to the
test point. The point can be classified as red, even though 2 of the 3
neighbors are blue.</figcaption>
</figure>
</aside>
</section></section>
<section>
<section id="bias-and-variance-of-knn" class="title-slide slide level2">
<h2>Bias and variance of KNN</h2>
<!--

See: https://stats.stackexchange.com/questions/189806/derivation-of-bias-variance-decomposition-expression-for-k-nearest-neighbor-regr

-->
</section>
<section id="true-function" class="slide level3">
<h3>True function</h3>
<p>Suppose data has true relation</p>
<p><span class="math display">\[ y = t(\mathbf{x}) + \epsilon, \quad
\epsilon \sim N(0, \sigma_\epsilon^2) \]</span></p>
<p>and our model predicts <span class="math inline">\(\hat{y} =
f(\mathbf{x})\)</span>.</p>
</section>
<section id="assumption-of-fixed-training-set" class="slide level3">
<h3>Assumption of fixed training set</h3>
<p>For this derivation, we consider the expectation over:</p>
<ul>
<li>the test points</li>
<li>the error <span class="math inline">\(\epsilon\)</span></li>
<li>the randomness in the <span class="math inline">\(y\)</span> values
in the training set!</li>
</ul>
<p>We do not consider randomness in the <span
class="math inline">\(x\)</span> values - we assume a fixed training
set.</p>
<aside class="notes">
<figure>
<img data-src="../images/5-fixed-training-set.png" style="width:30.0%"
alt="Assume the x values in the training set are fixed (and therefore, the neighbors of any given test point), but there is randomness in the y values." />
<figcaption aria-hidden="true">Assume the <span
class="math inline">\(x\)</span> values in the training set are fixed
(and therefore, the neighbors of any given test point), but there is
randomness in the <span class="math inline">\(y\)</span>
values.</figcaption>
</figure>
</aside>
</section>
<section id="expected-loss" class="slide level3">
<h3>Expected loss</h3>
<p>We will use an L2 loss function, so that the expected error of the
prediction <span class="math inline">\(\hat{y}\)</span> for a given test
point <span class="math inline">\(\mathbf{x_t}\)</span> is:</p>
<p><span class="math display">\[ \begin{aligned}
MSE(\mathbf{x_t}) :=&amp;  E[(y-\hat{y})^2]  \\
=&amp; \left( t(\mathbf{x_t})- E[f(\mathbf{x_t})] \right) ^2 + \\
&amp;E[(f(\mathbf{x_t}) - E[f(\mathbf{x_t})])^2] + \\
&amp;\sigma_\epsilon^2
\end{aligned}\]</span></p>
<p>i.e. squared bias, variance, and irreducible error.</p>
</section>
<section id="knn-output" class="slide level3">
<h3>KNN output</h3>
<p>The output of a KNN regression at the test point is</p>
<p><span class="math display">\[f(\mathbf{x_t}) = \frac{1}{K} \sum_{\ell
\in K_x} t(\mathbf{x}_\ell) + \epsilon_\ell \]</span></p>
<p>where <span class="math inline">\(K_x\)</span> is the set of K
nearest neighbors of <span class="math inline">\(\mathbf{x_t}\)</span>.
(We assume that these neighbors are fixed.)</p>
</section>
<section id="bias-of-knn" class="slide level3">
<h3>Bias of KNN</h3>
<p>When we take expectation of bias over test samples:</p>
<p><span class="math display">\[
\begin{aligned}
Bias^2 &amp;= \left( t(\mathbf{x_t})- E[f(\mathbf{x_t})] \right) ^2 \\
&amp;= \left( t(\mathbf{x_t})   - E \left( \frac{1}{K} \sum_{\ell \in
K_x} t(\mathbf{x}_\ell) + \epsilon_\ell \right)\right) ^2 \\
&amp;=\left( t(\mathbf{x_t}) - \frac{1}{K} \sum_{\ell \in K_x}
t(\mathbf{x}_\ell)  \right) ^2
\end{aligned}
\]</span></p>
<aside class="notes">
<p>The expectation is over the training sample draw - but note that the
<span class="math inline">\(x\)</span> values in the training samples
are fixed! So the only randomness is in <span
class="math inline">\(\epsilon_\ell\)</span>.</p>
<p>Since the <span class="math inline">\(x\)</span> values in the
training samples are fixed, the <span class="math inline">\(\frac{1}{K}
\sum_{\ell \in K_x} t(\mathbf{x}_\ell)\)</span> can come out of the
expectation as a constant. Then <span class="math inline">\(E
[\epsilon_\ell] = 0\)</span>.</p>
</aside>
</section>
<section id="variance-of-knn-1" class="slide level3">
<h3>Variance of KNN (1)</h3>
<p><span class="math display">\[
\begin{aligned}
Var(\hat{y}) &amp;= E[(f(\mathbf{x_t}) - E[f(\mathbf{x_t})])^2] \\
&amp;= E\left[\left(f(\mathbf{x_t}) - \frac{1}{K} \sum_{\ell \in K_x}
t(\mathbf{x}_\ell)  \right)^2\right] \\
&amp;= E\left[\left( \frac{1}{K} \sum_{\ell \in K_x} (t(\mathbf{x}_\ell)
+ \epsilon_\ell) - \frac{1}{K} \sum_{\ell \in K_x}
t(\mathbf{x}_\ell)  \right)^2\right] \\
&amp;= E\left[\left( \frac{1}{K} \sum_{\ell \in K_x}  \epsilon_\ell
\right)^2\right]
\end{aligned}
\]</span></p>
</section>
<section id="variance-of-knn-2" class="slide level3">
<h3>Variance of KNN (2)</h3>
<p><span class="math display">\[
\begin{aligned}
&amp;= E\left[\left( \frac{1}{K} \sum_{\ell \in K_x}  \epsilon_\ell
\right)^2\right] = \frac{1}{K^2} E\left[\left(  \sum_{\ell \in
K_x}  \epsilon_\ell \right)^2\right] \\
&amp;=\frac{1}{K^2} Var \left(  \sum_{\ell \in K_x}  \epsilon_\ell
\right)  = \frac{1}{K^2}   \sum_{\ell \in K_x} Var \left( \epsilon_\ell
\right) = \frac{K \sigma^2_\epsilon}{K^2} \\
&amp;= \frac{\sigma^2_\epsilon}{K}
\end{aligned}
\]</span></p>
<aside class="notes">
<p>Note: we use the fact that the <span
class="math inline">\(\epsilon\)</span> terms are independent, so the
variance of sum is equal to sum of variances.</p>
</aside>
</section>
<section id="error-of-knn" class="slide level3">
<h3>Error of KNN</h3>
<p>Then the expected error of KNN is</p>
<p><span class="math display">\[
\left( t(\mathbf{x_t}) - \frac{1}{K} \sum_{\ell \in K_x}
t(\mathbf{x}_\ell)  \right) ^2 + \frac{\sigma^2_\epsilon}{K} +
\sigma_\epsilon^2
\]</span></p>
<p>where <span class="math inline">\(K_x\)</span> is the set of K
nearest neighbors of <span
class="math inline">\(\mathbf{x}\)</span>.</p>
</section>
<section id="bias-variance-tradeoff" class="slide level3">
<h3>Bias variance tradeoff</h3>
<ul>
<li>Variance decreases with K</li>
<li>Bias likely to increase with K, if function <span
class="math inline">\(t()\)</span> is smooth.</li>
</ul>
<aside class="notes">
<p>Why does bias increase with <span class="math inline">\(K\)</span>?
For a smooth function, the few closest neighbors to the test point will
have similar values, so average will be close to <span
class="math inline">\(t(\mathbf{x})\)</span>; as K increases, neighbors
are further way, and average of neighbors moves away from <span
class="math inline">\(t(\mathbf{x})\)</span>.</p>
<p>You can think about the extreme case, where <span
class="math inline">\(K=n\)</span> so you use the average of
<em>all</em> of the training samples. This is equivalent to “prediction
by mean”.</p>
</aside>
</section></section>
<section>
<section id="the-curse-of-dimensionality"
class="title-slide slide level2">
<h2>The Curse of Dimensionality</h2>
<aside class="notes">
<figure>
<img data-src="../images/bishop1-21.png" style="width:55.0%"
alt="Feature space grows exponentially with dimension. From Bishop PRML, Fig. 1-21" />
<figcaption aria-hidden="true">Feature space grows exponentially with
dimension. From Bishop PRML, Fig. 1-21</figcaption>
</figure>
</aside>
</section>
<section id="knn-in-1d" class="slide level3">
<h3>KNN in 1D</h3>
<ul>
<li>Consider a dataset <span class="math inline">\((x_1, y_1), \ldots,
(x_N, y_N), N=100\)</span></li>
<li><span class="math inline">\(x\)</span> is uniformly distributed in
[0,1]</li>
<li>On average, one data point is located every 1/100 units along 1D
feature axis.</li>
<li>To find 3NN, would expect to cover 3/100 of the feature axis.</li>
</ul>
</section>
<section id="knn-in-2d" class="slide level3">
<h3>KNN in 2D</h3>
<ul>
<li>Now consider the same dataset with two features.</li>
<li>Each feature is uniformly distributed in [0,1]</li>
<li>To find 3NN, would expect to cover <span
class="math inline">\(0.03^{\frac{1}{2}}\)</span> of the unit
rectangle.</li>
</ul>
<aside class="notes">
<figure>
<img data-src="../images/5-dimensionality.png" style="width:45.0%"
alt="When d goes from 1 to 2, the density of the training points decreases and we need to cover more of the feature space to find the same number of neighbors." />
<figcaption aria-hidden="true">When <span
class="math inline">\(d\)</span> goes from 1 to 2, the density of the
training points decreases and we need to cover more of the feature space
to find the same number of neighbors.</figcaption>
</figure>
</aside>
</section>
<section id="density-of-samples-decreases-with-dimensions"
class="slide level3">
<h3>Density of samples decreases with dimensions</h3>
<p>To get 3NN,</p>
<ul>
<li>need to cover 3% of space in 1D</li>
<li>need to cover 17% of space in 2D</li>
<li>need to cover 70% of space in 10D. At this point, the nearest
neighbors are not much closer than the rest of the dataset.</li>
</ul>
</section>
<section id="density-of-samples-decreases-with-dimensions---general"
class="slide level3">
<h3>Density of samples decreases with dimensions - general</h3>
<p>The length of the smallest hyper-cube that contains all K-nearest
neighbors of a test point:</p>
<p><span class="math display">\[\left( \frac{K}{N} \right)
^{\frac{1}{d}}\]</span></p>
<p>for <span class="math inline">\(N\)</span> samples with
dimensionality <span class="math inline">\(d\)</span>.</p>
<!--
What happens to this quantity as $d$ increases?

![Image source: [https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote02_kNN.html](https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote02_kNN.html)](../images/curseanimation.gif){ width=60% }

-->
</section>
<section id="solutions-to-the-curse-1" class="slide level3">
<h3>Solutions to the curse (1)</h3>
<p>Add training data?</p>
<p><span
class="math display">\[\left(\frac{K}{N}\right)^{\frac{1}{d}}\]</span></p>
<p>As number of dimensions increases linearly, number of training
samples must increase exponentially to counter the “curse”.</p>
</section>
<section id="solutions-to-the-curse-2" class="slide level3">
<h3>Solutions to the curse (2)</h3>
<p>Reduce <span class="math inline">\(d\)</span>?</p>
<ul>
<li>Feature selection</li>
<li>Dimensionality reduction: a type of unsupervised learning that
<em>transforms</em> high-d data into lower-d data.</li>
</ul>
</section></section>
<section>
<section id="summary-of-nn-method" class="title-slide slide level2">
<h2>Summary of NN method</h2>

</section>
<section id="nn-learning" class="slide level3">
<h3>NN learning</h3>
<p>Learning:</p>
<ul>
<li>Store training data</li>
<li>Don’t do anything else until you have a new point to classify</li>
</ul>
<aside class="notes">
<p>In practice, we will usually store training data in a data structure
that makes it faster to compute nearest neighbors.</p>
</aside>
</section>
<section id="nn-prediction" class="slide level3">
<h3>NN prediction</h3>
<p>Prediction:</p>
<ul>
<li>Find nearest neighbors using distance metric</li>
<li>Classification: predict most frequently occuring class among nearest
neighbors</li>
<li>Regression: predict mean value of nearest neighbors</li>
</ul>
</section>
<section id="the-good-and-the-bad-1" class="slide level3">
<h3>The good and the bad (1)</h3>
<p>Good:</p>
<ul>
<li>Good interpretability</li>
<li>Fast “learning” (<em>memory-based</em>)</li>
<li>Works well in low dimensions for complex decision surfaces</li>
</ul>
</section>
<section id="the-good-and-the-bad-2" class="slide level3">
<h3>The good and the bad (2)</h3>
<p>Neutral:</p>
<ul>
<li>Assumes similar inputs have similar outputs</li>
</ul>
</section>
<section id="the-good-and-the-bad-3" class="slide level3">
<h3>The good and the bad (3)</h3>
<p>Bad:</p>
<ul>
<li>Slow prediction (especially with large N)</li>
<li>Curse of dimensionality</li>
</ul>
</section></section>
    </div>
  </div>

  <script src="reveal.js-master/dist/reveal.js"></script>

  <!-- reveal.js plugins -->
  <script src="reveal.js-master/plugin/notes/notes.js"></script>
  <script src="reveal.js-master/plugin/search/search.js"></script>
  <script src="reveal.js-master/plugin/zoom/zoom.js"></script>
  <script src="reveal.js-master/plugin/math/math.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
        // Display controls in the bottom right corner
        controls: true,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: true,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'bottom-right',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: false,

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: false,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: true,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'default',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: true,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'slide',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'fade',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        math: {
          mathjax: '/usr/share/javascript/mathjax/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [
          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    </body>
</html>
