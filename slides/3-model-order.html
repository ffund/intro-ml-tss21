<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="author" content="Fraida Fund">
  <title>Model selection</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="reveal.js-master/dist/reset.css">
  <link rel="stylesheet" href="reveal.js-master/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <link rel="stylesheet" href="reveal.js-master/dist/theme/white.css" id="theme">
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <h1 class="title">Model selection</h1>
  <p class="author">Fraida Fund</p>
</section>

<section>
<section id="model-selection-problems" class="title-slide slide level2">
<h2>Model selection problems</h2>
<aside class="notes">
<p>Model selection problem: how to select the <span class="math inline">\(f()\)</span> that maps features <span class="math inline">\(X\)</span> to target <span class="math inline">\(y\)</span>?</p>
<p>We’ll look at two examples of model selection problems, but there are many more.</p>
</aside>
</section>
<section id="choosing-model-complexity" class="slide level3">
<h3>Choosing model complexity</h3>
<p>We need to select a model of appropriate complexity -</p>
<ul>
<li>what does that mean, and</li>
<li>how do we select one?</li>
</ul>
</section>
<section id="model-order-selection-problem" class="slide level3">
<h3>Model order selection problem</h3>
<ul>
<li>Given data <span class="math inline">\((x_i, y_i), i=1\cdots,N\)</span> (one feature)</li>
<li>Polynomial model: <span class="math inline">\(\hat{y} = w_0 + w_1 x + \cdots + w_d x^d\)</span></li>
<li><span class="math inline">\(d\)</span> is degree of polynomial, called <strong>model order</strong></li>
<li><strong>Model order selection problem</strong>: choosing <span class="math inline">\(d\)</span></li>
</ul>
</section>
<section id="using-loss-function-for-model-order-selection" class="slide level3">
<h3>Using loss function for model order selection?</h3>
<p>Suppose we would “search” over each possible <span class="math inline">\(d\)</span>:</p>
<ul>
<li>Fit model of order <span class="math inline">\(d\)</span> on training data, get <span class="math inline">\(\mathbf{w}\)</span></li>
<li>Compute predictions on training data</li>
<li>Compute loss function on training data: <span class="math inline">\(MSE = \frac{1}{n}\sum_{i=1}^n (y_i - \hat{y_i})^2\)</span></li>
<li>Select <span class="math inline">\(d\)</span> that minimizes loss</li>
</ul>
<aside class="notes">
<ul>
<li>Problem: loss function always decreasing with <span class="math inline">\(d\)</span> (training error decreases with model complexity!)</li>
</ul>
</aside>
</section>
<section id="feature-selection-problem" class="slide level3">
<h3>Feature selection problem</h3>
<p>Given high dimensional data <span class="math inline">\(\mathbf{X} \in R^{n \times d}\)</span> and target variable <span class="math inline">\(y\)</span>,</p>
<p>Select a subset of <span class="math inline">\(k &lt;&lt; d\)</span> features, <span class="math inline">\(\mathbf{X}_S \in R^{n \times k}\)</span> that is most relevant to target <span class="math inline">\(y\)</span>.</p>
<aside class="notes">
<ul>
<li>Linear model: <span class="math inline">\(\hat{y} = w_0 + w_1 x_1 + \cdots + w_d x_d\)</span></li>
<li>Many features, only some are relevant</li>
<li><strong>Feature selection problem</strong>: fit a model with a small number of features</li>
</ul>
<p>Why use a subset of features?</p>
<ul>
<li>High risk of overfitting if you use all features!</li>
<li>For linear regression, there’s a unique OLS solution only if <span class="math inline">\(n \geq d\)</span></li>
<li>For linear regression, when <span class="math inline">\(N \geq p\)</span>, variance increases linearly with number of parameters, inversely with number of samples. (Not derived in class, but read extra notes posted after class at home.)</li>
</ul>
</aside>
</section></section>
<section>
<section id="validation" class="title-slide slide level2">
<h2>Validation</h2>

</section>
<section id="simple-trainvalidationtest-split" class="slide level3">
<h3>Simple train/validation/test split</h3>
<ul>
<li>Divide data into training, validation, test sets</li>
<li>For each candidate model, learn model parameters on training set</li>
<li>Measure error for all models on validation set</li>
<li>Select model that minimizes error on validation set</li>
<li>Evaluate model on test set</li>
</ul>
<aside class="notes">
<p>Note: sometimes you’ll hear “validation set” and “test set” used according to the reverse meanings.</p>
</aside>
</section>
<section id="simple-trainvalidationtest-algorithm-1" class="slide level3">
<h3>Simple train/validation/test algorithm (1)</h3>
<ul>
<li>Split <span class="math inline">\(X, y\)</span> into training, validation, and test.</li>
<li>Loop over models of increasing complexity: For <span class="math inline">\(p=1,\ldots,p_{max}\)</span>,
<ul>
<li><strong>Fit</strong>: <span class="math inline">\(\hat{w}_p = \text{fit}_p(X_{tr}, y_{tr})\)</span></li>
<li><strong>Predict</strong>: <span class="math inline">\(\hat{y}_{v,p} = \text{pred}(X_{v}, \hat{w}_p)\)</span></li>
<li><strong>Score</strong>: <span class="math inline">\(S_p = \text{score}(y_{v}, \hat{y}_{v,p})\)</span></li>
</ul></li>
</ul>
</section>
<section id="simple-trainvalidationtest-algorithm-1-1" class="slide level3">
<h3>Simple train/validation/test algorithm (1)</h3>
<ul>
<li>Select model order with best score (here, assuming “lower is better”): <span class="math display">\[p^* = \operatorname*{argmin}_p S_p\]</span></li>
<li>Evaluate: <span class="math display">\[S_{p^*} = \text{score}(y_{ts}, \hat{y}_{ts,p^*}), \quad \hat{y}_{ts,p^*} = \text{pred}(X_{ts}, \hat{w}_{p^*})\]</span></li>
</ul>
</section>
<section id="problems-with-simple-split" class="slide level3">
<h3>Problems with simple split</h3>
<aside class="notes">
<ul>
<li>Fitted model (and test error!) varies a lot depending on samples selected for training and validation.</li>
<li>Fewer samples available for estimating parameters.</li>
<li>Especially bad for problems with small number of samples.</li>
</ul>
</aside>
</section>
<section id="k-fold-cross-validation" class="slide level3">
<h3>K-fold cross validation</h3>
<p>Alternative to simple split:</p>
<ul>
<li>Divide data into <span class="math inline">\(K\)</span> equal-sized parts (typically 5, 10)</li>
<li>For each of the “splits”: evaluate model using <span class="math inline">\(K-1\)</span> parts for training, last part for validation</li>
<li>Average the <span class="math inline">\(K\)</span> validation scores and choose based on average</li>
</ul>
</section>
<section id="k-fold-cv-illustrated" class="slide level3">
<h3>K-fold CV illustrated</h3>
<figure>
<img data-src="../images/sklearn_cross_validation.png" style="width:60.0%" alt="K-fold CV" /><figcaption aria-hidden="true">K-fold CV</figcaption>
</figure>
</section>
<section id="k-fold-cv---algorithm-1" class="slide level3">
<h3>K-fold CV - algorithm (1)</h3>
<p><strong>Outer loop</strong> over folds: for <span class="math inline">\(i=1\)</span> to <span class="math inline">\(K\)</span></p>
<ul>
<li><p>Get training and validation sets for fold <span class="math inline">\(i\)</span>:</p></li>
<li><p><strong>Inner loop</strong> over models of increasing complexity: For <span class="math inline">\(p=1\)</span> to <span class="math inline">\(p_{max}\)</span>,</p>
<ul>
<li><strong>Fit</strong>: <span class="math inline">\(\hat{w}_{p,i} = \text{fit}_p(X_{tr_i}, y_{tr_i})\)</span></li>
<li><strong>Predict</strong>: <span class="math inline">\(\hat{y}_{v_i,p} = \text{pred}(X_{v_i}, \hat{w}_{p,i})\)</span></li>
<li><strong>Score</strong>: <span class="math inline">\(S_{p,i} = score(y_{v_i}, \hat{y}_{v_i,p})\)</span></li>
</ul></li>
</ul>
</section>
<section id="k-fold-cv---algorithm-2" class="slide level3">
<h3>K-fold CV - algorithm (2)</h3>
<ul>
<li>Find average score (across <span class="math inline">\(K\)</span> scores) for each model: <span class="math inline">\(\bar{S}_p\)</span></li>
<li>Select model with best <em>average</em> score: <span class="math inline">\(p^* = \operatorname*{argmin}_p \bar{S}_p\)</span></li>
<li>Re-train model on entire training set: <span class="math inline">\(\hat{w}_{p^*} = \text{fit}_p(X_{tr}, y_{tr})\)</span></li>
<li>Evaluate new fitted model on test set</li>
</ul>
<aside class="notes">
<figure>
<img data-src="../images/3-validation-options.png" style="width:100.0%" alt="Summary of approaches. Source." /><figcaption aria-hidden="true">Summary of approaches. <a href="https://sebastianraschka.com/faq/docs/evaluate-a-model.html">Source</a>.</figcaption>
</figure>
</aside>
</section>
<section id="k-fold-cv---how-to-split" class="slide level3">
<h3>K-fold CV - how to split?</h3>
<figure>
<img data-src="../images/3-kfold-variations.png" style="width:75.0%" alt="K-fold CV variations." /><figcaption aria-hidden="true">K-fold CV variations.</figcaption>
</figure>
<aside class="notes">
<p>Selecting the right K-fold CV is very important for avoiding data leakage!</p>
<p>Refer to <a href="https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation">the function documentation</a> for more examples.</p>
</aside>
</section>
<section id="one-standard-error-rule" class="slide level3">
<h3>One standard error rule</h3>
<ul>
<li>Model selection that minimizes mean error often results in too-complex model</li>
<li>One standard error rule: use simplest model where mean error is within one SE of the minimum mean error</li>
</ul>
</section>
<section id="one-standard-error-rule---algorithm-1" class="slide level3">
<h3>One standard error rule - algorithm (1)</h3>
<ul>
<li>Given data <span class="math inline">\(X, y\)</span></li>
<li>Compute score <span class="math inline">\(S_{p,i}\)</span> for model <span class="math inline">\(p\)</span> on fold <span class="math inline">\(i\)</span> (of <span class="math inline">\(K\)</span>)</li>
<li>Compute average (<span class="math inline">\(\bar{S}_p\)</span>), standard deviation <span class="math inline">\(\sigma_p\)</span>, and standard error of scores:</li>
</ul>
<p><span class="math display">\[SE_p = \frac{\sigma_p}{\sqrt{K-1}}\]</span></p>
</section>
<section id="one-standard-error-rule---algorithm-2" class="slide level3">
<h3>One standard error rule - algorithm (2)</h3>
<p>“Best score” model selection: <span class="math inline">\(p^* = \operatorname*{argmin}_p \bar{S}_p\)</span></p>
<p><strong>One SE rule</strong> for “lower is better” scoring metric: Compute target score: <span class="math inline">\(S_t = \bar{S}_{p^*} + SE_{p^*}\)</span></p>
<p>then select simplest model with score lower than target:</p>
<p><span class="math display">\[p^{*,1{\text{SE}}} = \min \{p | \bar{S}_p \leq S_t\}\]</span></p>
<aside class="notes">
<p>Note: this assumes you are using a “smaller is better” metric such as MSE. If you are using a “larger is better” metric, like R2, how would we change the algorithm?</p>
</aside>
</section>
<section id="one-standard-error-rule---algorithm-3" class="slide level3">
<h3>One standard error rule - algorithm (3)</h3>
<p>“Best score” model selection: <span class="math inline">\(p^* = \operatorname*{argmax}_p \bar{S}_p\)</span></p>
<p><strong>One SE rule</strong> for “higher is better” scoring metric: Compute target score: <span class="math inline">\(S_t = \bar{S}_{p^*} - SE_{p^*}\)</span></p>
<p>then select simplest model with score higher than target:</p>
<p><span class="math display">\[p^{*,1{\text{SE}}} = \min \{p | \bar{S}_p \geq S_t\}\]</span></p>
</section></section>
    </div>
  </div>

  <script src="reveal.js-master/dist/reveal.js"></script>

  // reveal.js plugins
  <script src="reveal.js-master/plugin/notes/notes.js"></script>
  <script src="reveal.js-master/plugin/search/search.js"></script>
  <script src="reveal.js-master/plugin/zoom/zoom.js"></script>
  <script src="reveal.js-master/plugin/math/math.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
        // Push each slide change to the browser history
        history: true,
        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [
          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    </body>
</html>
