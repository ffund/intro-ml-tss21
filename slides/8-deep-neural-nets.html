<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="author" content="Fraida Fund">
  <title>Deep neural networks</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="reveal.js-master/dist/reset.css">
  <link rel="stylesheet" href="reveal.js-master/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <link rel="stylesheet" href="reveal.js-master/dist/theme/white.css" id="theme">
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <h1 class="title">Deep neural networks</h1>
  <p class="author">Fraida Fund</p>
</section>

<section id="recap" class="title-slide slide level2">
<h2>Recap</h2>
<p>Last week: neural networks with one hidden layer</p>
<ul>
<li>Hidden layer learns feature representation</li>
<li>Output layer learns classification/regression tasks</li>
</ul>
<aside class="notes">
<p>With the neural network, the “transformed” feature representation is <em>learned</em> instead of specified by the designer.</p>
<figure>
<img data-src="../images/8-deep-learning-motivation.png" style="width:100.0%" alt="Image is based on a figure in Deep learning, by Goodfellow, Bengio, Courville." /><figcaption aria-hidden="true">Image is based on a figure in Deep learning, by Goodfellow, Bengio, Courville.</figcaption>
</figure>
</aside>
</section>

<section>
<section id="deep-neural-networks" class="title-slide slide level2">
<h2>Deep neural networks</h2>
<aside class="notes">
<figure>
<img data-src="../images/8-deep-network.png" style="width:60.0%" alt="Illustration of a deep network, with multiple hidden layers." /><figcaption aria-hidden="true">Illustration of a deep network, with multiple hidden layers.</figcaption>
</figure>
<p>Some comments:</p>
<ul>
<li>each layer is fully connected to the next layer</li>
<li>each unit still works the same way: take the weighted sum of inputs, apply an activation function, and that’s the unit output</li>
<li>still trained by backpropagation</li>
</ul>
<p>We call the number of layers the “depth” of the network and the number of hidden units in a layer its “width.”</p>
</aside>
<!--

Universal approximation theorem: https://cedar.buffalo.edu/~srihari/CSE676/6.4%20ArchitectureDesign.pdf

-->
</section>
<section id="challenges-with-deep-neural-networks" class="slide level3">
<h3>Challenges with deep neural networks</h3>
<ul>
<li>Optimization</li>
<li>Generalization</li>
</ul>
</section></section>
<section>
<section id="optimizing-deep-neural-networks" class="title-slide slide level2">
<h2>Optimizing deep neural networks</h2>

</section>
<section id="loss-landscape" class="slide level3">
<h3>Loss landscape</h3>
<figure>
<img data-src="../images/resnet56_noshort_small.jpg" style="width:40.0%" alt="“Loss landscape” of a deep neural network in a “slice” of the high-dimensional feature space." /><figcaption aria-hidden="true">“Loss landscape” of a deep neural network in a “slice” of the high-dimensional feature space.</figcaption>
</figure>
<aside class="notes">
<p>Image source: Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer and Tom Goldstein. Visualizing the Loss Landscape of Neural Nets. NIPS, 2018.</p>
<p>Neural networks are optimized using backpropagation over the computational graph, where the loss is a very challenging function of <em>all</em> the weights. (Not convex!)</p>
</aside>
</section>
<section id="effective-training-depends-on" class="slide level3">
<h3>Effective training depends on</h3>
<ul>
<li>Activation function</li>
<li>Pre-processing data</li>
<li>Initial weights</li>
<li>Optimizer</li>
</ul>
</section>
<section id="recall-activation-functions" class="slide level3">
<h3>Recall: activation functions</h3>
<figure>
<img data-src="../images/activation-functions.png" style="width:40.0%" alt="Candidate activation functions for a neural network." /><figcaption aria-hidden="true">Candidate activation functions for a neural network.</figcaption>
</figure>
<!--
### Zero-centered outputs

Remember that at each hidden unit, we compute

$\frac{\partial L_n}{\partial w_{j,i}} = \delta_j u_i$ 

where $\delta_j$ is the backpropagation error from the "upstream" nodes.

What happens if the $u_i$ terms are always positive?

::: notes

* To compute the derivative with respect to the weights at the input to a neuron, we compute the "local derivative" and then multiply by the "upstream" "backpropagation error" (scalar).

* The scalar multiplier may be positive or negative.

:::

-->
</section>
<section id="vanishingexploding-gradient" class="slide level3">
<h3>Vanishing/exploding gradient</h3>
<aside class="notes">
<p>What happens when you are in the far left or far right part of the sigmoid?</p>
<ul>
<li>Gradient is close to zero</li>
<li>Weight updates are also close to zero</li>
<li>The “downstream” gradients will also be values close to zero! (Because of backpropagation.)</li>
<li>And, when you multiply quantities close to zero - they get even smaller.</li>
</ul>
<p>When the sigmoid “saturates”, it “kills” the neuron!</p>
<p>(Same issue with tanh.)</p>
<p>(There is also an analagous “exploding gradient” problem when large gradients are propagated back through the network.)</p>
</aside>
</section>
<section id="dead-relu" class="slide level3">
<h3>Dead ReLU</h3>
<p>ReLU is a much better non-linear function:</p>
<ul>
<li>does not saturate in positive region</li>
<li>very very fast to compute</li>
<li>often converges faster than sigmoid/tanh</li>
</ul>
<p>But, can “die” in the negative region.</p>
<aside class="notes">
<p>When input is less than 0, the ReLU (and downstream units) is <em>completely</em> dead (not only very small!)</p>
<p>Alternative: <strong>leaky ReLU</strong> has small (non-zero) gradient in the negative region - won’t die.</p>
<p><span class="math display">\[f(x) = \text{max}(\alpha x, x)\]</span></p>
<p>(<span class="math inline">\(\alpha\)</span> is a hyperparameter.)</p>
<p>Also other variations on this…</p>
</aside>
</section>
<section id="skip-connections" class="slide level3">
<h3>Skip connections</h3>
<ul>
<li>Direct connection between some higher layers and lower layers</li>
<li>A “highway” for gradient info to go directly back to lower layers</li>
</ul>
</section>
<section id="data-pre-processing" class="slide level3">
<h3>Data pre-processing</h3>
<p>You can make the loss surface much “nicer” by pre-processing:</p>
<ul>
<li>Remove mean (zero center)</li>
<li>Normalize (divide by standard deviation)</li>
<li>OR decorrelation (whitening/rotation)</li>
</ul>
<aside class="notes">
<p>There are several reasons why this helps. We already discussed the “ravine” in the loss function that is created by correlated features.</p>
<p>What about zero-centering and normalization? Think about a binary classification problem of a data cloud that is far from the origin, vs. one close to the origin. In which case will the loss function react more (be more sensitive) to a small change in weights?</p>
<figure>
<img data-src="../images/8-pre-processing.png" style="width:30.0%" alt="The classifier on the right is more sensitive to small changes in the weights." /><figcaption aria-hidden="true">The classifier on the right is more sensitive to small changes in the weights.</figcaption>
</figure>
<p>Note: Whitening/decorrelation is not applied to image data. For image data, we sometimes subtract the “mean image” or the per-color mean.</p>
</aside>
</section>
<section id="data-preprocessing-1" class="slide level3">
<h3>Data preprocessing (1)</h3>
<figure>
<img data-src="../images/8-preprocessing-1.jpeg" style="width:50.0%" alt="Image source: Stanford CS231n." /><figcaption aria-hidden="true">Image source: Stanford CS231n.</figcaption>
</figure>
</section>
<section id="data-preprocessing-2" class="slide level3">
<h3>Data preprocessing (2)</h3>
<figure>
<img data-src="../images/8-preprocessing-2.jpeg" style="width:50.0%" alt="Image source: Stanford CS231n." /><figcaption aria-hidden="true">Image source: Stanford CS231n.</figcaption>
</figure>
</section>
<section id="weight-initialization" class="slide level3">
<h3>Weight initialization</h3>
<p>What if we initialize weights to:</p>
<ul>
<li>zero?</li>
<li>a constant (non-zero)?</li>
<li>a normal random value with large <span class="math inline">\(\sigma\)</span>?</li>
<li>a normal random value with small <span class="math inline">\(\sigma\)</span>?</li>
</ul>
<aside class="notes">
<p>Some comments:</p>
<ul>
<li>If weights are all initialized to zero, all the outputs are zero (for any input) - the network won’t learn.</li>
<li>If weights are all initialized to the same constant, we are more prone to “herding” - hidden units all move in the same direction at once, instead of “specializing”.</li>
<li>Large normal random values are bad - you want to be near the non-linear part of the activation function, and avoid exploding gradients.</li>
<li>Small normal random values work well for “shallow” networks, but not for deep networks - it makes the activation function outputs “collapse” toward zero.</li>
</ul>
</aside>
</section>
<section id="desirable-properties-for-initial-weights" class="slide level3">
<h3>Desirable properties for initial weights</h3>
<ul>
<li>The mean of the intial weights should be right in the middle</li>
<li>The variance of the activations should stay the same across every layer (<a href="https://www.deeplearning.ai/ai-notes/initialization/index.html">derivation</a>)</li>
</ul>
<p>Xavier initialization for tanh, He initialization for ReLU.</p>
<aside class="notes">
<p>Xavier scales variance by <span class="math inline">\(\frac{1}{N_{in}}\)</span>, He by <span class="math inline">\(\frac{2}{N_{in}}\)</span> where <span class="math inline">\(N_{in}\)</span> is the number of inputs to the layer.</p>
</aside>
</section>
<section id="desirable-properties---illustration-1" class="slide level3">
<h3>Desirable properties - illustration (1)</h3>
<figure>
<img data-src="../images/8-init-collapse.png" style="width:60.0%" alt="Activation function outputs with normal initialization of weights. Image source: Justin Johnson." /><figcaption aria-hidden="true">Activation function outputs with normal initialization of weights. Image source: Justin Johnson.</figcaption>
</figure>
</section>
<section id="desirable-properties---illustration-2" class="slide level3">
<h3>Desirable properties - illustration (2)</h3>
<figure>
<img data-src="../images/8-init-xavier.png" style="width:60.0%" alt="Activation function outputs with Xavier initialization of weights. Image source: Justin Johnson." /><figcaption aria-hidden="true">Activation function outputs with Xavier initialization of weights. Image source: Justin Johnson.</figcaption>
</figure>
</section></section>
<section>
<section id="optimizers" class="title-slide slide level2">
<h2>Optimizers</h2>

</section>
<section id="standard-batch-gradient-descent" class="slide level3">
<h3>Standard (“batch”) gradient descent</h3>
<p>For each step <span class="math inline">\(t\)</span> along the error curve:</p>
<p><span class="math display">\[W^{t+1} = W^t - \alpha \nabla L(W^t) = W^t - \frac{\alpha}{N} \sum_{i=1}^N \nabla L_i(W^t, \mathbf{x}_i, y_i)\]</span></p>
<p>Repeat until stopping criterion is met.</p>
</section>
<section id="stochastic-gradient-descent" class="slide level3">
<h3>Stochastic gradient descent</h3>
<p>Idea: at each step, compute estimate of gradient using only one randomly selected sample, and move in the direction it indicates.</p>
<p>Many of the steps will be in the wrong direction, but progress towards minimum occurs <em>on average</em>, as long as the steps are small.</p>
<p>Bonus: helps escape local minima.</p>
</section>
<section id="mini-batch-also-stochastic-gradient-descent" class="slide level3">
<h3>Mini-batch (also “stochastic”) gradient descent</h3>
<p>Idea: In each step, select a small subset of training data (“mini-batch”), and evaluate gradient on that mini-batch. Then move in the direction it indicates.</p>
<p>For each step <span class="math inline">\(t\)</span> along the error curve:</p>
<ul>
<li>Select random mini-batch <span class="math inline">\(I_t\subset{1,\ldots,N}\)</span></li>
<li>Compute gradient approximation: <span class="math inline">\(g^t = \frac{1}{|I_t|} \sum_{i\in I_t} \nabla L(\mathbf{x}_i, y_i, W)\)</span></li>
<li>Update parameters: <span class="math inline">\(W^{t+1} = W^t - \alpha^t g^t\)</span></li>
</ul>
</section>
<section id="comparison-batch-size" class="slide level3">
<h3>Comparison: batch size</h3>
<figure>
<img data-src="../images/grad-descent-comparison.png" style="width:50.0%" alt="Effect of batch size on gradient descent." /><figcaption aria-hidden="true">Effect of batch size on gradient descent.</figcaption>
</figure>
<!--

### Why does mini-batch gradient help? (Intuition)

* Standard error of mean over $m$ samples is $\frac{\sigma}{\sqrt{m}}$, where $\sigma$ is standard deviation.
* The benefit of more examples in reducing error is less than linear!
* Example: gradient based on 10,000 samples requires 100x more computation than one based on 100 samples, but reduces SE only 10x.
* Also: memory required scales with mini-batch size.
* Also: there is often redundancy in training set.

-->
</section>
<section id="gradient-descent-terminology" class="slide level3">
<h3>Gradient descent terminology</h3>
<ul>
<li>Mini-batch size is <span class="math inline">\(B\)</span>, training size is <span class="math inline">\(N\)</span></li>
<li>A training <em>epoch</em> is the sequence of updates over which we see all non-overlapping mini-batches</li>
<li>There are <span class="math inline">\(\frac{N}{B}\)</span> steps per training epoch</li>
<li>Data shuffling: at the beginning of each epoch, randomly shuffle training samples. Then, select mini-batches in order from shuffled samples.</li>
</ul>
</section>
<section id="selecting-the-learning-rate" class="slide level3">
<h3>Selecting the learning rate</h3>
<figure>
<img data-src="../images/learning_rate_comparison.png" style="width:55.0%" alt="Choice of learning rate \alpha is critical" /><figcaption aria-hidden="true">Choice of learning rate <span class="math inline">\(\alpha\)</span> is critical</figcaption>
</figure>
</section>
<section id="annealing-the-learning-rate" class="slide level3">
<h3>Annealing the learning rate</h3>
<p>One approach: decay learning rate slowly over time, such as</p>
<ul>
<li>Exponential decay: <span class="math inline">\(\alpha_t = \alpha_0 e^{-k t}\)</span></li>
<li>1/t decay: <span class="math inline">\(\alpha_t = \alpha_0 / (1 + k t )\)</span></li>
</ul>
<p>(where <span class="math inline">\(k\)</span> is tuning parameter).</p>
</section>
<section id="gradient-descent-in-a-ravine-1" class="slide level3">
<h3>Gradient descent in a ravine (1)</h3>
<figure>
<img data-src="../images/ravine-grad-descent.png" style="width:40.0%" alt="Gradient descent path bounces along ridges of ravine, because surface curves much more steeply in direction of w_1." /><figcaption aria-hidden="true">Gradient descent path bounces along ridges of ravine, because surface curves much more steeply in direction of <span class="math inline">\(w_1\)</span>.</figcaption>
</figure>
</section>
<section id="gradient-descent-in-a-ravine-2" class="slide level3">
<h3>Gradient descent in a ravine (2)</h3>
<figure>
<img data-src="../images/ravine-grad-descent2.png" style="width:40.0%" alt="Gradient descent path bounces along ridges of ravine, because surface curves much more steeply in direction of w_1." /><figcaption aria-hidden="true">Gradient descent path bounces along ridges of ravine, because surface curves much more steeply in direction of <span class="math inline">\(w_1\)</span>.</figcaption>
</figure>
</section>
<section id="momentum-1" class="slide level3">
<h3>Momentum (1)</h3>
<ul>
<li>Idea: Update includes a <em>velocity</em> vector <span class="math inline">\(v\)</span>, that accumulates gradient of past steps.</li>
<li>Each update is a linear combination of the gradient and the previous updates.</li>
<li>(Go faster if gradient keeps pointing in the same direction!)</li>
</ul>
</section>
<section id="momentum-2" class="slide level3">
<h3>Momentum (2)</h3>
<p>Classical momentum: for some <span class="math inline">\(0 \leq \gamma_t &lt; 1\)</span>,</p>
<p><span class="math display">\[v_{t+1} = \gamma_t v_t - \alpha_t \nabla L\left(W_t\right)\]</span></p>
<p>so</p>
<p><span class="math display">\[W_{t+1} = W_t + v_{t+1} = W_t  - \alpha_t \nabla L\left(W_t\right) + \gamma_t v_t\]</span></p>
<p>(<span class="math inline">\(\gamma_t\)</span> is often around 0.9, or starts at 0.5 and anneals to 0.99 over many epochs.)</p>
</section>
<section id="momentum-illustrated" class="slide level3">
<h3>Momentum: illustrated</h3>
<figure>
<img data-src="../images/ravine-momentum.png" style="width:50.0%" alt="Momentum dampens oscillations by reinforcing the component along w_2 while canceling out the components along w_1." /><figcaption aria-hidden="true">Momentum dampens oscillations by reinforcing the component along <span class="math inline">\(w_2\)</span> while canceling out the components along <span class="math inline">\(w_1\)</span>.</figcaption>
</figure>
</section>
<section id="rmsprop" class="slide level3">
<h3>RMSProp</h3>
<p>Idea: Track <em>per-parameter</em> EWMA of <em>square</em> of gradient, and use it to adapt learning rate.</p>
<p><span class="math display">\[W_{t+1,i} = W_{t,i} -\frac{\alpha}
{\sqrt {\epsilon + E[g^2]_t }} \nabla L(W_{t,i})\]</span></p>
<p>where</p>
<p><span class="math display">\[E[g^2]_t=(1-\gamma)g^2 + \gamma E[g^2]_{t-1}, \quad g = \nabla J(W_{t,i})\]</span></p>
<aside class="notes">
<p>Weights with recent gradients of large magnitude have smaller learning rate, weights with small recent gradients have larger learning rates.</p>
</aside>
</section>
<section id="rmsprop-illustrated-beales-function" class="slide level3">
<h3>RMSProp: illustrated (Beale’s function)</h3>
<figure>
<img data-src="../images/beale-gradient.gif" style="width:40.0%" alt="Animation credit: Alec Radford. Link to view animation." /><figcaption aria-hidden="true">Animation credit: Alec Radford. <a href="https://imgur.com/a/Hqolp">Link to view animation</a>.</figcaption>
</figure>
<aside class="notes">
<p>Due to the large initial gradient, velocity based techniques shoot off and bounce around, RMSProps proceed more like faster SGD.</p>
</aside>
</section>
<section id="rmsprop-illustrated-long-valley" class="slide level3">
<h3>RMSProp: illustrated (Long valley)</h3>
<figure>
<img data-src="../images/long-valley-gradient.gif" style="width:40.0%" alt="Animation credit: Alec Radford. Link to view animation." /><figcaption aria-hidden="true">Animation credit: Alec Radford. <a href="https://imgur.com/a/Hqolp">Link to view animation</a>.</figcaption>
</figure>
<aside class="notes">
<p>SGD stalls and momentum has oscillations until it builds up velocity in optimization direction. Algorithms that scale step size quickly break symmetry and descend in optimization direction.</p>
</aside>
</section>
<section id="adam-adaptive-moments-estimation-2014" class="slide level3">
<h3>Adam: Adaptive moments estimation (2014)</h3>
<p>Idea: Track the EWMA of <em>both</em> first and second moments of the gradient, <span class="math inline">\(\{m_t, v_t\}\)</span> at each time <span class="math inline">\(t\)</span>.</p>
<p>If <span class="math inline">\(L_t(W)\)</span> is evaluation of loss function on a mini-batch of data at time <span class="math inline">\(t\)</span>,</p>
<p><span class="math display">\[
\begin{aligned}
\{m_t, v_t\}, \mathbb{E}[m_t] \, \, &amp;\approx \, \, \mathbb{E}[\,\nabla \, L_t(W)\,], \mathbb{E}[v_t] \,  \\
                                  \, &amp;\approx \, \, \mathbb{E}\big[\,(\nabla \, L_t(W))^2\,\big]
 \end{aligned}
 \]</span></p>
<p>Scale <span class="math inline">\(\alpha\)</span> by <span class="math inline">\(\frac{m_t}{\sqrt{v_t}}\)</span> at each step.</p>
</section></section>
<section>
<section id="generalization" class="title-slide slide level2">
<h2>Generalization</h2>
<aside class="notes">
<p>Why don’t deep neural networks overfit?</p>
</aside>
</section>
<section id="double-descent-curve" class="slide level3">
<h3>Double descent curve</h3>
<figure>
<img data-src="../images/8-double-descent.png" style="width:100.0%" alt="Double descent curve (left) and realization in a real neural network (right)." /><figcaption aria-hidden="true">Double descent curve (left) and realization in a real neural network (right).</figcaption>
</figure>
<aside class="notes">
<p>Interpolation threshold: where the model is just big enough to fit the training data exactly.</p>
</aside>
</section>
<section id="double-descent-animation" class="slide level3">
<h3>Double descent: animation</h3>
<figure>
<img data-src="../images/8-polynomial-animation.gif" style="width:40.0%" alt="Polynomial model before and after the interpolation threshold. Image source: Boaz Barak, click link to see animation." /><figcaption aria-hidden="true">Polynomial model before and after the interpolation threshold. Image source: <a href="https://windowsontheory.org/2021/01/31/a-blitz-through-classical-statistical-learning-theory/">Boaz Barak, click link to see animation</a>.</figcaption>
</figure>
<aside class="notes">
<p>Explanation (via <a href="https://windowsontheory.org/2021/01/31/a-blitz-through-classical-statistical-learning-theory/">Boaz Barak</a>):</p>
<blockquote>
<p>When <span class="math inline">\(d\)</span> of the model is less than <span class="math inline">\(d_t\)</span> of the polynomial, we are “under-fitting” and will not get good performance. As <span class="math inline">\(d\)</span> increases between <span class="math inline">\(d_t\)</span> and <span class="math inline">\(n\)</span>, we fit more and more of the noise, until for <span class="math inline">\(d=n\)</span> we have a perfect interpolating polynomial that will have perfect training but very poor test performance. When <span class="math inline">\(d\)</span> grows beyond <span class="math inline">\(n\)</span>, more than one polynomial can fit the data, and (under certain conditions) SGD will select the minimal norm one, which will make the interpolation smoother and smoother and actually result in better performance.</p>
</blockquote>
<p>What this means: in practice, we let the network get big (have capacity to learn complicated data representations!) and use other methods to help select a “good” set of weights from all these candidates.</p>
</aside>
</section>
<section id="regularization" class="slide level3">
<h3>Regularization</h3>
<p>As with other models, we can add a penalty on the norm of the weights:</p>
<ul>
<li>L1 penalty</li>
<li>L2 penalty</li>
<li>Combination (ElasticNet)</li>
</ul>
</section>
<section id="early-stopping" class="slide level3">
<h3>Early stopping</h3>
<ul>
<li>Compute validation loss each performance</li>
<li>Stop training when validation loss hasn’t improved in a while</li>
<li>Risk of stopping <em>too</em> early</li>
</ul>
<aside class="notes">
<p>Why does it work? Some ideas:</p>
<ul>
<li>The network is effectively “smaller” when we stop training early, because many units still in linear region of activation.</li>
<li>Earlier layers (which learn simpler features) and late layers (near the output - used for response mapping) converge to their final weights first. See <a href="https://windowsontheory.org/2021/02/17/what-do-deep-networks-learn-and-when-do-they-learn-it/">Boaz Barak</a>.</li>
</ul>
</aside>
<!-- See "Why early stopping works" https://www.cs.toronto.edu/~guerzhoy/411/lec/W05/overfitting_prevent.pdf -->
<!-- See https://windowsontheory.org/2021/02/17/what-do-deep-networks-learn-and-when-do-they-learn-it/ -->
</section>
<section id="dropout" class="slide level3">
<h3>Dropout</h3>
<figure>
<img data-src="../images/8-dropout.jpeg" style="width:40.0%" alt="Dropout networks." /><figcaption aria-hidden="true">Dropout networks.</figcaption>
</figure>
<aside class="notes">
<ul>
<li>During each training step: some portion of neurons are randomly “dropped”.</li>
<li>During each test step: don’t “drop” any neurons, but we need to scale activations by dropout probability</li>
</ul>
<p>Why does it work? Some ideas:</p>
<ul>
<li>Forces some redundancy, makes neurons learn robust representation</li>
<li>Effectively training an ensemble of networks (with shared weights)</li>
</ul>
<p>Alternative: DropConnect zeros weights, instead of neurons.</p>
<p>Note: when you use Dropout layers, you may notice that the validation/test loss seems better than the training loss! Why?</p>
</aside>
</section>
<section id="batch-normalization" class="slide level3">
<h3>Batch normalization</h3>
<ul>
<li>Re-center and re-scale between layers</li>
<li>Training: Mean and standard deviation per training mini-batch</li>
<li>Test: Using fixed statistics</li>
</ul>
<!--
::: notes

Why does it work? Some ideas:

* It introduces some randomness during training?

:::

-->
</section>
<section id="data-augmentation" class="slide level3">
<h3>Data augmentation</h3>
<figure>
<img data-src="../images/cats-augmentation.png" style="width:80.0%" alt="Data augmentation on a cat image." /><figcaption aria-hidden="true">Data augmentation on a cat image.</figcaption>
</figure>
<aside class="notes">
<p>It doesn’t restrict network capacity - but it helps generalization by increasing the size of your training set!</p>
<p>Apply rotation, crops, scales, change contrast, brightness, color… etc. during training.</p>
</aside>
<!--

Neural networks of all types: https://www.asimovinstitute.org/neural-network-zoo/

-->
</section></section>
    </div>
  </div>

  <script src="reveal.js-master/dist/reveal.js"></script>

  // reveal.js plugins
  <script src="reveal.js-master/plugin/notes/notes.js"></script>
  <script src="reveal.js-master/plugin/search/search.js"></script>
  <script src="reveal.js-master/plugin/zoom/zoom.js"></script>
  <script src="reveal.js-master/plugin/math/math.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
        // Push each slide change to the browser history
        history: true,
        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [
          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    </body>
</html>
