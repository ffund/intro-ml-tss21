---
markmap:
  color: '#2980b9'
---

# Finding a "better" optimum

## Dataset

- Get more data
- Data augmentation
- Use related data (transfer learning)
- Use unlabeled data (self-supervised, semi-supervised)

## Architecture/setup

- Activation functions
- Weight initialization
- Convolutional units
- Recurrent units

## Normalization

- Input standardization
- Batch normalization

## Gradient descent

- Adaptive learning rates
- Gradient clipping

## Regularization

- L2 or L1 regularization
- Early stopping
- Dropout


<!-- 
Credit: Sebastian Raschka
https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L10_regularization__slides.pdf 
-->
