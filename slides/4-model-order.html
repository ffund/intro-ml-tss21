<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="author" content="Fraida Fund">
  <title>Model selection</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="reveal.js-master/dist/reset.css">
  <link rel="stylesheet" href="reveal.js-master/dist/reveal.css">
  <style>
    .reveal .sourceCode {  /* see #7635 */
      overflow: visible;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="reveal.js-master/dist/theme/white.css" id="theme">
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <h1 class="title">Model selection</h1>
  <p class="author">Fraida Fund</p>
</section>

<section class="slide level3">

<aside class="notes">
<p><strong>Math prerequisites for this lecture</strong>: None.</p>
</aside>
</section>
<section id="a-supervised-machine-learning-recipe"
class="title-slide slide level2 cell markdown">
<h2>A supervised machine learning “recipe”</h2>
<ul>
<li><em>Step 1</em>: Get labeled data: <span
class="math inline">\((\mathbf{x_i}, y_i), i=1,2,\cdots,N\)</span>.</li>
<li><em>Step 2</em>: Choose a candidate <strong>model</strong> <span
class="math inline">\(f\)</span>: <span class="math inline">\(\hat{y} =
f(x)\)</span>.</li>
<li><em>Step 3</em>: Select a <strong>loss function</strong>.</li>
<li><em>Step 4</em>: Find the model <strong>parameter</strong> values
that minimize the loss function (<strong>training</strong>).</li>
<li><em>Step 5</em>: Use trained model to <strong>predict</strong> <span
class="math inline">\(\hat{y}\)</span> for new samples not used in
training (<strong>inference</strong>).</li>
<li><em>Step 6</em>: Evaluate how well your model
<strong>generalizes</strong>.</li>
</ul>
<aside class="notes">
<figure>
<img data-src="../images/3-validation-testonly.png" style="width:80.0%"
alt="When we have only one model to consider, with no “hyperparameters”." />
<figcaption aria-hidden="true">When we have only one model to consider,
with no “hyperparameters”.</figcaption>
</figure>
</aside>
</section>

<section>
<section id="model-selection-problems" class="title-slide slide level2">
<h2>Model selection problems</h2>
<aside class="notes">
<p>Model selection problem: how to select the <span
class="math inline">\(f()\)</span> that maps features <span
class="math inline">\(X\)</span> to target <span
class="math inline">\(y\)</span>?</p>
<p>We’ll look at a few examples of model selection problems (polynomial
order selection, selecting number of knots and degrees in spline
features, selecting number of features), but there are many more.</p>
</aside>
</section>
<section id="problem-1-polynomial-order-selection-problem"
class="slide level3">
<h3>Problem 1: Polynomial order selection problem</h3>
<ul>
<li>Given data <span class="math inline">\((x_i, y_i),
i=1\cdots,N\)</span> (one feature)</li>
<li>Polynomial model: <span class="math inline">\(\hat{y} = w_0 + w_1 x
+ \cdots + w_d x^d\)</span></li>
<li><span class="math inline">\(d\)</span> is degree of polynomial,
called <strong>model order</strong></li>
<li><strong>Model order selection problem</strong>: choosing <span
class="math inline">\(d\)</span></li>
</ul>
</section>
<section id="using-training-loss-for-polynomial-order-selection"
class="slide level3">
<h3>Using training loss for polynomial order selection?</h3>
<p>Suppose we would “search” over each possible <span
class="math inline">\(d\)</span>:</p>
<ul>
<li>Fit model of order <span class="math inline">\(d\)</span> on
training data, get <span class="math inline">\(\mathbf{w}\)</span></li>
<li>Compute predictions on training data</li>
<li>Compute loss function on training data: <span
class="math inline">\(MSE = \frac{1}{n}\sum_{i=1}^n (y_i -
\hat{y_i})^2\)</span></li>
<li>Select <span class="math inline">\(d\)</span> that minimizes loss on
training set</li>
</ul>
<aside class="notes">
<figure>
<img data-src="../images/3-validation-select.png" style="width:80.0%"
alt="This approach does not work, because the loss function always decreases with d (model will overfit to data, training error decreases with model complexity!)" />
<figcaption aria-hidden="true">This approach does <em>not</em> work,
because the loss function always decreases with <span
class="math inline">\(d\)</span> (model will overfit to data, training
error decreases with model complexity!)</figcaption>
</figure>
<p>Note that we shouldn’t use the test data to select a model either -
the test set must be left as an “unused” data set on which to evaluate
how well the model generalizes.</p>
</aside>
</section>
<section id="recap-spline-features-1" class="slide level3">
<h3>Recap: Spline features (1)</h3>
<p>Polynomial models of high <span class="math inline">\(d\)</span> are
actually bad, usually -</p>
<figure>
<img data-src="../images/4-polynomial-regressions-one-plot.png"
style="width:80.0%"
alt="Polynomial model - note the boundary behavior. (Image source)" />
<figcaption aria-hidden="true">Polynomial model - note the boundary
behavior. (<a
href="https://madrury.github.io/jekyll/update/statistics/2017/08/04/basis-expansions.html">Image
source</a>)</figcaption>
</figure>
<aside class="notes">
<ul>
<li>tends to get kind of weird at the boundaries of the data (Runge’s
phenomenon)</li>
<li>really bad if you need to extrapolate past the range of the training
data</li>
<li>acts <em>globally</em> when different regions of the data might have
different behavior</li>
</ul>
</aside>
</section>
<section id="recap-spline-features-2" class="slide level3">
<h3>Recap: Spline features (2)</h3>
<p>Instead, we tend to prefer lower-<span
class="math inline">\(d\)</span> piecewise functions, so we can fit
<em>local</em> behavior:</p>
<figure>
<img data-src="../images/4-piecewise.png" style="width:50.0%"
alt="The blue line is a true function we are trying to approximate. The black lines are piecewise polynomials of increasing order. (Image source)" />
<figcaption aria-hidden="true">The blue line is a true function we are
trying to approximate. The black lines are piecewise polynomials of
increasing order. (<a
href="https://bayesiancomputationbook.com/markdown/chp_05.html">Image
source</a>)</figcaption>
</figure>
<aside class="notes">
<p>The feature axis is divided into breakpoints - we call each one a
“knot” - and then we define basis functions that are equal to a
polynomial function of the feature between two knots.</p>
<p>If we constrain the piecewise function to meet at the knots, we call
these splines - basis splines or “B splines”.</p>
</aside>
</section>
<section id="recap-spline-features-3" class="slide level3">
<h3>Recap: Spline features (3)</h3>
<p>For constant functions (degree 0) - given “knots” at positions <span
class="math inline">\(k_t, k_{t+1}\)</span>:</p>
<p><span class="math display">\[
\phi_{t,0}(x_{i,j}) = \begin{cases}
1, \quad  k_t \leq x &lt; k_{t+1} \\
0, \quad  \text{otherwise}
\end{cases}
\]</span></p>
</section>
<section id="recap-spline-features-4" class="slide level3">
<h3>Recap: Spline features (4)</h3>
<p>For degree <span class="math inline">\(p&gt;0\)</span>, defined
recursively:</p>
<p><span class="math display">\[
\phi_{t, p}( x ) := \dfrac{ x - k_t }{k_{t+p} - k_t} \phi_{t,p-1}( x ) +
\dfrac{k_{t+p+1} - x }{k_{t+p+1} - k_{t+1}} \phi_{t+1,p-1}( x )
\]</span></p>
<aside class="notes">
<p>You won’t need to compute this yourself - use <a
href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.SplineTransformer.html"><code>SplineTransformer</code></a>
in <code>sklearn</code>.</p>
</aside>
</section>
<section
id="problem-2-selecting-number-of-knots-degree-for-spline-features"
class="slide level3">
<h3>Problem 2: Selecting number of knots, degree for spline
features</h3>
<figure>
<img data-src="../images/4-bins-various-n-cuts.png" style="width:80.0%"
alt="Increasing number of knots makes the model more complex. (Image source.)" />
<figcaption aria-hidden="true">Increasing number of knots makes the
model more complex. (<a
href="https://madrury.github.io/jekyll/update/statistics/2017/08/04/basis-expansions.html">Image
source</a>.)</figcaption>
</figure>
<aside class="notes">
<p>Now we have two “knobs” for tuning model complexity:</p>
<ul>
<li>the degree of the splines,</li>
<li>and the number of knots!</li>
</ul>
<p>The number of features will be: number of knots + degree - 1</p>
<p>We have the same problem as before:</p>
<ul>
<li>if we select the number of knots, degree to minimize error on the
training set - we will always pick the model with the most knots,
highest degree</li>
<li>if we choose the model that minimizes error on the test set, we
don’t have a held-out set on which to evaluate our model on new, unseen
data. (If we use the test data for model selection, then again for
evaluation, we will have an overly optimistic evaluation of model
performance on unseen data.)</li>
</ul>
</aside>
</section>
<section id="problem-3-feature-selection-1" class="slide level3">
<h3>Problem 3: Feature selection (1)</h3>
<p>Given high dimensional data <span class="math inline">\(\mathbf{X}
\in R^{n \times d}\)</span> and target variable <span
class="math inline">\(y\)</span>,</p>
<p>Linear model: <span class="math inline">\(\hat{y} = w_0 +
\sum_{j=1}^d w_j x_j\)</span></p>
<aside class="notes">
<ul>
<li>Many features, only some are relevant (you don’t know which, or how
many!)</li>
<li><strong>Feature selection problem</strong>: fit a model with a small
number of features</li>
</ul>
</aside>
</section>
<section id="problem-3-feature-selection-2" class="slide level3">
<h3>Problem 3: Feature selection (2)</h3>
<p>Select a subset of <span class="math inline">\(k &lt;&lt; d\)</span>
features, <span class="math inline">\(\mathbf{X}_S \in R^{n \times
k}\)</span> that is most relevant to target <span
class="math inline">\(y\)</span>.</p>
<p>Linear model: <span class="math inline">\(\hat{y} = w_0 + \sum_{x \in
\mathbf{X}_S} w_j x_j\)</span></p>
<aside class="notes">
<p>Why use a subset of features?</p>
<ul>
<li>High risk of overfitting if you use all features!</li>
<li>For linear regression, when <span class="math inline">\(N \geq
p\)</span>, variance increases linearly with number of parameters,
inversely with number of samples. (Refer to extra notes posted after
class at home.)</li>
</ul>
<p>Today we consider the challenge of selecting the number of features
<span class="math inline">\(k\)</span>, in a future lesson we will
discuss how to decide <span class="math inline">\(which\)</span>
features to include.</p>
<p>Once again:</p>
<ul>
<li>if we select the number of features to minimize error on the
training set - we will always pick the model with the most features. Our
model will happily overfit to the “noise” of irrelevant features!</li>
<li>and we also shouldn’t use our test set.</li>
</ul>
</aside>
</section></section>
<section>
<section id="validation" class="title-slide slide level2">
<h2>Validation</h2>
<aside class="notes">
<p>We will discuss a few types of validation:</p>
<ul>
<li>Hold-out validation</li>
<li>K-fold cross validation</li>
<li>Leave-p-out validation</li>
</ul>
</aside>
</section>
<section id="hold-out-validation-1" class="slide level3">
<h3>Hold-out validation (1)</h3>
<ul>
<li>Divide data into training, validation, test sets</li>
<li>For each candidate model, learn model parameters on training
set</li>
<li>Measure error for all models on validation set</li>
<li>Select model that minimizes error on validation set</li>
<li>Evaluate <em>that</em> model on test set</li>
</ul>
<aside class="notes">
<figure>
<img data-src="../images/3-validation-single.png" style="width:80.0%"
alt="Model selection with a validation set." />
<figcaption aria-hidden="true">Model selection with a validation
set.</figcaption>
</figure>
<p>Note: sometimes you’ll hear “validation set” and “test set” used
according to the reverse meanings.</p>
</aside>
</section>
<section id="hold-out-validation-2" class="slide level3">
<h3>Hold-out validation (2)</h3>
<ul>
<li>Split <span class="math inline">\(X, y\)</span> into training,
validation, and test.</li>
<li>Loop over models of increasing complexity: For <span
class="math inline">\(p=1,\ldots,p_{max}\)</span>,
<ul>
<li><strong>Fit</strong>: <span class="math inline">\(\hat{w}_p =
\text{fit}_p(X_{tr}, y_{tr})\)</span></li>
<li><strong>Predict</strong>: <span class="math inline">\(\hat{y}_{v,p}
= \text{pred}(X_{v}, \hat{w}_p)\)</span></li>
<li><strong>Score</strong>: <span class="math inline">\(S_p =
\text{score}(y_{v}, \hat{y}_{v,p})\)</span></li>
</ul></li>
</ul>
</section>
<section id="hold-out-validation-3" class="slide level3">
<h3>Hold-out validation (3)</h3>
<ul>
<li>Select model order with best score (here, assuming “lower is
better”): <span class="math display">\[p^* = \operatorname*{argmin}_p
S_p\]</span></li>
<li>Evaluate: <span class="math display">\[S_{p^*} =
\text{score}(y_{ts}, \hat{y}_{ts,p^*}), \quad \hat{y}_{ts,p^*} =
\text{pred}(X_{ts}, \hat{w}_{p^*})\]</span></li>
</ul>
</section>
<section id="problems-with-hold-out-validation" class="slide level3">
<h3>Problems with hold-out validation</h3>
<aside class="notes">
<ul>
<li>Fitted model (and test error!) varies a lot depending on samples
selected for training and validation.</li>
<li>Fewer samples available for estimating parameters.</li>
<li>Especially bad for problems with small number of samples.</li>
</ul>
</aside>
</section>
<section id="k-fold-cross-validation" class="slide level3">
<h3>K-fold cross validation</h3>
<p>Alternative to single split:</p>
<ul>
<li>Divide data into <span class="math inline">\(K\)</span> equal-sized
parts (typically 5, 10)</li>
<li>For each of the “splits”: evaluate model using <span
class="math inline">\(K-1\)</span> parts for training, last part for
validation</li>
<li>Average the <span class="math inline">\(K\)</span> validation scores
and choose based on average</li>
</ul>
<aside class="notes">
<figure>
<img data-src="../images/3-validation-kfold.png" style="width:80.0%"
alt="K-fold CV for model selection." />
<figcaption aria-hidden="true">K-fold CV for model
selection.</figcaption>
</figure>
</aside>
</section>
<section id="k-fold-cv---algorithm-1" class="slide level3">
<h3>K-fold CV - algorithm (1)</h3>
<p><strong>Outer loop</strong> over folds: for <span
class="math inline">\(i=1\)</span> to <span
class="math inline">\(K\)</span></p>
<ul>
<li><p>Get training and validation sets for fold <span
class="math inline">\(i\)</span>:</p></li>
<li><p><strong>Inner loop</strong> over models of increasing complexity:
For <span class="math inline">\(p=1\)</span> to <span
class="math inline">\(p_{max}\)</span>,</p>
<ul>
<li><strong>Fit</strong>: <span class="math inline">\(\hat{w}_{p,i} =
\text{fit}_p(X_{tr_i}, y_{tr_i})\)</span></li>
<li><strong>Predict</strong>: <span
class="math inline">\(\hat{y}_{v_i,p} = \text{pred}(X_{v_i},
\hat{w}_{p,i})\)</span></li>
<li><strong>Score</strong>: <span class="math inline">\(S_{p,i} =
\text{score}(y_{v_i}, \hat{y}_{v_i,p})\)</span></li>
</ul></li>
</ul>
</section>
<section id="k-fold-cv---algorithm-2" class="slide level3">
<h3>K-fold CV - algorithm (2)</h3>
<ul>
<li>Find average score (across <span class="math inline">\(K\)</span>
scores) for each model: <span
class="math inline">\(\bar{S}_p\)</span></li>
<li>Select model with best <em>average</em> score: <span
class="math inline">\(p^* = \operatorname*{argmin}_p
\bar{S}_p\)</span></li>
<li>Re-train model on entire training set: <span
class="math inline">\(\hat{w}_{p^*} = \text{fit}_p(X_{tr},
y_{tr})\)</span></li>
<li>Evaluate new fitted model on test set</li>
</ul>
<aside class="notes">
<figure>
<img data-src="../images/3-validation-options.png" style="width:100.0%"
alt="Summary of approaches. Source." />
<figcaption aria-hidden="true">Summary of approaches. <a
href="https://sebastianraschka.com/faq/docs/evaluate-a-model.html">Source</a>.</figcaption>
</figure>
</aside>
</section>
<section id="leave-p-out-cv" class="slide level3">
<h3>Leave-p-out CV</h3>
<ul>
<li>In each iteration, <span class="math inline">\(p\)</span> validation
points</li>
<li>Remaining <span class="math inline">\(n-p\)</span> points are for
training</li>
<li>Repeat for <em>all</em> possible sets of <span
class="math inline">\(p\)</span> validation points</li>
</ul>
<aside class="notes">
<p>This is <em>not</em> like K-fold CV which uses non-overlapping
validation sets (they are only the same for <span
class="math inline">\(p=1\)</span>)!</p>
</aside>
</section>
<section id="computation-leave-p-out-cv" class="slide level3">
<h3>Computation (leave-p-out CV)</h3>
<p><span class="math inline">\({n \choose p}\)</span> iterations, in
each:</p>
<ul>
<li>train on <span class="math inline">\(n-p\)</span> samples</li>
<li>score on <span class="math inline">\(p\)</span> samples</li>
</ul>
<aside class="notes">
<p>Usually, this is too expensive - but sometimes LOO CV can be a good
match to the model (KNN).</p>
</aside>
</section>
<section id="computation-k-fold-cv" class="slide level3">
<h3>Computation (K-fold CV)</h3>
<p>K iterations, in each:</p>
<ul>
<li>train on <span class="math inline">\(n-n/k\)</span> samples</li>
<li>score on <span class="math inline">\(n/k\)</span> samples</li>
</ul>
</section>
<section id="k-fold-cv---how-to-split" class="slide level3">
<h3>K-fold CV - how to split?</h3>
<figure>
<img data-src="../images/3-kfold-variations.png" style="width:65.0%"
alt="K-fold CV variations." />
<figcaption aria-hidden="true">K-fold CV variations.</figcaption>
</figure>
<aside class="notes">
<p>Selecting the right K-fold CV is very important for avoiding data
leakage! (Also for training/test split.)</p>
<ul>
<li>if there is no structure in the data - shuffle split (avoid
accidental patterns)</li>
<li>if there is group structure - use a split that keeps members of each
group in either training set, or validation set, but not both</li>
<li>for time series data - use a split that keeps validation data in the
future, relative to training data</li>
</ul>
<p>Think about the task that the model will be asked to do in
“production,” relative to the data it is trained on! Refer to <a
href="https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation">the
function documentation</a> for more examples.</p>
<figure>
<img data-src="../images/4-shuffle-split.png" style="width:60.0%"
alt="Example 1: The data is not homogeneous with respect to sample index, so splitting data data as shown on left would be a very bad idea - the training, validation, and test sets would not be similar! Instead, we should shuffle the indices before splitting the data, as shown on right." />
<figcaption aria-hidden="true">Example 1: The data is not homogeneous
with respect to sample index, so splitting data data as shown on left
would be a very bad idea - the training, validation, and test sets would
not be similar! Instead, we should shuffle the indices before splitting
the data, as shown on right.</figcaption>
</figure>
<figure>
<img data-src="../images/4-shuffle-group.png" style="width:60.0%"
alt="Example 2: The split on the left seems OK, unless (as shown on the right), each person contributes several samples to the dataset, and the value of y is similar for different samples from the same person. This is an example of data leakage. The model is learning from data from an individual, then it is validated and evaluated on data from the same individual - but in production, the model is expected to make predictions about individuals it has never seen. The training, validation, and evaluation process will have overly optimistic performance compared to production (and the model may overfit)." />
<figcaption aria-hidden="true">Example 2: The split on the left seems
OK, unless (as shown on the right), each person contributes several
samples to the dataset, and the value of <span
class="math inline">\(y\)</span> is similar for different samples from
the same person. This is an example of data leakage. The model is
learning from data from an individual, then it is validated and
evaluated on data from the same individual - but in production, the
model is expected to make predictions about individuals it has never
seen. The training, validation, and evaluation process will have overly
optimistic performance compared to production (and the model may
overfit).</figcaption>
</figure>
<figure>
<img data-src="../images/4-fold-group.png" style="width:74.0%"
alt="Example 2 - continued: Instead, we should make sure that each person is only in one type of “set” at a time (e.g. with GroupKFoldCV or equivalent)." />
<figcaption aria-hidden="true">Example 2 - continued: Instead, we should
make sure that each person is <em>only</em> in one type of “set” at a
time (e.g. with GroupKFoldCV or equivalent).</figcaption>
</figure>
<figure>
<img data-src="../images/4-bad-split-timeseries.png" style="width:77.0%"
alt="Example 3: if we would split this time series data as shown on the left, we would get overly optimistic performance in training/validation/evaluation, but then much worse error in production! (This is also an example of data leakage: the model learns from future data, and from adjacent data points, in training - but that data is not available during production.)" />
<figcaption aria-hidden="true">Example 3: if we would split this time
series data as shown on the left, we would get overly optimistic
performance in training/validation/evaluation, but then much worse error
in production! (This is also an example of data leakage: the model
learns from future data, and from adjacent data points, in training -
but that data is not available during production.)</figcaption>
</figure>
<figure>
<img data-src="../images/4-best-split-timeseries.png"
style="width:77.0%"
alt="A better way would be to train and validate like this (example shown is 3-fold CV)." />
<figcaption aria-hidden="true">A better way would be to train and
validate like this (example shown is 3-fold CV).</figcaption>
</figure>
</aside>
</section></section>
<section>
<section id="one-standard-error-rule" class="title-slide slide level2">
<h2>One standard error rule</h2>
<ul>
<li>Model selection that minimizes mean error often results in
too-complex model</li>
<li>One standard error rule: use simplest model where mean error is
within one SE of the minimum mean error</li>
</ul>
</section>
<section id="one-standard-error-rule---algorithm-1"
class="slide level3">
<h3>One standard error rule - algorithm (1)</h3>
<ul>
<li>Given data <span class="math inline">\(X, y\)</span></li>
<li>Compute score <span class="math inline">\(S_{p,i}\)</span> for model
<span class="math inline">\(p\)</span> on fold <span
class="math inline">\(i\)</span> (of <span
class="math inline">\(K\)</span>)</li>
<li>Compute average (<span class="math inline">\(\bar{S}_p\)</span>),
standard deviation <span class="math inline">\(\sigma_p\)</span>, and
standard error of scores:</li>
</ul>
<p><span class="math display">\[SE_p =
\frac{\sigma_p}{\sqrt{K-1}}\]</span></p>
</section>
<section id="one-standard-error-rule---algorithm-2"
class="slide level3">
<h3>One standard error rule - algorithm (2)</h3>
<p>“Best score” model selection: <span class="math inline">\(p^* =
\operatorname*{argmin}_p \bar{S}_p\)</span></p>
<p><strong>One SE rule</strong> for “lower is better” scoring metric:
Compute target score: <span class="math inline">\(S_t = \bar{S}_{p^*} +
SE_{p^*}\)</span></p>
<p>then select simplest model with score lower than target:</p>
<p><span class="math display">\[p^{*,1{\text{SE}}} = \min \{p |
\bar{S}_p \leq S_t\}\]</span></p>
<aside class="notes">
<figure>
<img data-src="../images/4-one-se-mse.png" style="width:60.0%"
alt="Model selection using one SE rule on MSE. The best scoring model is d=8, but d=6 is simplest model within one SE of the best scoring model, and so d=6 would be selected according to the one-SE rule." />
<figcaption aria-hidden="true">Model selection using one SE rule on MSE.
The best scoring model is <span class="math inline">\(d=8\)</span>, but
<span class="math inline">\(d=6\)</span> is simplest model within one SE
of the best scoring model, and so <span
class="math inline">\(d=6\)</span> would be selected according to the
one-SE rule.</figcaption>
</figure>
<p>Note: this assumes you are using a “smaller is better” metric such as
MSE. If you are using a “larger is better” metric, like R2, how would we
change the algorithm?</p>
</aside>
</section>
<section id="one-standard-error-rule---algorithm-3"
class="slide level3">
<h3>One standard error rule - algorithm (3)</h3>
<p>“Best score” model selection: <span class="math inline">\(p^* =
\operatorname*{argmax}_p \bar{S}_p\)</span></p>
<p><strong>One SE rule</strong> for “higher is better” scoring metric:
Compute target score: <span class="math inline">\(S_t = \bar{S}_{p^*} -
SE_{p^*}\)</span></p>
<p>then select simplest model with score higher than target:</p>
<p><span class="math display">\[p^{*,1{\text{SE}}} = \min \{p |
\bar{S}_p \geq S_t\}\]</span></p>
<aside class="notes">
<figure>
<img data-src="../images/4-one-se-r2.png" style="width:60.0%"
alt="Model selection using one SE rule on R2. In this example, the best scoring model is d=2, and there is no simpler model within one SE, so the one-SE rule would also select d=2." />
<figcaption aria-hidden="true">Model selection using one SE rule on R2.
In this example, the best scoring model is <span
class="math inline">\(d=2\)</span>, and there is no simpler model within
one SE, so the one-SE rule would also select <span
class="math inline">\(d=2\)</span>.</figcaption>
</figure>
</aside>
</section></section>
<section id="placing-computation"
class="title-slide slide level2 cell markdown">
<h2>Placing computation</h2>

</section>

<section id="placement-options" class="slide level3 cell markdown">
<h3>Placement options</h3>
<p>Any “step” could be placed:</p>
<ul>
<li>before the train/test split</li>
<li>after the split, outside loop</li>
<li>in the first (outer) loop</li>
<li>in the second (inner) loop</li>
</ul>
<aside class="notes">
<p>We want to place each “step” in the appropriate position to minimize
the computation required, but also in order to use the split
effectively! In placing a “step”, we need to ask ourselves:</p>
<ul>
<li>does the result of this computation depend on the train/test
split?</li>
<li>does the result of this computation depend on the first loop
variable?</li>
<li>does the result of this computation depend on the second loop
variable?</li>
</ul>
<p>Note: the order of the loops (first loop over models, then over
splits; or first loop over splits, then over models) is up to us - we
can select the order that is most efficient for computation.</p>
<figure>
<img data-src="../images/4-kfold-loop-order.png" style="width:60.0%"
alt="Possible arrangements of inner vs outer loop." />
<figcaption aria-hidden="true">Possible arrangements of inner vs outer
loop.</figcaption>
</figure>
<p>Data pre-processing should be considered part of model training, so
steps where the value after pre-processing depends on the data, should
use <em>only</em> the training data. For example -</p>
<ul>
<li>filling missing values with some statistic from the data (mean,
median, max, etc.)</li>
<li>standardizing (removing mean and scaling to unit variance) or other
scaling</li>
</ul>
</aside>
</section>
<section id="example-design-matrix-for-n-way-interactions"
class="slide level3">
<h3>Example: design matrix for n-way interactions</h3>
<p>Suppose we want to evaluate models for increasing <span
class="math inline">\(n\)</span>, where the model of <span
class="math inline">\(n\)</span> includes <span
class="math inline">\(n\)</span>-way interaction features. For
example:</p>
<ul>
<li>Model 1: <span class="math inline">\(x_1, x_2, x_3\)</span></li>
<li>Model 2: <span class="math inline">\(x_1, x_2, x_3, x_1 \times x_2,
x_1 \times x_3, x_2 \times x_3\)</span></li>
<li>Model 3: <span class="math inline">\(x_1, x_2, x_3, x_1 \times x_2,
x_1 \times x_3, x_2 \times x_3, x_1 \times x_2 \times x_3\)</span></li>
</ul>
<aside class="notes">
<p>If we place the computation of the interaction features in the
innermost loop, we will compute the same values repeatedly - and we
don’t need to! We can actually compute the entire design matrix
<em>outside</em> the entire K-fold CV, and inside the K-fold CV -</p>
<ul>
<li>select from this overall matrix, the columns corresponding to the
current model</li>
<li>select from this overall matrix, the rows corresponding to the
training/validation data for this split</li>
</ul>
<figure>
<img data-src="../images/4-kfold-slicing.png" style="width:40.0%"
alt="Slicing rows and columns from an “overall” matrix." />
<figcaption aria-hidden="true">Slicing rows and columns from an
“overall” matrix.</figcaption>
</figure>
</aside>
</section>
<section id="example-design-matrix-for-models-with-spline-features"
class="slide level3">
<h3>Example: design matrix for models with spline features</h3>
<p>Suppose we want to evaluate models on a spline transformation of the
data, with a fixed degree (e.g. <span
class="math inline">\(d=2\)</span>) and an increasing number of “knots”.
For example:</p>
<ul>
<li>Model 1: 3 knots</li>
<li>Model 2: 4 knots</li>
<li>Model 3: 5 knots</li>
</ul>
<p>(we will specify the positions of the knots ourselves, rather than
having them be inferred from the data.)</p>
<aside class="notes">
<p>In this example, we cannot put the computation of the spline features
outside the K-fold CV - <em>all</em> the values in the “transformed”
dataset are different in each model. (Unlike the previous example, there
is no “repetition” of features from one model to the next.)</p>
<p>However, we can consider two <em>valid</em> ways to place the
computation of spline features:</p>
<figure>
<img data-src="../images/4-kfold-spline.png" style="width:60.0%"
alt="Either of these are valid ways to compute the spline features." />
<figcaption aria-hidden="true">Either of these are valid ways to compute
the spline features.</figcaption>
</figure>
<ul>
<li>In the first (left) case, however, we re-compute the spline features
repeatedly for the same samples - we don’t need to!</li>
<li>Instead, we should use the second (right) loop order, and then in
the inner loop, just select the rows corresponding to training and
validation from the design matrix <em>for the given model</em>.</li>
</ul>
</aside>
<!-- 

* pre-processing: fill missing values with median?
* compute design matrix: n-way interactions?
* compute design matrix: splines with increasing number of knots?

-->
</section>
    </div>
  </div>

  <script src="reveal.js-master/dist/reveal.js"></script>

  <!-- reveal.js plugins -->
  <script src="reveal.js-master/plugin/notes/notes.js"></script>
  <script src="reveal.js-master/plugin/search/search.js"></script>
  <script src="reveal.js-master/plugin/zoom/zoom.js"></script>
  <script src="reveal.js-master/plugin/math/math.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
        // Display controls in the bottom right corner
        controls: true,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: true,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'bottom-right',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: false,

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: false,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: true,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'default',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: true,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'slide',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'fade',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [
          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    </body>
</html>
