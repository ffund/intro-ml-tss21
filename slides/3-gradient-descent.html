<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="author" content="Fraida Fund">
  <title>Gradient descent</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="reveal.js-master/dist/reset.css">
  <link rel="stylesheet" href="reveal.js-master/dist/reveal.css">
  <style>
    .reveal .sourceCode {  /* see #7635 */
      overflow: visible;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="reveal.js-master/dist/theme/white.css" id="theme">
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <h1 class="title">Gradient descent</h1>
  <p class="author">Fraida Fund</p>
</section>

<section class="slide level3">

<aside class="notes">
<p><strong>Math prerequisites for this lecture</strong>: You should know
about:</p>
<ul>
<li>derivatives and optimization (Appendix C in Boyd and
Vandenberghe)</li>
<li>complexity of algorithms and especially of vector and matrix
operations (Appendix B in Boyd and Vandenberghe, also the complexity
part of Section I, Chapter 1 and Section II, Chapter 5)</li>
</ul>
</aside>
</section>
<section id="in-this-lecture" class="title-slide slide level2">
<h2>In this lecture</h2>
<p>Addresses “How do we train the model efficiently?”</p>
<ul>
<li>Runtime of OLS solution for multiple/LBF regression</li>
<li>Solution using gradient descent</li>
<li>Variations on main idea</li>
</ul>
</section>

<section>
<section id="runtime-of-ols-solution" class="title-slide slide level2">
<h2>Runtime of OLS solution</h2>

</section>
<section id="limitations-of-ols-solution" class="slide level3">
<h3>Limitations of OLS solution</h3>
<ul>
<li>Specific to linear regression, L2 loss</li>
<li>For extremely large datasets: runtime, memory</li>
</ul>
</section>
<section id="background-big-o-notation" class="slide level3">
<h3>Background: Big O notation</h3>
<p>Approximate the number of operations required, as a function of input
size.</p>
<ul>
<li>Ignore constant terms, constant factors</li>
<li>Ignore all but the dominant term</li>
</ul>
<p>Example: <span class="math inline">\(3n^3 + 100n^2 + 1000\)</span>
would be <span class="math inline">\(O(n^3)\)</span>.</p>
</section>
<section id="computing-ols-solution" class="slide level3">
<h3>Computing OLS solution</h3>
<p>How long does it take to compute</p>
<p><span class="math display">\[\mathbf{w^*} = \left(\Phi^T \Phi
\right)^{-1} \Phi^T \mathbf{y}\]</span></p>
<p>where <span class="math inline">\(\Phi\)</span> is an <span
class="math inline">\(n \times d\)</span> matrix?</p>
<aside class="notes">
<p>Runtime of a “naive” solution using “standard” matrix
multiplication:</p>
<ul>
<li><span class="math inline">\(O(d^2n)\)</span> to multiply <span
class="math inline">\(\Phi^T \Phi\)</span></li>
<li><span class="math inline">\(O(dn)\)</span> to multiply <span
class="math inline">\(\Phi^T y\)</span></li>
<li><span class="math inline">\(O(d^3)\)</span> to compute the inverse
of <span class="math inline">\(\Phi^T \Phi\)</span> (Note: in practice,
we can do it a bit faster.)</li>
</ul>
<p>Since <span class="math inline">\(n\)</span> is generally much larger
than <span class="math inline">\(d\)</span>, the first term dominates
and the runtime is <span class="math inline">\(O(d^2n)\)</span>.</p>
<figure>
<img data-src="../images/3-gd-ols-compute.png" style="width:60.0%"
alt="Computing \Phi^T \Phi. For each entry in the matrix, we need n multiplications, and then we need to fill in d \times d entries to complete the matrix." />
<figcaption aria-hidden="true">Computing <span
class="math inline">\(\Phi^T \Phi\)</span>. For each entry in the
matrix, we need <span class="math inline">\(n\)</span> multiplications,
and then we need to fill in <span class="math inline">\(d \times
d\)</span> entries to complete the matrix.</figcaption>
</figure>
</aside>
</section></section>
<section>
<section id="solution-using-gradient-descent"
class="title-slide slide level2">
<h2>Solution using gradient descent</h2>

</section>
<section id="iterative-solution" class="slide level3">
<h3>Iterative solution</h3>
<p>Suppose we would start with all-zero or random weights. Then
iteratively (for <span class="math inline">\(t\)</span> rounds):</p>
<ul>
<li>pick random weights</li>
<li>if loss performance is better, keep those weights</li>
<li>if loss performance is worse, discard them</li>
</ul>
<aside class="notes">
<p>For infinite <span class="math inline">\(t\)</span>, we’d eventually
find optimal weights - but clearly we could do better.</p>
</aside>
</section>
<section id="background-gradients-and-optimization"
class="slide level3">
<h3>Background: Gradients and optimization</h3>
<p>Gradient has <em>two</em> important properties for optimization:</p>
<p>At a minima (or maxima, or saddle point),</p>
<p><span class="math display">\[\nabla L(\mathbf{w}) = 0\]</span></p>
<p>At other points, <span class="math inline">\(\nabla
L(\mathbf{w})\)</span> points towards direction of maximum
(infinitesimal) rate of <em>increase</em>.</p>
</section>
<section id="gradient-descent-idea" class="slide level3">
<h3>Gradient descent idea</h3>
<p>To move towards minimum of a (smooth, convex) function, use first
order approximation:</p>
<p>Start from some initial point, then iteratively</p>
<ul>
<li>compute gradient at current point, and</li>
<li>add some fraction of the negative gradient to the current point</li>
</ul>
</section>
<section id="gradient-descent-illustration" class="slide level3">
<h3>Gradient descent illustration</h3>
<figure>
<img data-src="../images/gradient-descent-animation.gif"
style="width:80.0%"
alt="Link for animation. Image credit: Peter Roelants" />
<figcaption aria-hidden="true"><a
href="https://miro.medium.com/max/700/1*KQVi812_aERFRolz_5G3rA.gif">Link
for animation</a>. Image credit: Peter Roelants</figcaption>
</figure>
<!--

### Visual example: least square solution 3D plot

![Regression parameters - 3D plot.](../images/3.2b.svg){ width=40% }

-->
</section>
<section id="standard-batch-gradient-descent" class="slide level3">
<h3>Standard (“batch”) gradient descent</h3>
<p>For each step <span class="math inline">\(t\)</span> along the error
curve:</p>
<p><span class="math display">\[
\begin{aligned}
\mathbf{w}^{t+1} &amp;= \mathbf{w}^t - \alpha \nabla L(\mathbf{w}^t) \\
&amp;= \mathbf{w}^t - \frac{\alpha}{n} \sum_{i=1}^n \nabla
L_i(\mathbf{w}^t, \mathbf{x}_i, y_i)
\end{aligned}
\]</span></p>
<p>Repeat until stopping criterion is met.</p>
<aside class="notes">
<p>“Stopping criteria” may be: loss is sufficiently small, gradient is
sufficiently close to zero, or a pre-set max number of iterations is
reached.</p>
<p>Note: the superscript <span class="math inline">\(t\)</span> tracks
what iteration we are on. It’s not an exponent!</p>
</aside>
</section>
<section id="example-gradient-descent-for-linear-regression-1"
class="slide level3">
<h3>Example: gradient descent for linear regression (1)</h3>
<p>With a mean squared error loss function</p>
<p><span class="math display">\[
\begin{aligned}
L(w) &amp;= \frac{1}{n} \sum_{i=1}^n (y_i - \langle w, x_i \rangle)^2 \\
     &amp;= \frac{1}{n} \|y - Xw\|^2
\end{aligned}
\]</span></p>
<aside class="notes">
<p>Gradient of the loss function is:</p>
<ul>
<li>Vector form: <span class="math inline">\(\frac{-\alpha^t}{n}
\sum_{i=1}^n (y_i - \langle w^t,x_i \rangle) x_i\)</span></li>
<li>Matrix form: <span class="math inline">\(\frac{-\alpha^t}{n} X^T (y
- X w^t)\)</span></li>
</ul>
<p>we move in the <em>opposite</em> direction, so…</p>
</aside>
</section>
<section id="example-gradient-descent-for-linear-regression-2"
class="slide level3">
<h3>Example: gradient descent for linear regression (2)</h3>
<p>We will compute the weights at each step as</p>
<p><span class="math display">\[
\begin{aligned}
w^{t+1} &amp;= w^t + \frac{\alpha^t}{n} \sum_{i=1}^n (y_i - \langle
w^t,x_i \rangle) x_i \\
        &amp;= w^t + \frac{\alpha^t}{n} X^T (y - X
w^t)                  
\end{aligned}
\]</span></p>
<p>(dropping the constant 2 factor)</p>
<aside class="notes">
<p>To update <span class="math inline">\(\mathbf{w}\)</span>, must
compute <span class="math inline">\(n\)</span> loss functions and
gradients - each iteration is <span
class="math inline">\(O(nd)\)</span>. We need multiple iterations, but
in many cases it’s more efficient than the previous approach.</p>
<p>However, if <span class="math inline">\(n\)</span> is large, it may
still be expensive!</p>
</aside>
</section></section>
<section>
<section id="variations-on-main-idea" class="title-slide slide level2">
<h2>Variations on main idea</h2>
<aside class="notes">
<p>Two main “knobs” to turn:</p>
<ul>
<li>“batch” size</li>
<li>learning rate</li>
</ul>
</aside>
</section>
<section id="stochastic-gradient-descent" class="slide level3">
<h3>Stochastic gradient descent</h3>
<p>Idea:</p>
<p>At each step, compute estimate of gradient using only one randomly
selected sample, and move in the direction it indicates.</p>
<aside class="notes">
<figure>
<img data-src="../images/3-gd-sgd.png" style="width:70.0%"
alt="Full-batch gradient descent (left), SGD (right). Many of the steps will be in the wrong direction, but progress towards minimum occurs on average, as long as the steps are small." />
<figcaption aria-hidden="true">Full-batch gradient descent (left), SGD
(right). Many of the steps will be in the wrong direction, but progress
towards minimum occurs <em>on average</em>, as long as the steps are
small.</figcaption>
</figure>
<p>Each iteration is now only <span class="math inline">\(O(d)\)</span>,
but we may need more iterations than for gradient descent. However, in
many cases we still come out ahead (especially if <span
class="math inline">\(n\)</span> is large!).</p>
<p>See <a
href="https://chinmayhegde.github.io/introml-notes-sp2020/pages/lecture3_notes.html">supplementary
notes</a> for an analysis of the number of iterations needed.</p>
<p>Also:</p>
<ul>
<li>SGD is often more efficient because of <em>redundancy</em> in the
data - data points have some similarity.</li>
<li>But, we miss out on the benefits of vectorization. In practice, it
takes longer to compute something over 1 sample 1024 times, than over
1024 samples 1 time.</li>
<li>If the function we want to optimize does not have a global minimum,
the noise can be helpful - we can “bounce” out of a local minimum.</li>
</ul>
</aside>
</section>
<section id="mini-batch-also-stochastic-gradient-descent-1"
class="slide level3">
<h3>Mini-batch (also “stochastic”) gradient descent (1)</h3>
<p>Idea:</p>
<p>At each step, select a small subset of training data (“mini-batch”),
and evaluate gradient on that mini-batch.</p>
<p>Then move in the direction it indicates.</p>
</section>
<section id="mini-batch-also-stochastic-gradient-descent-2"
class="slide level3">
<h3>Mini-batch (also “stochastic”) gradient descent (2)</h3>
<p>For each step <span class="math inline">\(t\)</span> along the error
curve:</p>
<ul>
<li>Select random mini-batch <span
class="math inline">\(I_t\subset{1,\ldots,n}\)</span></li>
<li>Compute gradient approximation:</li>
</ul>
<p><span class="math display">\[g^t = \frac{1}{|I_t|} \sum_{i\in I_t}
\nabla L(\mathbf{x}_i, y_i, \mathbf{w}^t)\]</span></p>
<ul>
<li>Update parameters: <span class="math inline">\(\mathbf{w}^{t+1} =
\mathbf{w}^t - \alpha^t g^t\)</span></li>
</ul>
<aside class="notes">
<p>Now that each iteration is not equal to an iteration over
<em>all</em> data, we need to introduce the idea of an “epoch”:</p>
<ul>
<li>One epoch = one pass over <em>all</em> the data</li>
<li>Mini-batch SGD is often used in practice because we get some benefit
of vectorization, but also take advantage of redundancy in data.</li>
</ul>
<p>After a fixed number of epochs (passes over the entire data), * we
may end up at a better minimum (lower loss) with a small batch size, *
<em>but</em>, the time per epoch may be longer with a small batch
size.</p>
</aside>
<!-- 

https://www.cs.cornell.edu/courses/cs4787/2021sp/
https://www.cs.cornell.edu/courses/cs6787/2018fa/Lecture2.pdf


https://ruder.io/optimizing-gradient-descent/

https://sebastianraschka.com/faq/docs/sgd-methods.html
https://sebastianraschka.com/faq/docs/gradient-optimization.html
https://distill.pub/2017/momentum/
https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf
https://sebastianraschka.com/pdf/lecture-notes/stat479ss19/L12_optim_slides.pdf
https://vis.ensmallen.org/
https://sebastianraschka.com/pdf/lecture-notes/stat479ss19/L05_gradient-descent_slides.pdf

-->
</section>
<section id="selecting-the-learning-rate" class="slide level3">
<h3>Selecting the learning rate</h3>
<figure>
<img data-src="../images/learning_rate_comparison.png"
style="width:85.0%" alt="Choice of learning rate \alpha is critical" />
<figcaption aria-hidden="true">Choice of learning rate <span
class="math inline">\(\alpha\)</span> is critical</figcaption>
</figure>
<aside class="notes">
<p>Image credit: Hands-On Machine Learning with Scikit-Learn, Keras, and
TensorFlow, 2nd Edition, by Aurélien Géron.</p>
<p>Also note: SGD “noise ball”</p>
</aside>
</section>
<section id="annealing-the-learning-rate" class="slide level3">
<h3>Annealing the learning rate</h3>
<p>One approach: decay learning rate slowly over time, such as</p>
<ul>
<li>Exponential decay: <span class="math inline">\(\alpha_t = \alpha_0
e^{-k t}\)</span></li>
<li>1/t decay: <span class="math inline">\(\alpha_t = \alpha_0 / (1 + k
t )\)</span></li>
</ul>
<p>(where <span class="math inline">\(k\)</span> is tuning
parameter).</p>
<aside class="notes">
<p>But: this is still sensitive, requires careful selection of gradient
descent parameters for the specific learning problem.</p>
<p>Can we do this in a way that is somehow “tuned” to the shape of the
loss function?</p>
</aside>
</section>
<section id="gradient-descent-in-a-ravine-1" class="slide level3">
<h3>Gradient descent in a ravine (1)</h3>
<figure>
<img data-src="../images/ravine-grad-descent.png" style="width:50.0%"
alt="Gradient descent path bounces along ridges of ravine, because surface curves much more steeply in direction of w_1." />
<figcaption aria-hidden="true">Gradient descent path bounces along
ridges of ravine, because surface curves much more steeply in direction
of <span class="math inline">\(w_1\)</span>.</figcaption>
</figure>
</section>
<section id="gradient-descent-in-a-ravine-2" class="slide level3">
<h3>Gradient descent in a ravine (2)</h3>
<figure>
<img data-src="../images/ravine-grad-descent2.png" style="width:50.0%"
alt="Gradient descent path bounces along ridges of ravine, because surface curves much more steeply in direction of w_1." />
<figcaption aria-hidden="true">Gradient descent path bounces along
ridges of ravine, because surface curves much more steeply in direction
of <span class="math inline">\(w_1\)</span>.</figcaption>
</figure>
</section>
<section id="momentum-1" class="slide level3">
<h3>Momentum (1)</h3>
<ul>
<li>Idea: Update includes a <em>velocity</em> vector <span
class="math inline">\(v\)</span>, that accumulates gradient of past
steps.</li>
<li>Each update is a linear combination of the gradient and the previous
updates.</li>
<li>(Go faster if gradient keeps pointing in the same direction!)</li>
</ul>
<!--

### Momentum (2)

Classical momentum: for some $0 \leq \gamma_t < 1$,

$$v_{t+1} = \gamma_t v_t - \alpha_t \nabla L\left(w_t\right)$$

so

$$w_{t+1} = w_t + v_{t+1} = w_t  - \alpha_t \nabla L\left(w_t\right) + \gamma_t v_t$$

($\gamma_t$ is often around 0.9, or starts at 0.5 and anneals to 0.99 over many epochs.)

Note: $v_{t+1} = w_{t+1} - w_t$ is $\Delta w$.

-->
</section>
<section id="momentum-2" class="slide level3">
<h3>Momentum (2)</h3>
<p>Classical momentum: for some <span class="math inline">\(0 \leq
\gamma_t &lt; 1\)</span>,</p>
<p><span class="math display">\[v^{t+1} = \gamma^t v^t +   \nabla
L\left(w^t\right)\]</span></p>
<p>so</p>
<p><span class="math display">\[w^{t+1} = w^t - \alpha^t v^{t+1} =
w^t  - \alpha^t \left( \gamma^t v^t + \nabla L\left(w^t\right)
\right)\]</span></p>
<p>(<span class="math inline">\(\gamma\)</span> may be in range 0.9 -
0.99.)</p>
</section>
<section id="momentum-pseudocode" class="slide level3">
<h3>Momentum: pseudocode</h3>
GD:

<pre><code>for t in range(num_steps):
  dw = compute_grad(w)
  w -= lr * dw
</code></pre>

GD + Momentum:

<pre><code>v = 0
for t in range(num_steps):
  dw = compute_grad(w)
  <span style="color: blue">v = gamma * v + dw</span>
  w -= lr * <span style="color: blue">v</span>
</code></pre>
</section>
<section id="momentum-illustrated" class="slide level3">
<h3>Momentum: illustrated</h3>
<figure>
<img data-src="../images/ravine-momentum.png" style="width:45.0%"
alt="Momentum dampens oscillations by reinforcing the component along w_2 while canceling out the components along w_1." />
<figcaption aria-hidden="true">Momentum dampens oscillations by
reinforcing the component along <span class="math inline">\(w_2\)</span>
while canceling out the components along <span
class="math inline">\(w_1\)</span>.</figcaption>
</figure>
</section>
<section id="adagrad-1" class="slide level3">
<h3>AdaGrad (1)</h3>
<p>Next idea: “per-parameter learning rates”!</p>
<p>Track per-parameter square of gradient, to normalize parameter update
step.</p>
</section>
<section id="adagrad-2" class="slide level3">
<h3>AdaGrad (2)</h3>
<p><span class="math display">\[w^{t+1} = w^t  - \frac{\alpha}{\sqrt{
v^{t+1} + \epsilon } } \nabla L\left(w^t\right) \]</span></p>
<p>where</p>
<p><span class="math display">\[v^{t+1} = v^{t} + \nabla
L\left(w^{t}\right) ^2\]</span></p>
<aside class="notes">
<p>Weights with large gradient have smaller learning rate, weights with
small gradients have larger learning rates.</p>
<p>i.e.: take smaller steps in steep directions, take bigger steps where
the gradient is flat.</p>
</aside>
</section>
<section id="adagrad-pseudocode" class="slide level3">
<h3>AdaGrad: pseudocode</h3>
GD:

<pre><code>for t in range(num_steps):
  dw = compute_grad(w)
  w -= lr * dw
</code></pre>


GD + AdaGrad:

<pre><code>grad_sq = 0
for t in range(num_steps):
  dw = compute_grad(w)
  <span style="color: purple">grad_sq = grad_sq +  dw * dw</span>
  w -= lr * dw <span style="color: purple">/ sqrt(grad_sq + epsilon)</span>
</code></pre>

</section>
<section id="rmsprop-leaky-adagrad" class="slide level3">
<h3>RMSProp: Leaky AdaGrad</h3>
<p>Idea: Use EWMA to emphasize <em>recent</em> gradient magnitudes.</p>
<p><span class="math display">\[w^{t+1} = w^t  - \frac{\alpha}{\sqrt{
v^{t+1} + \epsilon } } \nabla L\left(w^t\right) \]</span></p>
<p>where</p>
<p><span class="math display">\[v^{t+1} = \gamma v^{t} + (1 - \gamma)
\nabla L\left(w^{t}\right) ^2\]</span></p>
</section>
<section id="rmsprop-pseudocode" class="slide level3">
<h3>RMSProp: pseudocode</h3>

GD + AdaGrad:

<pre><code>grad_sq = 0
for t in range(num_steps):
  dw = compute_grad(w)
  <span style="color: purple">grad_sq = grad_sq +  dw * dw</span>
  w -= lr * dw <span style="color: purple">/ sqrt(grad_sq + epsilon)</span>
</code></pre>


GD + RMSProp:

<pre><code>grad_sq = 0
for t in range(num_steps):
  dw = compute_grad(w)
  <span style="color: magenta">grad_sq = gamma * grad_sq + (1 - gamma) * dw * dw</span>
  w -= lr * dw <span style="color: magenta">/ sqrt(grad_sq + epsilon)</span>
</code></pre>

</section>
<section id="adam-adaptive-moment-estimation" class="slide level3">
<h3>Adam: Adaptive Moment Estimation</h3>
<ul>
<li>Uses ideas from momentum (first moment) and RMSProp (second
moment)!</li>
<li>plus bias correction
<!-- https://web.eecs.umich.edu/~justincj/slides/eecs498/498_FA2019_lecture04.pdf --></li>
</ul>
</section>
<section id="adam-pseudocode-vs-momentum" class="slide level3">
<h3>Adam: pseudocode vs Momentum</h3>

GD + Momentum:

<pre><code>v = 0
for t in range(num_steps):
  dw = compute_grad(w)
  <span style="color: blue">v = gamma * v + dw</span>
  w -= lr * <span style="color: blue">v</span>
</code></pre>


GD + Adam (without bias correction):

<pre><code>moment1 = 0
moment2 = 0
for t in range(num_steps):
  dw = compute_grad(w)
  <span style="color: blue">moment1 = b1 * moment1 + (1 - b1) * dw</span>
  moment2 = b2 * moment2 + (1 - b2) * dw * dw
  w -= lr * <span style="color: blue">moment1</span> / sqrt(moment2 + epsilon)
</code></pre>

</section>
<section id="adam-pseudocode-vs-rmsprop" class="slide level3">
<h3>Adam: pseudocode vs RMSProp</h3>

GD + RMSProp:

<pre><code>grad_sq = 0
for t in range(num_steps):
  dw = compute_grad(w)
  <span style="color: magenta">grad_sq = gamma * grad_sq + (1 - gamma) * dw * dw</span>
  w -= lr * dw <span style="color: magenta">/ sqrt(grad_sq + epsilon)</span>
</code></pre>

GD + Adam (without bias correction):

<pre><code>moment1 = 0
moment2 = 0
for t in range(num_steps):
  dw = compute_grad(w)
  <span style="color: blue">moment1 = b1 * moment1 + (1 - b1) * dw</span>
  <span style="color: magenta">moment2 = b2 * moment2 + (1 - b2) * dw * dw</span>
  w -= lr * <span style="color: blue">moment1</span> <span style="color: magenta">/ sqrt(moment2 + epsilon)</span>
</code></pre>

<aside class="notes">
<p>Usually <code>b1</code> is smaller than <code>b2</code>, i.e. we
update <code>moment1</code> more aggressively than
<code>moment2</code>.</p>
</aside>
</section>
<section id="adam-pseudocode-with-bias-correction" class="slide level3">
<h3>Adam: Pseudocode with bias correction</h3>

<pre><code>moment1 = 0
moment2 = 0
for t in range(num_steps):
  dw = compute_grad(w)
  <span style="color: blue">moment1 = b1 * moment1 + (1 - b1) * dw</span>
  <span style="color: magenta">moment2 = b2 * moment2 + (1 - b2) * dw * dw</span>
  <span style="color: green">moment1_unbias = moment1 / (1 - b1 ** t)
  moment2_unbias = moment2 / (1 - b2 ** t)</span>
  w -= lr * <span style="color: blue">moment1_unbias</span> <span style="color: magenta">/ sqrt(moment2_unbias + epsilon)</span>
</code></pre>
<aside class="notes">
<p>When we initialize both moments to zero, they are initially “biased”
to smaller values (since they update slowly!) This bias correction
accounts for that.</p>
</aside>
</section>
<section id="illustration-beales-function" class="slide level3">
<h3>Illustration (Beale’s function)</h3>
<figure>
<img data-src="../images/beale-gradient.gif" style="width:40.0%"
alt="Animation credit: Alec Radford. Link for animation." />
<figcaption aria-hidden="true">Animation credit: Alec Radford. <a
href="https://imgur.com/a/Hqolp">Link for animation</a>.</figcaption>
</figure>
<aside class="notes">
<p>Due to the large initial gradient, velocity based techniques shoot
off and bounce around, while those that scale gradients/step sizes like
RMSProp proceed more like accelerated SGD.</p>
</aside>
</section>
<section id="illustration-long-valley" class="slide level3">
<h3>Illustration (Long valley)</h3>
<figure>
<img data-src="../images/long-valley-gradient.gif" style="width:40.0%"
alt="Animation credit: Alec Radford. Link for animation." />
<figcaption aria-hidden="true">Animation credit: Alec Radford. <a
href="https://imgur.com/a/Hqolp">Link for animation</a>.</figcaption>
</figure>
<aside class="notes">
<p>SGD stalls and momentum has oscillations until it builds up velocity
in optimization direction. Algorithms that scale step size quickly break
symmetry and descend in optimization direction.</p>
</aside>
</section></section>
<section id="recap" class="title-slide slide level2">
<h2>Recap</h2>
<ul>
<li>Gradient descent as a general approach to training</li>
<li>Variations</li>
</ul>
<aside class="notes">
<p>Gradient descent is easy on linear regression! You won’t get to apply
any of these more advanced techniques until later in the semester, when
we work with less friendly loss surfaces.</p>
</aside>
</section>
    </div>
  </div>

  <script src="reveal.js-master/dist/reveal.js"></script>

  <!-- reveal.js plugins -->
  <script src="reveal.js-master/plugin/notes/notes.js"></script>
  <script src="reveal.js-master/plugin/search/search.js"></script>
  <script src="reveal.js-master/plugin/zoom/zoom.js"></script>
  <script src="reveal.js-master/plugin/math/math.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
        // Display controls in the bottom right corner
        controls: true,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: true,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'bottom-right',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: false,

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: false,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: true,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'default',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: true,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'slide',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'fade',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [
          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    </body>
</html>
