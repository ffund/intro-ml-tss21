<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="author" content="Fraida Fund">
  <title>Exploring your data</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="reveal.js-master/dist/reset.css">
  <link rel="stylesheet" href="reveal.js-master/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <link rel="stylesheet" href="reveal.js-master/dist/theme/white.css" id="theme">
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <h1 class="title">Exploring your data</h1>
  <p class="author">Fraida Fund</p>
</section>

<section>
<section id="garbage-in-garbage-out" class="title-slide slide level2">
<h2>Garbage in, garbage out</h2>
<aside class="notes">
<p>If you remember nothing else from this semester, remember this!</p>
<p>If you use “garbage” to train a machine learning model, you will only ever get “garbage” out. Also, since you are testing on the same data, you might not even realize it is “garbage” until the model is in production!</p>
</aside>
</section>
<section id="recall-ml-as-a-leaky-pipeline" class="slide level3">
<h3>Recall: ML as a “leaky pipeline”</h3>
<figure>
<img data-src="../images/1-wot-leaky-pipelines.png" style="width:40.0%" alt="Source: Boaz Barak." /><figcaption aria-hidden="true">Source: <a href="https://windowsontheory.org/2021/01/31/a-blitz-through-classical-statistical-learning-theory/">Boaz Barak</a>.</figcaption>
</figure>
<aside class="notes">
<p>“Garbage” affects the pipeline in several places.</p>
</aside>
</section>
<section id="example-a-data-problem-1" class="slide level3">
<h3>Example: a data problem (1)</h3>
<p>Data analysis: use PubMed, and identify the year of first publication for the 100,000 most cited authors.</p>
<aside class="notes">
<p>What are our expectations about what this should look like?</p>
</aside>
</section>
<section id="example-a-data-problem-2" class="slide level3">
<h3>Example: a data problem (2)</h3>
<figure>
<img data-src="../images/1-pubmed-authors.png" style="width:50.0%" alt="Does this look reasonable?" /><figcaption aria-hidden="true">Does this look reasonable?</figcaption>
</figure>
<aside class="notes">
<p>We can think of many potential explanations for this pattern, even though it is actually a data artifact.</p>
<p>The true explanation: in 2002, PubMed started using full first names in authors instead of just initials. The same author is represented in the dataset as a “new” author with a first date of publication in 2002.</p>
</aside>
</section>
<section id="example-a-data-problem-3" class="slide level3">
<h3>Example: a data problem (3)</h3>
<figure>
<img data-src="../images/1-pubmed-authors2.png" style="width:50.0%" alt="The real distribution, after name unification. Example via Steven Skiena @ SBU." /><figcaption aria-hidden="true">The real distribution, after name unification. Example via <a href="https://www3.cs.stonybrook.edu/~skiena/519/">Steven Skiena @ SBU</a>.</figcaption>
</figure>
</section>
<section id="example-another-data-problem-1" class="slide level3">
<h3>Example: another data problem (1)</h3>
<figure>
<img data-src="../images/1-election2020.png" style="width:30.0%" alt="Data like this was widely (wrongly) used as evidence of anomaly in the 2020 U.S. Presidential election." /><figcaption aria-hidden="true">Data like this was widely (wrongly) used as evidence of anomaly in the 2020 U.S. Presidential election.</figcaption>
</figure>
<aside class="notes">
<p>What are our assumptions about election night data, and how are they violated here?</p>
<p>We expect that per-candidate vote totals (computed by multiplying total votes and vote share) should increase as more votes are counted, but never decrease.</p>
<p>What are possible explanations?</p>
</aside>
</section>
<section id="example-another-data-problem-2" class="slide level3">
<h3>Example: another data problem (2)</h3>
<figure>
<img data-src="../images/1-election2020-process.png" style="width:75.0%" alt="Process by which data is collected by Edison and AP." /><figcaption aria-hidden="true">Process by which data is collected by Edison and AP.</figcaption>
</figure>
<aside class="notes">
<p>This anomaly makes a lot of sense as a correction of a data entry or duplicate entry error.</p>
<p>How Edison/AP collects the data for their Election Night feed:</p>
<ul>
<li>There are “stringers” (temporary reporters) at various elections offices who call results into their phone center</li>
<li>They have people who look at official government websites for new results that they manually enter into the system</li>
<li>They have people who monitor results sent by fax from counties and cities</li>
</ul>
<p>all working as fast as they can! Data entry and duplicate entry errors are not only likely, they are almost guaranteed. When they are corrected, vote totals may decrease.</p>
<p>Source: <a href="https://web.archive.org/web/20210410214207/https://www.ap.org/en-us/topics/politics/elections/counting-the-vote">AP</a>, <a href="http://www.edisonresearch.com/wp-content/uploads/2020/10/Web-Entry-Team-Handout-2020.pdf">Edison</a></p>
</aside>
</section></section>
<section>
<section id="what-kinds-of-data-problems" class="title-slide slide level2">
<h2>What kinds of data problems?</h2>

</section>
<section id="what-kind-of-problems-might-you-encounter-1" class="slide level3">
<h3>What kind of problems might you encounter? (1)</h3>
<ul>
<li>Rows where some fields are missing data</li>
<li>Missing data encoded as zero</li>
<li>Different units, time zones, etc. in different rows</li>
<li>Same value represented several different ways (e.g. names, dates)</li>
<li>Unreasonable values</li>
</ul>
<aside class="notes">
<p>How should you handle little bits of missing data? It always depends on the data and the circumstances. Some possibilities include:</p>
<ul>
<li>omit the row</li>
<li>fill with mean</li>
<li>fill back/forward (ordered rows)</li>
<li>train a model on the rest of the data to “predict” the missing value</li>
<li>what <strong>not</strong> to do: fill with zero</li>
</ul>
<p>How should you handle unreasonable values or outliers?</p>
<ul>
<li>e.g. suppose in a dataset of voter information, some have impossible year of birth - would make the voter a child, or some indicate the voter is 120 years old. (Voters with no known DOB, who registered before DOB was required, are often encoded with a January 1900 DOB.)</li>
<li><strong>not</strong> a good idea to just remove outliers unless you are sure they are a data entry error or otherwise not a “true” value.</li>
<li>Even if an outlier is due to some sort of error, if you remove them, you may skew the dataset (as in the 1/1/1900 voters example).</li>
</ul>
</aside>
</section>
<section id="what-kind-of-problems-might-you-encounter-2" class="slide level3">
<h3>What kind of problems might you encounter? (2)</h3>
<ul>
<li>Rows that are completely missing</li>
<li>Data is not sampled evenly</li>
<li>Data or labels reflect human bias</li>
<li>Data is not representative of your target situation</li>
</ul>
<aside class="notes">
<p>Examples:</p>
<ul>
<li>Twitter API terms of use don’t allow researchers to share tweets directly, only message IDs (except for limited distribution, e.g. by email). To reproduce the dataset, you use the Twitter API to download messages using their IDs. But, tweets that have been removed are not available - the distribution of removed tweets is not flat! (For example: you might end up with a dataset that has offensive posts but few “obvious” offensive posts.)</li>
<li>Many social media datasets used for “offensive post” classification have biased labels (especially if they were produced without adequate training procedures in place). For example, they may label posts containing African-American dialects of English as “offensive” much more often. <a href="https://www.aclweb.org/anthology/P19-1163.pdf">Source</a>, <a href="https://www.vox.com/recode/2019/8/15/20806384/social-media-hate-speech-bias-black-african-american-facebook-twitter">User-friendly article</a></li>
<li>A dataset of Tweets following Hurricane Sandy makes it looks like Manhattan was the hub of the disaster, because of power blackouts and limited cell service in the most affected areas. <a href="https://hbr.org/2013/04/the-hidden-biases-in-big-data">Source</a></li>
<li>The City of Boston released a smartphone app that uses accelerometer and GPS data to detect potholes and report them automatically. But, low income and older residents are less likely to have smartphones, so this dataset presents a skewed view of where potholes are. <a href="https://hbr.org/2013/04/the-hidden-biases-in-big-data">Source</a></li>
</ul>
</aside>
</section>
<section id="what-kind-of-problems-might-you-encounter-3" class="slide level3">
<h3>What kind of problems might you encounter? (3)</h3>
<ul>
<li>Data ethics fails</li>
<li>Data leakage</li>
</ul>
<aside class="notes">
<p>Some data ethics fails:</p>
<ul>
<li><a href="http://www.michaelzimmer.org/2008/09/30/on-the-anonymity-of-the-facebook-dataset/">On the anonymity of the Facebook dataset</a></li>
<li><a href="https://www.vice.com/en_us/art*cle/8q88nx/70000-okcupid-users-just-had-their-data-published">70,000 OkCupid Users Just Had Their Data Published</a>; <a href="https://www.wired.com/2016/05/*kcupid-study-reveals-perils-big-data-science/">OkCupid Study Reveals the Perils of Big-Data Science</a>; <a href="https://ironholds.org/scientific-consent/">Ethics, scientific consent and OKCupid</a></li>
<li><a href="https://www.theverge.com/2019/3/12/18262646/ibm-didnt-inform-people-when-it-used-their-flickr-photos-for-facial-recognition-training">IBM didn’t inform people when it used their Flickr photos for facial recognition training</a></li>
</ul>
</aside>
</section></section>
<section>
<section id="data-leakage" class="title-slide slide level2">
<h2>Data leakage</h2>
<aside class="notes">
<p>In machine learning, we train models on a training set of data, then evaluate their performance on a set of data that was not used in training.</p>
<p>Sometimes, information from the training set can “leak” into the evaluation - this is called data leakage.</p>
<p>Or, information from the target variable (which should not be available during inference) leaks into the feature data.</p>
</aside>
</section>
<section id="some-types-of-data-leakage" class="slide level3">
<h3>Some types of data leakage</h3>
<ul>
<li>Learning from adjacent temporal data</li>
<li>Learning from duplicate data</li>
<li>Learning from features that are not available at prediction time (e.g. data from the future)</li>
<li>Learning from a feature that is a proxy for target variable</li>
</ul>
</section>
<section id="covid-19-chest-radiography-1" class="slide level3">
<h3>COVID-19 chest radiography (1)</h3>
<ul>
<li><strong>Problem</strong>: diagnose COVID-19 from chest radiography images</li>
<li><strong>Input</strong>: image of chest X-ray (or other radiography)</li>
<li><strong>Target variable</strong>: COVID or no COVID</li>
</ul>
</section>
<section id="covid-19-chest-radiography-2" class="slide level3">
<h3>COVID-19 chest radiography (2)</h3>
<figure>
<img data-src="../images/1-covid-xrays.png" style="width:60.0%" alt="Neural networks can classify the source dataset of these chest X-ray images, even without lungs! Source" /><figcaption aria-hidden="true">Neural networks can classify the source dataset of these chest X-ray images, even <em>without lungs</em>! <a href="https://arxiv.org/abs/2004.12823">Source</a></figcaption>
</figure>
<aside class="notes">
<p>In Spring 2020, many papers were published that claimed to use machine learning to diagnose COVID-19 patients based on chest X-rays or other radiography.</p>
<p>To train these models, people used an emerging COVID-19 chest X-ray dataset, along with one or more existing chest X-ray dataset, for example a pre-existing dataset used to try and classify viral vs. bacterial pneumonia.</p>
<p>The problem is that the chest X-rays for each dataset were so “distinctive” to that dataset, that a neural network could be trained with high accuracy to classify an image into its source dataset, even without the lungs showing!</p>
</aside>
</section>
<section id="covid-19-chest-radiography-2-1" class="slide level3">
<h3>COVID-19 chest radiography (2)</h3>
<p>Findings:</p>
<ul>
<li>some non-COVID datasets were pediatric images, COVID images were adult</li>
<li>there were dataset-level differences in patient positioning</li>
<li>many COVID images came from screenshots of published papers, which often had text, arrows, or other annotations over the images. (Some non-COVID images did, too.)</li>
</ul>
</section>
<section id="covid-19-chest-radiography-3" class="slide level3">
<h3>COVID-19 chest radiography (3)</h3>
<figure>
<img data-src="../images/1-covid-xrays-saliency.png" style="width:90.0%" alt="Saliency map showing the “important” pixels for classification. Source" /><figcaption aria-hidden="true">Saliency map showing the “important” pixels for classification. <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7523163/">Source</a></figcaption>
</figure>
<aside class="notes">
<p>These findings are based on techniques like</p>
<ul>
<li>saliency maps, where the model is made to highlight the part of the image (the pixels) that it considered most relevant to its decision.</li>
<li>using generative models and asking it to take a COVID-negative X-ray and make it positive (or v.v.)</li>
</ul>
<p>Many of the findings are not easy to understand without domain knowledge (e.g. knowing what part of the X-ray <em>should</em> be important and what part should not be.) For example: should the diaphragm area be helpful?</p>
</aside>
</section>
<section id="signs-of-potential-data-leakage-after-training" class="slide level3">
<h3>Signs of potential data leakage (after training)</h3>
<ul>
<li>Performance is “too good to be true”</li>
<li>Unexpected behavior of model (e.g. learns from a feature that shouldn’t help)</li>
</ul>
</section>
<section id="detecting-data-leakage" class="slide level3">
<h3>Detecting data leakage</h3>
<ul>
<li>Exploratory data analysis</li>
<li>Study the data before, during, and after you use it!</li>
<li>Explainable ML methods</li>
<li>Early testing in production</li>
</ul>
</section></section>
    </div>
  </div>

  <script src="reveal.js-master/dist/reveal.js"></script>

  // reveal.js plugins
  <script src="reveal.js-master/plugin/notes/notes.js"></script>
  <script src="reveal.js-master/plugin/search/search.js"></script>
  <script src="reveal.js-master/plugin/zoom/zoom.js"></script>
  <script src="reveal.js-master/plugin/math/math.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
        // Push each slide change to the browser history
        history: true,
        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [
          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    </body>
</html>
