<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="author" content="Fraida Fund">
  <title>Deep learning</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="reveal.js-master/dist/reset.css">
  <link rel="stylesheet" href="reveal.js-master/dist/reveal.css">
  <style>
    .reveal .sourceCode {  /* see #7635 */
      overflow: visible;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="reveal.js-master/dist/theme/white.css" id="theme">
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <h1 class="title">Deep learning</h1>
  <p class="author">Fraida Fund</p>
</section>

<section id="in-this-lecture" class="title-slide slide level2">
<h2>In this lecture</h2>
<ul>
<li>Deep neural networks</li>
<li>Challenges and tricks</li>
</ul>
<aside class="notes">
<p><strong>Math prerequisites for this lesson</strong>: None.</p>
</aside>
</section>

<section id="recap" class="title-slide slide level2">
<h2>Recap</h2>
<p>Last week: neural networks with one hidden layer</p>
<ul>
<li>Hidden layer learns feature representation</li>
<li>Output layer learns classification/regression tasks</li>
</ul>
<aside class="notes">
<p>With the neural network, the “transformed” feature representation is
<em>learned</em> instead of specified by the designer.</p>
<figure>
<img data-src="../images/8-deep-learning-motivation.png"
style="width:100.0%"
alt="Image is based on a figure in Deep learning, by Goodfellow, Bengio, Courville." />
<figcaption aria-hidden="true">Image is based on a figure in Deep
learning, by Goodfellow, Bengio, Courville.</figcaption>
</figure>
<p>A neural network with non-linear activation, with one hidden layer
and many units in that layer <em>can</em> approximate virtually any
continuous real-valued function, with the right weights. (Refer to the
<em>Universal Approximation Theorem</em>.) But (1) it may need a very
large number of units to represent the function, and (2) those weights
might not be learned by gradient descent - the loss surface is very
unfriendly.</p>
<p>Instead of a single hidden layer, if we use multiple hidden layers
they can “compose” functions learned by the previous layers into more
complex functions - use fewer units, and tends to learn better weights
.</p>
<!--

Universal approximation theorem: https://cedar.buffalo.edu/~srihari/CSE676/6.4%20ArchitectureDesign.pdf

-->
</aside>
</section>

<section>
<section id="deep-neural-networks" class="title-slide slide level2">
<h2>Deep neural networks</h2>
<aside class="notes">
<figure>
<img data-src="../images/8-deep-network.png" style="width:60.0%"
alt="Illustration of a deep network, with multiple hidden layers." />
<figcaption aria-hidden="true">Illustration of a deep network, with
multiple hidden layers.</figcaption>
</figure>
<p>Some comments:</p>
<ul>
<li>each layer is fully connected to the next layer</li>
<li>each unit still works the same way: take the weighted sum of inputs,
apply an activation function, and that’s the unit output</li>
<li>still trained by backpropagation</li>
</ul>
<p>We call the number of layers the “depth” of the network and the
number of hidden units in a layer its “width.”</p>
</aside>
</section>
<section id="double-descent-animation" class="slide level3">
<h3>Double descent: animation</h3>
<figure>
<img data-src="../images/8-polynomial-animation.gif" style="width:40.0%"
alt="Polynomial model before and after the interpolation threshold. Image source: Boaz Barak, click link to see animation." />
<figcaption aria-hidden="true">Polynomial model before and after the
interpolation threshold. Image source: <a
href="https://windowsontheory.org/2021/01/31/a-blitz-through-classical-statistical-learning-theory/">Boaz
Barak, click link to see animation</a>.</figcaption>
</figure>
<aside class="notes">
<p>Explanation (via <a
href="https://windowsontheory.org/2021/01/31/a-blitz-through-classical-statistical-learning-theory/">Boaz
Barak</a>):</p>
<blockquote>
<p>When <span class="math inline">\(d\)</span> of the model is less than
<span class="math inline">\(d_t\)</span> of the polynomial, we are
“under-fitting” and will not get good performance. As <span
class="math inline">\(d\)</span> increases between <span
class="math inline">\(d_t\)</span> and <span
class="math inline">\(n\)</span>, we fit more and more of the noise,
until for <span class="math inline">\(d=n\)</span> we have a perfect
interpolating polynomial that will have perfect training but very poor
test performance. When <span class="math inline">\(d\)</span> grows
beyond <span class="math inline">\(n\)</span>, more than one polynomial
can fit the data, and (under certain conditions) SGD will select the
minimal norm one, which will make the interpolation smoother and
smoother and actually result in better performance.</p>
</blockquote>
</aside>
</section>
<section id="double-descent-curve" class="slide level3">
<h3>Double descent curve</h3>
<figure>
<img data-src="../images/8-double-descent.png" style="width:100.0%"
alt="Double descent curve (left) and realization in a real neural network (right)." />
<figcaption aria-hidden="true">Double descent curve (left) and
realization in a real neural network (right).</figcaption>
</figure>
<aside class="notes">
<p>With a deep neural network, we are not trying to operate in the
“classical ML” bias-variance tradeoff regime (to the left of the
interpolation threshold).</p>
<p>(Interpolation threshold: where the model is just big enough to fit
the training data exactly.)</p>
<ul>
<li>too-small models: can’t represent the “true” function well (lacks
capacity to learn complicated data representations!)</li>
<li>too-big models (before interpolation threshold): memorizes the
input, doesn’t generalize well to unseen data (very sensitive to
noise)</li>
<li>REALLY big models: many possible weights that memorize the input,
our challenge is to find the weight combination that memorizes the input
<em>and</em> does well on unseen data</li>
</ul>
<p>This is complicated by the “loss landscape” of a deep neural network
(trained using backpropagation over the computational graph) looking not
so friendly…</p>
</aside>
</section>
<section id="loss-landscape" class="slide level3">
<h3>Loss landscape</h3>
<figure>
<img data-src="../images/resnet56_noshort_small.jpg" style="width:30.0%"
alt="“Loss landscape” of a deep neural network in a “slice” of the high-dimensional feature space." />
<figcaption aria-hidden="true">“Loss landscape” of a deep neural network
in a “slice” of the high-dimensional feature space.</figcaption>
</figure>
<aside class="notes">
<p>Image source: Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer and
Tom Goldstein. Visualizing the Loss Landscape of Neural Nets. NIPS,
2018.</p>
<p>There are a variety of techniques we can use to improve the network’s
ability to learn good weights + find them efficiently, even on this
difficult loss surface. (Including what we call “conditioning”
techniques, and some broader techniques.)</p>
</aside>
</section>
<section id="addressing-the-challenges" class="slide level3">
<h3>Addressing the challenges</h3>
<aside class="notes">
<figure>
<img data-src="../images/deep-learning-markmap.svg" style="width:60.0%"
alt="Image credit: Sebastian Raschka" />
<figcaption aria-hidden="true">Image credit: Sebastian
Raschka</figcaption>
</figure>
</aside>
</section></section>
<section>
<section id="dataset" class="title-slide slide level2">
<h2>Dataset</h2>
<ul>
<li>Get more data</li>
<li><strong>Data augmentation</strong></li>
<li><strong>Use related data (transfer learning)</strong></li>
<li>Use unlabeled data (self-supervised, semi-supervised)</li>
</ul>
<aside class="notes">
<p>We won’t talk much about getting more data, but if it’s possible to
get more labeled data, it is almost always the most helpful thing you
can do!</p>
<ul>
<li>Example: JFT-300M, a Google internal dataset for training image
models, has 300M images</li>
<li>Example: GTP-3 trained on 45TB of compressed plaintext, about 570GB
after filtering</li>
</ul>
<p><em>Reference: Revisiting Unreasonable Effectiveness of Data in Deep
Learning Era. Chen Sun, Abhinav Shrivastava, Saurabh Singh, Abhinav
Gupta; Proceedings of the IEEE International Conference on Computer
Vision (ICCV), 2017, pp. 843-852.</em></p>
<p>It’s not always possible to get a lot of <em>labeled</em> data for
training a supervised learning model. But sometimes we can use
<em>unlabeled</em> data, which is much easier to get. For example:</p>
<ul>
<li>In self-supervised learning, the label can be inferred automatically
from the unlabeled data. e.g. GPT is trained to predict “next
word”.</li>
<li>In semi-supervised/weakly-supervised learning, we generate labels
(probably imperfectly) for unlabeled data (maybe using a smaller volume
of labeled data to train a model to label the data!)</li>
</ul>
<p>These are mostly out of scope of this course. But we <em>will</em>
talk about data augmentation and transfer learning…</p>
</aside>
</section>
<section id="data-augmentation" class="slide level3">
<h3>Data augmentation</h3>
<figure>
<img data-src="../images/cats-augmentation.png" style="width:50.0%"
alt="Data augmentation on a cat image." />
<figcaption aria-hidden="true">Data augmentation on a cat
image.</figcaption>
</figure>
<aside class="notes">
<p>It doesn’t restrict network capacity - but it helps generalization by
increasing the size of your training set!</p>
<ul>
<li>For image data: apply rotation, crops, scales, change contrast,
brightness, color.</li>
<li>For text you can replace words with synonyms, simulate typos.</li>
<li>For audio you can adjust pitch or speed, add background noise
etc.</li>
</ul>
</aside>
</section>
<section id="transfer-learning" class="slide level3">
<h3>Transfer learning</h3>
<p>Idea: leverage model trained on <em>related</em> data.</p>
<aside class="notes">
<p>State-of-the-art networks involve millions of parameters, huge
datasets, and days of training on GPU clusters</p>
<p>But you don’t have to repeat this process “from scratch” each time
you train a neural network. (For many tasks, you may not even have
enough data to get good results by training a network “from
scratch”.)</p>
<p>The “feature extraction” part of a neural network trained on related
data, is likely still very useful for a slightly different task.</p>
</aside>
</section>
<section id="transfer-learning-from-pre-trained-networks"
class="slide level3">
<h3>Transfer learning from pre-trained networks</h3>
<p>Use pre-trained network for a different task</p>
<ul>
<li>Use early layers from pre-trained network, freeze their
parameters</li>
<li>Only train small number of parameters at the end</li>
</ul>
<aside class="notes">
<p>The base model is a powerful feature extractor (learns transformation
into a more useful feature space), then you just have to train for the
mapping from this feature space to the target variable.</p>
<p>In practice: when applying deep learning, we almost always use a
pre-trained base model! It saves time, energy, and cost.</p>
<p>Example: To pre-train the 7B parameter (smallest) Llama 2, Meta’s
open source language model, takes 184,320 GPU hours (21 GPU YEARS!) and
anywhere from $100,000-800,000 (depending on cost of GPU instances).</p>
</aside>
</section>
<section id="transfer-learning-illustration-1" class="slide level3">
<h3>Transfer learning illustration (1)</h3>
<figure>
<img data-src="../images/8-transfer-similar.png" style="width:80.0%"
alt="When the network is trained on a very similar task, even the abstract high-level features are probably very relevant, so you might tune just the classification head." />
<figcaption aria-hidden="true">When the network is trained on a very
similar task, even the abstract high-level features are probably very
relevant, so you might tune just the classification head.</figcaption>
</figure>
</section>
<section id="transfer-learning-illustration-2" class="slide level3">
<h3>Transfer learning illustration (2)</h3>
<figure>
<img data-src="../images/8-transfer-less.png" style="width:80.0%"
alt="If the original network is not as relevant, may fine-tune more layers." />
<figcaption aria-hidden="true">If the original network is not as
relevant, may fine-tune more layers.</figcaption>
</figure>
</section></section>
<section>
<section id="architecturesetup" class="title-slide slide level2">
<h2>Architecture/setup</h2>
<ul>
<li><strong>Activation functions</strong> (+ skip connections)</li>
<li><strong>Weight initialization</strong></li>
<li><strong>Convolutional units</strong></li>
<li>Recurrent units</li>
<li>… many more ideas</li>
</ul>
<aside class="notes">
<p>Again, this is mostly out of scope of this course, but we’ll talk
about some of these items very briefly.</p>
</aside>
</section>
<section id="recall-activation-functions" class="slide level3">
<h3>Recall: activation functions</h3>
<figure>
<img data-src="../images/activation-functions.png" style="width:40.0%"
alt="Candidate activation functions for a neural network." />
<figcaption aria-hidden="true">Candidate activation functions for a
neural network.</figcaption>
</figure>
<!--
### Zero-centered outputs

Remember that at each hidden unit, we compute

$\frac{\partial L_n}{\partial w_{j,i}} = \delta_j u_i$ 

where $\delta_j$ is the backpropagation error from the "upstream" nodes.

What happens if the $u_i$ terms are always positive?

::: notes

* To compute the derivative with respect to the weights at the input to a neuron, we compute the "local derivative" and then multiply by the "upstream" "backpropagation error" (scalar).

* The scalar multiplier may be positive or negative.

:::

-->
</section>
<section id="sigmoid-vanishing-gradient-1" class="slide level3">
<h3>Sigmoid: vanishing gradient (1)</h3>
<figure>
<img data-src="../images/deep-vanishing-network.png" style="width:80.0%"
alt="Computing gradients by backpropagation. At a hidden unit, \delta_j = g&#39;(z_j) \sum_k w_{k,j} \delta_k." />
<figcaption aria-hidden="true">Computing gradients by backpropagation.
At a hidden unit, <span class="math inline">\(\delta_j = g&#39;(z_j)
\sum_k w_{k,j} \delta_k\)</span>.</figcaption>
</figure>
<aside class="notes">
<p>Suppose we want to compute the gradient of the loss with respect to
the weight shown in pink.</p>
<ul>
<li>We will <em>multiply</em> local gradients (pink) all along each path
between weight and loss function, starting from the end and moving
toward the input.</li>
<li>Then we <em>add</em> up the products of all the paths.</li>
<li>With <span class="math inline">\(\sigma\)</span> activation,
gradient along each path includes the product of a LOT of <span
class="math inline">\(\sigma &#39;\)</span> terms.</li>
</ul>
</aside>
</section>
<section id="sigmoid-vanishing-gradient-2" class="slide level3">
<h3>Sigmoid: vanishing gradient (2)</h3>
<figure>
<img data-src="../images/deep-vanishing-sigmoid.png" style="width:40.0%"
alt="Sigmoid function and its derivative." />
<figcaption aria-hidden="true">Sigmoid function and its
derivative.</figcaption>
</figure>
<aside class="notes">
<p>What happens when you are in the far left or far right part of the
sigmoid?</p>
<ul>
<li>Gradient is close to zero</li>
<li>Weight updates are also close to zero</li>
<li>The “downstream” gradients will also be values close to zero!
(Because of backpropagation.)</li>
<li>And, when you multiply quantities close to zero - they get even
smaller.</li>
</ul>
<p>The network “learns fastest” when the gradient is large. When the
sigmoid “saturates”, it “kills” the neuron!</p>
<p>Even the maximum value of the gradient is only 0.25 - so the gradient
is always less than 1, and we know what happens if you multiply many
quantities less than 1…</p>
<p>(tanh is slightly better - gradient has a larger max + some other
advantages - still has vanishing gradient.)</p>
</aside>
</section>
<section id="relu-dead-relu" class="slide level3">
<h3>ReLU: Dead ReLU</h3>
<figure>
<img data-src="../images/deep-dead-relu.png" style="width:40.0%"
alt="ReLU function and its derivative." />
<figcaption aria-hidden="true">ReLU function and its
derivative.</figcaption>
</figure>
<aside class="notes">
<p>ReLU is a much better non-linear function:</p>
<ul>
<li>does not saturate in positive region</li>
<li>very very fast to compute</li>
</ul>
<p>But, can “die” in the negative region. Once we end up there (e.g. by
learning negative bias weight) we cannot recover - since gradient is
also zero, gradient descent will not update the weights.</p>
<p>(ReLU is also more subject to “exploding” gradient than
sigmoid/tanh.)</p>
</aside>
</section>
<section id="relu-leaky-relu" class="slide level3">
<h3>ReLU: Leaky ReLU</h3>
<figure>
<img data-src="../images/deep-leaky-relu.png" style="width:40.0%"
alt="Leaky ReLU function and its derivative." />
<figcaption aria-hidden="true">Leaky ReLU function and its
derivative.</figcaption>
</figure>
<aside class="notes">
<p>When input is less than 0, the ReLU (and downstream units) is
<em>completely</em> dead (not only very small!)</p>
<p>Alternative: <strong>leaky ReLU</strong> has small (non-zero)
gradient <span class="math inline">\(\alpha\)</span> in the negative
region - can recover.</p>
<p><span class="math display">\[f(x) = \text{max}(\alpha x,
x)\]</span></p>
</aside>
<!-- 

### Other activations

![Other activations. Image via [Sefik Ilkin Serengil](https://sefiks.com/2020/02/02/dance-moves-of-deep-learning-activation-functions/).](../images/sample-activation-functions-square.png){ width=60% }


-->
</section>
<section id="skip-connections" class="slide level3">
<h3>Skip connections</h3>
<p>Alternative solution for “vanishing gradient”:</p>
<ul>
<li>Direct connection between some higher layers and lower layers</li>
<li>A “highway” for gradient info to go directly back to lower
layers</li>
</ul>
<aside class="notes">
<figure>
<img data-src="../images/deep-learning-skip.png"
alt="Diagram of skip connection." />
<figcaption aria-hidden="true">Diagram of skip connection.</figcaption>
</figure>
<p>Suppose we want this sequence of layers to learn <span
class="math inline">\(z = H(x)\)</span>.</p>
<p>Instead of learning <span class="math inline">\(H(x)\)</span>
directly, the network learns the <strong>residual function</strong>:</p>
<p><span class="math display">\[\mathcal{F}(x) = H(x) - x\]</span></p>
<p>Then it reconstructs the desired transformation as:</p>
<p><span class="math display">\[H(x) = \mathcal{F}(x) + x\]</span></p>
<p>This helps avoid vanishing gradient.</p>
<p>In the standard neural network, we would have:</p>
<p><span class="math display">\[\frac{dL}{dx} = \frac{dL}{dz} \cdot
g&#39;(h(x)) \cdot h&#39;(x)\]</span></p>
<p>where <span class="math inline">\(g&#39;()\)</span> and <span
class="math inline">\(h&#39;()\)</span> multiply the gradient that
backpropagates from the higher layers, <span
class="math inline">\(\frac{dL}{dz}\)</span>, and scale it down. But
with the skip connection, we now have</p>
<p><span class="math display">\[\frac{dL}{dx} = \frac{dL}{dz} \cdot
\left( \frac{d\mathcal{F}(x)}{dx} + I \right)
= \frac{dL}{dz} \cdot \left( g&#39;(h(x)) \cdot h&#39;(x) + I
\right)\]</span></p>
<p>The identity matrix <span class="math inline">\(I\)</span> passes on
the gradient from higher layers as an additive term, without scaling it
down.</p>
</aside>
</section>
<section id="weight-initialization" class="slide level3">
<h3>Weight initialization</h3>
<figure>
<img data-src="../images/deep-network-initialization.png"
style="width:80.0%"
alt="One path in forward pass on a neural network." />
<figcaption aria-hidden="true">One path in forward pass on a neural
network.</figcaption>
</figure>
<aside class="notes">
<p>What if we initialize weights to:</p>
<ul>
<li><strong>zero?</strong> If weights are all initialized to zero, all
the outputs are zero (for any input) - won’t learn.</li>
<li><strong>a constant (non-zero)?</strong> If weights are all
initialized to the same constant, we are more prone to “herding” -
hidden units all move in the same direction at once, instead of
“specializing”.</li>
<li><strong>a normal random value with small <span
class="math inline">\(\sigma\)</span>?</strong> Small normal random
values work well for “shallow” networks, but not for deep networks - it
makes the outputs “collapse” toward zero at later layers.</li>
<li><strong>a normal random value with large <span
class="math inline">\(\sigma\)</span>?</strong> Large normal random
values are bad - it makes the outputs “explode” at later layers.</li>
</ul>
</aside>
</section>
<section id="weight-initialization---normal" class="slide level3">
<h3>Weight initialization - normal</h3>
<figure>
<img data-src="../images/deep-weight-init-normal.png"
style="width:75.0%"
alt="Initial weights and ReLU unit outputs for each layer in a network." />
<figcaption aria-hidden="true">Initial weights and ReLU unit outputs for
each layer in a network.</figcaption>
</figure>
<aside class="notes">
<ul>
<li>top row: too-small initial weights, by the last layer the outputs
“collapse” toward zero</li>
<li>middle row: good initial weights, distribution is similar from input
to output</li>
<li>bottom row: too-large initial weights, by the last layer the outputs
“explode”</li>
</ul>
<!-- Image via [Andre Perunicic](https://intoli.com/blog/neural-network-initialization/) -->
</aside>
</section>
<section id="desirable-properties-for-initial-weights---principle"
class="slide level3">
<h3>Desirable properties for initial weights - principle</h3>
<ul>
<li>The mean of the intial weights should be around 0</li>
<li>The variance of the activations should stay the same across every
layer</li>
</ul>
<aside class="notes">
<p>If you are interested, <a
href="https://www.deeplearning.ai/ai-notes/initialization/index.html">here’s
a derivation</a>.</p>
</aside>
</section>
<section id="desirable-properties-for-initial-weights---practice"
class="slide level3">
<h3>Desirable properties for initial weights - practice</h3>
<ul>
<li>For tanh: Xavier scales by <span
class="math inline">\(\frac{1}{\sqrt{N_{in}}}\)</span></li>
<li>For ReLU: He scales by <span
class="math inline">\(\frac{2}{\sqrt{N_{in}}}\)</span></li>
</ul>
<p><span class="math inline">\(N_{in}\)</span> is the number of inputs
to the layer (“fan-in”).</p>
</section>
<section id="weight-initialization---he" class="slide level3">
<h3>Weight initialization - He</h3>
<figure>
<img data-src="../images/deep-weight-init-he.png" style="width:80.0%"
alt="Initial weights and ReLU unit outpus for each layer in a network, He initialization. In this example, the size of the layers is: 100, 150, 200, 250, 300." />
<figcaption aria-hidden="true">Initial weights and ReLU unit outpus for
each layer in a network, He initialization. In this example, the size of
the layers is: 100, 150, 200, 250, 300.</figcaption>
</figure>
<!--

### Desirable properties - illustration (1)

![Activation function outputs with normal initialization of weights. Image source: Justin Johnson.](../images/8-init-collapse.png){ width=70% }



### Desirable properties - illustration (2)

![Activation function outputs with Xavier initialization of weights. Image source: Justin Johnson.](../images/8-init-xavier.png){ width=70% }

-->
</section>
<section id="convolutional-units" class="slide level3">
<h3>Convolutional units</h3>
<!-- 

https://ajcr.net/stride-guide-part-2/

Explaining how it is implemented "flat" by matrix multiplication
https://ca.meron.dev/blog/Vectorized-CNN/

-->
</section>
<section id="problems-with-fully-connected-layers-for-images"
class="slide level3">
<h3>Problems with “fully connected” layers for images</h3>
<figure>
<img data-src="../images/deep-image-data.svg" style="width:60.0%"
alt="Problems with fully connected layers." />
<figcaption aria-hidden="true">Problems with fully connected
layers.</figcaption>
</figure>
<aside class="notes">
<p>Convolutional neural networks address two major problems that make it
difficult to train a “fully connected” neural network:</p>
<ol type="1">
<li>Each pixel is a feature, images tend to be very large, so a “fully
connected” layer requires a very large number of parameters. (Each
“neuron” in the first layer requires a weight for every pixel in the
image!)</li>
<li>Each learned weight corresponds to specific pixels in the image. If
the relevant pixels are at a different position in the image, the
weights that were learned are not helpful for that other position.
(i.e. it is not <em>spatial translation invariant</em>.)</li>
</ol>
</aside>
</section>
<section id="using-convolution-to-address-the-problem"
class="slide level3">
<h3>Using convolution to address the problem</h3>
<figure>
<img data-src="../images/deep-image-conv.svg" style="width:60.0%"
alt="Addressing the problem." />
<figcaption aria-hidden="true">Addressing the problem.</figcaption>
</figure>
<aside class="notes">
<p>With a convolutional layer (not “fully connected”):</p>
<ol type="1">
<li>The unit connects to one “patch” of the image at a time, so it only
needs as many weights as there are pixels in the patch. The same weights
are <em>shared</em> as the unit moves across the image, one patch at a
time.</li>
<li>It can “match” a specific arrangement of pixels <em>anywhere</em> it
occurs in the image, not only one specific location.</li>
</ol>
</aside>
</section>
<section id="the-convolution-operation---one-patch"
class="slide level3">
<h3>The convolution operation - one “patch”</h3>
<figure>
<img data-src="../images/deep-image-conv-op.png" style="width:80.0%"
alt="Convolution on one patch." />
<figcaption aria-hidden="true">Convolution on one patch.</figcaption>
</figure>
<aside class="notes">
<ul>
<li>Layer has a set of learnable “filters” (illustration shows one
filter)</li>
<li>Each filter has small width and height, but full depth</li>
<li>During forward pass, filter “slides” across width and height of
input, and computes dot product</li>
<li>Effectively performs “convolution”</li>
</ul>
</aside>
</section>
<section id="the-convolution-operation---stride-and-padding"
class="slide level3">
<h3>The convolution operation - stride and padding</h3>
<figure>
<img data-src="../images/deep-conv-stride-pad.png" style="width:80.0%"
alt="Stride and padding." />
<figcaption aria-hidden="true">Stride and padding.</figcaption>
</figure>
</section>
<section id="the-convolution-operation---full-depth"
class="slide level3">
<h3>The convolution operation - full depth</h3>
<figure>
<img data-src="../images/deep-image-conv-depth.png" style="width:80.0%"
alt="Convolution across full depth." />
<figcaption aria-hidden="true">Convolution across full
depth.</figcaption>
</figure>
</section>
<section id="the-convolution-operation---multiple-filters"
class="slide level3">
<h3>The convolution operation - multiple filters</h3>
<figure>
<img data-src="../images/deep-conv-multi-filter.png" style="width:80.0%"
alt="Convolution with multiple filters." />
<figcaption aria-hidden="true">Convolution with multiple
filters.</figcaption>
</figure>
<aside class="notes">
<p>Note that the output is a 3D volume - can be input to another conv
layer!</p>
<p>Basic dimension arithmetic:</p>
<ul>
<li>Accepts input volume <span class="math inline">\(W_1 \times H_1
\times D_1\)</span></li>
<li>Four hyperparameters: number of filters <span
class="math inline">\(K\)</span>, filter size <span
class="math inline">\(F\)</span>, stride <span
class="math inline">\(S\)</span>, amount of zero padding <span
class="math inline">\(P\)</span></li>
<li>Produces volume of size</li>
</ul>
<p><span class="math display">\[W_2 = \frac{W_1 - F + 2P}{S} + 1 , H_2
=  \frac{H_1 - F + 2P}{S} + 1 \]</span> <span class="math display">\[D_2
= K \]</span></p>
<ul>
<li>With parameter sharing: <span class="math inline">\(F \cdot F \cdot
D_1\)</span> weights per filter, for <span class="math inline">\(F \cdot
F \cdot D_1 \cdot K\)</span> weights and <span
class="math inline">\(K\)</span> biases</li>
</ul>
</aside>
</section>
<section id="the-convolution-operation---all-together"
class="slide level3">
<h3>The convolution operation - all together</h3>
<figure>
<img data-src="../images/convolution-example.png" style="width:90.0%"
alt="Animated demo at https://cs231n.github.io/assets/conv-demo/index.html" />
<figcaption aria-hidden="true">Animated demo at <a
href="https://cs231n.github.io/assets/conv-demo/index.html">https://cs231n.github.io/assets/conv-demo/index.html</a></figcaption>
</figure>
<p>Basic insight - parameter sharing:</p>
<ul>
<li>A particular filter with a set of weights represents a feature to
look for</li>
<li>If it is useful to look for a feature at position <span
class="math inline">\(x,y\)</span>, it is probably useful to look for
the same feature at <span
class="math inline">\(x&#39;,y&#39;\)</span></li>
<li>All neurons within a “depth slice” can share the same weights.</li>
</ul>
<!-- 

### Receptive field



Important for VGG -->
</section>
<section id="activation" class="slide level3">
<h3>Activation</h3>
<ul>
<li>Convolutional typically followed by non-linear activation function
e.g. ReLU</li>
<li>Several Conv + ReLU layers may be followed by a <em>pooling</em>
layer</li>
</ul>
</section>
<section id="pooling-layer" class="slide level3">
<h3>Pooling layer</h3>
<figure>
<img data-src="../images/deep-conv-pool.png" style="width:70.0%"
alt="Example of pooling layer." />
<figcaption aria-hidden="true">Example of pooling layer.</figcaption>
</figure>
<aside class="notes">
<ul>
<li>No parameters! Just applies an aggregating function</li>
<li>Typically uses max operation (other possible operations: mean,
median)</li>
<li>Reduces spatial size of image (reduce computation, prevent
overfitting)</li>
<li>Typical example: 2x2 filter size, stride of 2, downsamples by a
factor of 2 along width and height</li>
<li>Works independently on each depth slice</li>
</ul>
<p>Pooling math:</p>
<ul>
<li>Accepts input volume <span class="math inline">\(W_1 \times H_1
\times D_1\)</span></li>
<li>Two hyperparameters: filter size <span
class="math inline">\(F\)</span>, stride <span
class="math inline">\(S\)</span></li>
<li>Produces volume of size</li>
</ul>
<p><span class="math display">\[W_2 = \frac{W_1 - F}{S} + 1 , H_2
=  \frac{H_1 - F}{S} + 1 , D_2 = D_1 \]</span></p>
</aside>
</section>
<section id="the-typical-lenet-like-architecture" class="slide level3">
<h3>The typical “LeNet”-like architecture</h3>
<figure>
<img data-src="../images/deep-conv-lenet-like.png" style="width:80.0%"
alt="“LeNet”-like architecture." />
<figcaption aria-hidden="true">“LeNet”-like architecture.</figcaption>
</figure>
</section>
<section id="actual-lenet-5-1998" class="slide level3">
<h3>Actual LeNet-5 (1998)</h3>
<figure>
<img data-src="../images/deep-conv-lenet-5.png" style="width:100.0%"
alt="LeNet-5." />
<figcaption aria-hidden="true">LeNet-5.</figcaption>
</figure>
</section>
<section id="recurrent-neural-networks" class="slide level3">
<h3>Recurrent neural networks</h3>
<ul>
<li>Where ConvNet find <em>spatial</em> patterns wherever they occur in
images,</li>
<li>RNNs find <em>temporal patterns</em> wherever they occur in a
sequence</li>
</ul>
</section></section>
<section>
<section id="normalization" class="title-slide slide level2">
<h2>Normalization</h2>
<ul>
<li><strong>Input standardization</strong></li>
<li><strong>Batch normalization</strong></li>
</ul>
</section>
<section id="data-pre-processing" class="slide level3">
<h3>Data pre-processing</h3>
<p>You can make the loss surface much “nicer” by pre-processing:</p>
<ul>
<li>Remove mean (zero center)</li>
<li>Normalize (divide by standard deviation)</li>
<li>OR decorrelation (whitening/rotation)</li>
</ul>
<aside class="notes">
<p>There are several reasons why this helps. We already discussed the
“ravine” in the loss function that is created by correlated
features.</p>
<!-- What about zero-centering and normalization? Think about a binary classification problem of a data cloud that is far from the origin, vs. one close to the origin. In which case will the loss function react more (be more sensitive) to a small change in weights?

![The classifier on the right is more sensitive to small changes in the weights.](../images/8-pre-processing.png){ width=30% }

-->
<!-- Note: Whitening/decorrelation is not applied to image data. For image data, we sometimes subtract the "mean image" or the per-color mean. -->
</aside>
</section>
<section id="data-preprocessing-1" class="slide level3">
<h3>Data preprocessing (1)</h3>
<figure>
<img data-src="../images/8-preprocessing-1.jpeg" style="width:50.0%"
alt="Image source: Stanford CS231n." />
<figcaption aria-hidden="true">Image source: Stanford
CS231n.</figcaption>
</figure>
</section>
<section id="data-preprocessing-2" class="slide level3">
<h3>Data preprocessing (2)</h3>
<figure>
<img data-src="../images/8-preprocessing-2.jpeg" style="width:50.0%"
alt="Image source: Stanford CS231n." />
<figcaption aria-hidden="true">Image source: Stanford
CS231n.</figcaption>
</figure>
<aside class="notes">
<p>Input standardization helps with the first hidden layer, but what
about the intermediate hidden layers?</p>
</aside>
</section>
<section id="batch-normalization" class="slide level3">
<h3>Batch normalization</h3>
<ul>
<li>Re-center and re-scale between layers</li>
<li>Training: Mean and standard deviation per training mini-batch</li>
<li>Test: Using fixed statistics</li>
</ul>
</section></section>
<section id="gradient-descent" class="title-slide slide level2">
<h2>Gradient descent</h2>
<!-- 
### Standard ("batch") gradient descent

For each step $t$ along the error curve:

$$W^{t+1} = W^t - \alpha \nabla L(W^t) = W^t - \frac{\alpha}{N} \sum_{i=1}^N \nabla L_i(W^t, \mathbf{x}_i, y_i)$$


Repeat until stopping criterion is met.


### Stochastic gradient descent 

Idea: at each step, compute estimate of gradient using only one randomly selected sample, and move in the direction it indicates.

Many of the steps will be in the wrong direction, but progress towards minimum occurs *on average*, as long as the steps are small.

Bonus: helps escape local minima. 


### Mini-batch (also "stochastic") gradient descent

Idea: In each step, select a small subset of training data ("mini-batch"), and evaluate gradient on that mini-batch. Then move in the direction it indicates.

For each step $t$ along the error curve: 

* Select random mini-batch $I_t\subset{1,\ldots,N}$
* Compute gradient approximation: $g^t = \frac{1}{|I_t|} \sum_{i\in I_t} \nabla L(\mathbf{x}_i, y_i, W)$
* Update parameters: $W^{t+1} = W^t - \alpha^t g^t$


### Comparison: batch size

![Effect of batch size on gradient descent.](../images/grad-descent-comparison.png){ width=50% }


### Why does mini-batch gradient help? (Intuition)

* Standard error of mean over $m$ samples is $\frac{\sigma}{\sqrt{m}}$, where $\sigma$ is standard deviation.
* The benefit of more examples in reducing error is less than linear!
* Example: gradient based on 10,000 samples requires 100x more computation than one based on 100 samples, but reduces SE only 10x.
* Also: memory required scales with mini-batch size.
* Also: there is often redundancy in training set.


### Gradient descent terminology

* Mini-batch size is $B$, training size is $N$
* A training *epoch* is the sequence of updates over which we see all non-overlapping mini-batches
* There are $\frac{N}{B}$ steps per training epoch
* Data shuffling: at the beginning of each epoch, randomly shuffle training samples. Then, select mini-batches in order from shuffled samples.


### Selecting the learning rate


![Choice of learning rate $\alpha$ is critical](../images/learning_rate_comparison.png){ width=55%}

### Annealing the learning rate

One approach: decay learning rate slowly over time, such as 

* Exponential decay: $\alpha_t = \alpha_0 e^{-k t}$
* 1/t decay: $\alpha_t = \alpha_0 / (1 + k t )$ 

(where $k$ is tuning parameter).


### Gradient descent in a ravine (1)

![Gradient descent path bounces along ridges of ravine, because surface curves much more steeply in direction of $w_1$.](../images/ravine-grad-descent.png){width=40%}

### Gradient descent in a ravine (2)

![Gradient descent path bounces along ridges of ravine, because surface curves much more steeply in direction of $w_1$.](../images/ravine-grad-descent2.png){width=40%}


### Momentum (1)

* Idea:  Update includes a *velocity* vector $v$, that accumulates gradient of past steps. 
* Each update is a linear combination of the gradient and the previous updates. 
* (Go faster if gradient keeps pointing in the same direction!)


### Momentum (2)

Classical momentum: for some $0 \leq \gamma_t < 1$,

$$v_{t+1} = \gamma_t v_t - \alpha_t \nabla L\left(W_t\right)$$

so

$$W_{t+1} = W_t + v_{t+1} = W_t  - \alpha_t \nabla L\left(W_t\right) + \gamma_t v_t$$

($\gamma_t$ is often around 0.9, or starts at 0.5 and anneals to 0.99 over many epochs.)


### Momentum: illustrated

![Momentum dampens oscillations by reinforcing the component along $w_2$ while canceling out the components along $w_1$.](../images/ravine-momentum.png){width=50%}

### RMSProp

Idea: Track *per-parameter* EWMA of *square* of gradient, and use it to adapt learning rate. 


$$W_{t+1,i} = W_{t,i} -\frac{\alpha}
{\sqrt {\epsilon + E[g^2]_t }} \nabla L(W_{t,i})$$

where 

$$E[g^2]_t=(1-\gamma)g^2 + \gamma E[g^2]_{t-1}, \quad g = \nabla J(W_{t,i})$$


::: notes

Weights with recent gradients of large magnitude have smaller learning rate, weights with small recent gradients have larger learning rates.


:::


### RMSProp: illustrated (Beale's function)


![Animation credit: Alec Radford. [Link to view animation](https://imgur.com/a/Hqolp).](../images/beale-gradient.gif){width=40%}


::: notes

Due to the large initial gradient, velocity based techniques shoot off and bounce around, RMSProps proceed more like faster SGD.

:::

### RMSProp: illustrated (Long valley)


![Animation credit: Alec Radford. [Link to view animation](https://imgur.com/a/Hqolp). ](../images/long-valley-gradient.gif){width=40%}

::: notes

SGD stalls and momentum has oscillations until it builds up velocity in optimization direction. Algorithms that scale step size quickly break symmetry and descend in optimization direction.

:::

### Adam: Adaptive moments estimation (2014)
 
Idea: Track the EWMA of *both* first and second moments of the gradient, $\{m_t, v_t\}$ at each time $t$. 

If $L_t(W)$ is evaluation of loss function on a mini-batch of data at time $t$, 

$$
\begin{aligned}
\{m_t, v_t\}, \mathbb{E}[m_t] \, \, &\approx \, \, \mathbb{E}[\,\nabla \, L_t(W)\,], \mathbb{E}[v_t] \,  \\
                                  \, &\approx \, \, \mathbb{E}\big[\,(\nabla \, L_t(W))^2\,\big]
 \end{aligned}
 $$

Scale $\alpha$ by $\frac{m_t}{\sqrt{v_t}}$ at each step.

-->
</section>

<section>
<section id="regularization" class="title-slide slide level2">
<h2>Regularization</h2>
<ul>
<li><strong>L2 or L1 regularization</strong></li>
<li><strong>Early stopping</strong></li>
<li><strong>Dropout</strong></li>
</ul>
</section>
<section id="l1-or-l2-regularization" class="slide level3">
<h3>L1 or L2 regularization</h3>
<aside class="notes">
<p>As with other models, we can add a penalty on the norm of the
weights.</p>
<p>Normal gradient descent update rule:</p>
<p><span class="math display">\[w_{i,j}^{t+1} = w_{i,j}^t - \alpha
\frac{\partial L}{\partial w_{i,j}^t} \]</span></p>
<p>With L2 regularization:</p>
<p><span class="math display">\[w_{i,j}^{t+1} = w_{i,j}^t - \alpha (
\frac{\partial L}{\partial w_{i,j}^t} + \frac{2 \lambda}{n}  w_{i,j}^t
)\]</span></p>
<p>Often called “weight decay” in the context of neural nets.</p>
</aside>
</section>
<section id="early-stopping" class="slide level3">
<h3>Early stopping</h3>
<aside class="notes">
<ul>
<li>Compute validation loss each performance</li>
<li>Stop training when validation loss hasn’t improved in a while</li>
<li>Risk of stopping <em>too</em> early</li>
</ul>
<p>Important: <em>must</em> divide data into training, validation, and
test data - use validation data (not test data!) to decide when to stop
training.</p>
<!-- 

Why does it work? Some ideas:

* The network is effectively "smaller" when we stop training early, because many units still in linear region of activation.
* Earlier layers (which learn simpler features) and late layers (near the output - used for response mapping) converge to their final weights first. See [Boaz Barak](https://windowsontheory.org/2021/02/17/what-do-deep-networks-learn-and-when-do-they-learn-it/).

-->
<!-- See "Why early stopping works" https://www.cs.toronto.edu/~guerzhoy/411/lec/W05/overfitting_prevent.pdf -->
<!-- See https://windowsontheory.org/2021/02/17/what-do-deep-networks-learn-and-when-do-they-learn-it/ -->
</aside>
</section>
<section id="dropout" class="slide level3">
<h3>Dropout</h3>
<figure>
<img data-src="../images/8-dropout.jpeg" style="width:35.0%"
alt="Dropout networks." />
<figcaption aria-hidden="true">Dropout networks.</figcaption>
</figure>
<aside class="notes">
<ul>
<li>During each training step: some portion of neurons are randomly
“dropped”.</li>
<li>During each test step: don’t “drop” any neurons, but we need to
scale activations by dropout probability</li>
</ul>
<p>Why does it work? Some ideas:</p>
<ul>
<li>Forces some redundancy, makes neurons learn robust
representation</li>
<li>Effectively training an ensemble of networks (with shared
weights)</li>
</ul>
<p>Note: when you use Dropout layers, you may notice that the
validation/test loss seems better than the training loss! Why?</p>
</aside>
<!--

Neural networks of all types: https://www.asimovinstitute.org/neural-network-zoo/

-->
<aside class="notes">
<hr />
<p>Big picture: What are all these techniques for?</p>
<ul>
<li>Allow us to train models with more ‘capacity’</li>
<li>Improve performance even without adding ‘capacity’</li>
</ul>
</aside>
</section></section>
<section
id="example-deep-neural-nets-33-years-ago-and-33-years-from-now"
class="title-slide slide level2">
<h2>Example: Deep Neural Nets: 33 years ago and 33 years from now</h2>
<figure>
<img data-src="../images/deep-nets-markmap-example.png"
style="width:50.0%" alt="Techniques we will apply in this example." />
<figcaption aria-hidden="true">Techniques we will apply in this
example.</figcaption>
</figure>
<aside class="notes">
<p>In the Colab lesson, we will reproduce a 1989 paper about a neural
network for handwritten digit classification, that was used in the late
90s to process 10-20% of all checks in the US.</p>
<ul>
<li>The original paper had 5% error, our realization has about
4.14%</li>
<li>With a bunch of these changes (but keeping the basic network the
same) we get to 2.09%</li>
<li>The original model without any changes, but with more training data,
gets to 3.05%</li>
<li>With our changes + more data, we get to 1.31%</li>
</ul>
<p>What does it mean that we can do this without changing the basic
network? It means the network always had the <em>capacity</em> to do
this well, but wasn’t learning the best weights.</p>
</aside>
</section>

<section id="example-progress-on-imagenet"
class="title-slide slide level2">
<h2>Example: Progress on ImageNet</h2>
<ul>
<li>Data: 1.2M images from 1000 categories</li>
<li>ImageNet Large Scale Visual Recognition Challenge (ILSVRC): running
since 2010</li>
</ul>
<aside class="notes">
<figure>
<img data-src="../images/deep-conv-imagenet.png" style="width:80.0%"
alt="Progress on ImageNet." />
<figcaption aria-hidden="true">Progress on ImageNet.</figcaption>
</figure>
<p>If you want to learn more, including receptive field, 1x1
convolution, and ConvNet architectures, I recommend:</p>
<ul>
<li><a
href="https://fullstackdeeplearning.com/spring2021/lecture-2a/">Lecture
2A</a> and <a
href="https://fullstackdeeplearning.com/spring2021/lecture-2b/">Lecture
2B</a> from FSDL 2021</li>
<li><a
href="https://d2l.ai/chapter_convolutional-neural-networks/index.html">Chapter
7</a> and <a
href="https://d2l.ai/chapter_convolutional-modern/index.html">Chapter
8</a> of Dive into Deep Learning</li>
</ul>
</aside>
</section>
    </div>
  </div>

  <script src="reveal.js-master/dist/reveal.js"></script>

  <!-- reveal.js plugins -->
  <script src="reveal.js-master/plugin/notes/notes.js"></script>
  <script src="reveal.js-master/plugin/search/search.js"></script>
  <script src="reveal.js-master/plugin/zoom/zoom.js"></script>
  <script src="reveal.js-master/plugin/math/math.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
        // Display controls in the bottom right corner
        controls: true,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: true,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'bottom-right',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: false,

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: false,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: true,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'default',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: true,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'slide',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'fade',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        math: {
          mathjax: '/usr/share/javascript/mathjax/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [
          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    </body>
</html>
