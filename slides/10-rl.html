<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="author" content="Fraida Fund">
  <title>Reinforcement learning</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="reveal.js-master/dist/reset.css">
  <link rel="stylesheet" href="reveal.js-master/dist/reveal.css">
  <style>
    .reveal .sourceCode {  /* see #7635 */
      overflow: visible;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="reveal.js-master/dist/theme/white.css" id="theme">
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <h1 class="title">Reinforcement learning</h1>
  <p class="author">Fraida Fund</p>
</section>

<section>
<section id="reinforcement-learning" class="title-slide slide level2">
<h2>Reinforcement learning</h2>

</section>
<section id="elements-of-rl" class="slide level3">
<h3>Elements of RL</h3>
<ul>
<li>An <em>agent</em> acts in an <em>environment</em></li>
<li>The agent sees a sequence of <em>observations</em> about the
environment</li>
<li>The agent wants to achieve a <em>goal</em>, in spite of some
<em>uncertainty</em> about the environment.</li>
</ul>
<aside class="notes">
<p>May need to consider indirect, delayed result of actions.</p>
</aside>
</section>
<section id="elements-of-rl---environment" class="slide level3">
<h3>Elements of RL - environment</h3>
<ul>
<li>The <em>state</em> of the agent at time <span
class="math inline">\(t\)</span> is <span
class="math inline">\(S_t\)</span> (from <span class="math inline">\(s
\in \mathcal{S}\)</span>)</li>
<li>The agent chooses action <span class="math inline">\(A_t\)</span> at
time <span class="math inline">\(t\)</span> (from <span
class="math inline">\(a \in \mathcal{A}\)</span>)</li>
<li>The agent earns a reward <span class="math inline">\(R_t\)</span>
for its actions (possibly stochastic)</li>
<li>The next state is determined by current state and current action,
using a (possibly stochastic) state transition function <span
class="math inline">\(\delta(s,a)\)</span>:</li>
</ul>
<p><span class="math display">\[P(s&#39;, r \vert s, a)  = \mathbb{P}
[S_{t+1} = s&#39;, R_{t+1} = r \vert S_t = s, A_t = a]\]</span></p>
<aside class="notes">
<p>The set of states <span class="math inline">\(\mathcal{S}\)</span>,
actions <span class="math inline">\(\mathcal{A}\)</span>, the reward,
and the state transition function, “live” outside the agent - part of
the environment.</p>
</aside>
</section>
<section id="elements-of-rl---observations" class="slide level3">
<h3>Elements of RL - observations</h3>
<p>Over interactions in <span class="math inline">\(T\)</span> time
steps, the agent takes a sequence of actions and observes next states
and rewards.</p>
<p>This sequence of interactions is called a <em>trajectory</em>:</p>
<p><span class="math display">\[S_1, A_1, R_2, S_2, A_2, \dots,
S_T\]</span></p>
</section>
<section id="elements-of-rl---what-agent-may-learn"
class="slide level3">
<h3>Elements of RL - what agent may learn</h3>
<ul>
<li>the <em>policy</em> <span class="math inline">\(\pi\)</span> is the
agent’s mapping from state to action (or probabilities of action). We
will always have a policy at the end, but it won’t always be explicitly
learned.</li>
<li>We already said that the environment sends a <em>reward</em> back to
the agent. The agent may learn a <em>value function</em> that describes
expected total <strong>future</strong> reward from a state.</li>
<li>The agent may have/learn a <em>model</em> of the environment, which
we can use to <strong>plan</strong> before or during interactions with
the environment</li>
</ul>
</section>
<section id="taxonomy-of-rl-agents" class="slide level3">
<h3>Taxonomy of RL agents</h3>
<aside class="notes">
<figure>
<img data-src="../images/10-taxnomy-rl.png" style="width:40.0%"
alt="Taxonomy of RL agents." />
<figcaption aria-hidden="true">Taxonomy of RL agents.</figcaption>
</figure>
<!-- Via [this blog post](https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#exploration-exploitation-dilemma) -->
<ul>
<li>Policy-based: learn an explicit representation of policy <span
class="math inline">\(\pi: S \rightarrow A\)</span>.</li>
<li>Value-based: try to learn what is the expected total reward for each
state or state-action pair. We still end up with a policy, but it’s not
learned directly. For example, we might use a greedy policy: always pick
the action that leads to the best expected reward, according to the
value function that we learned.</li>
<li>Actor-critic methods use both policy and value function
learning.</li>
<li>Model-based: uses either a known or learned model <em>of
environment</em>.</li>
<li>Model-free: does not know or try to explicitly learn a model <em>of
environment</em>.</li>
<li>(Model-free methods interact with the environment by
trial-and-error, where model-based methods can plan for future
situations by computation on the model.)</li>
</ul>
<!--
* On-policy: Use the deterministic outcomes or samples from the target policy to train the algorithm.
* Off-policy: Training on a distribution of transitions or episodes produced by a different behavior policy rather than that produced by the target policy.
-->
</aside>
</section></section>
<section>
<section id="the-optimization-problem" class="title-slide slide level2">
<h2>The optimization problem</h2>

</section>
<section id="reward" class="slide level3">
<h3>Reward</h3>
<p>Suppose the state transition function is</p>
<p><span class="math display">\[P(s&#39;, r \vert s, a)  = \mathbb{P}
[S_{t+1} = s&#39;, R_{t+1} = r \vert S_t = s, A_t = a]\]</span></p>
<p>the reward for a state-action will be</p>
<p><span class="math display">\[ R(s, a) = \mathbb{E} [R_{t+1} \vert S_t
= s, A_t = a] = \sum_{r\in\mathcal{R}} r \sum_{s&#39; \in \mathcal{S}}
P(s&#39;, r \vert s, a) \]</span></p>
<aside class="notes">
<p>The state transition function gives the probability of transitioning
from state <span class="math inline">\(s\)</span> to <span
class="math inline">\(s&#39;\)</span> after taking action <span
class="math inline">\(a\)</span>, while obtaining reward <span
class="math inline">\(r\)</span>.</p>
</aside>
</section>
<section id="policy" class="slide level3">
<h3>Policy</h3>
<p>We want a <em>policy</em>, or a probability distribution over actions
for a given state:</p>
<p><span class="math display">\[\pi(a \vert s) = \mathbb{P}_\pi [A=a
\vert S=s]\]</span></p>
</section>
<section id="value-function" class="slide level3">
<h3>Value function</h3>
<p>Let future reward (<strong>return</strong>) from time <span
class="math inline">\(t\)</span> on be</p>
<p><span class="math display">\[G_t = R_{t+1} + \gamma R_{t+2} + \dots =
\sum_{k=0}^{\infty} \gamma^k R_{t+k+1}\]</span></p>
<p>where the discount factor <span class="math inline">\(0 &lt; \gamma
&lt; 1\)</span> penalizes future reward.</p>
</section>
<section id="state-value" class="slide level3">
<h3>State-value</h3>
<p>The state-value of a state <span class="math inline">\(s\)</span> is
the expected return if we are in the state at time t:</p>
<p><span class="math display">\[V_{\pi}(s) = \mathbb{E}_{\pi}[G_t \vert
S_t = s]\]</span></p>
</section>
<section id="action-value" class="slide level3">
<h3>Action-value</h3>
<p>The action value of a state-action pair is</p>
<p><span class="math display">\[Q_{\pi}(s, a) = \mathbb{E}_{\pi}[G_t
\vert S_t = s, A_t = a]\]</span></p>
</section>
<section id="relationship-between-q-and-v" class="slide level3">
<h3>Relationship between Q and V</h3>
<p>For a policy <span class="math inline">\(\pi\)</span>, we can sum the
action values weighted by the probability of that action to get:</p>
<p><span class="math display">\[V_{\pi}(s) = \sum_{a \in \mathcal{A}}
Q_{\pi}(s, a) \pi(a \vert s)\]</span></p>
</section>
<section id="action-advantage-function" class="slide level3">
<h3>Action advantage function</h3>
<p>The difference between them is the action advantage:</p>
<p><span class="math display">\[A_{\pi}(s, a) = Q_{\pi}(s, a) -
V_{\pi}(s)\]</span></p>
<aside class="notes">
<p>“Taking this action in this state” vs. “getting to this state.”</p>
</aside>
</section>
<section id="optimal-value-function" class="slide level3">
<h3>Optimal value function</h3>
<p>The optimal value function maximizes the return (future expected
reward):</p>
<p><span class="math display">\[V_{*}(s) = \max_{\pi}
V_{\pi}(s)\]</span> <span class="math display">\[Q_{*}(s, a) =
\max_{\pi} Q_{\pi}(s, a)\]</span></p>
</section>
<section id="optimal-policy" class="slide level3">
<h3>Optimal policy</h3>
<p>The optimal policy achieves the optimal value functions:</p>
<p><span class="math display">\[\pi_{*} = \arg\max_{\pi}
V_{\pi}(s)\]</span> <span class="math display">\[\pi_{*} =
\arg\max_{\pi} Q_{\pi}(s, a)\]</span></p>
<p>i.e. <span class="math inline">\(V_{\pi_{*}}(s)=V_{*}(s)\)</span> and
<span class="math inline">\(Q_{\pi_{*}}(s, a) = Q_{*}(s,
a)\)</span>.</p>
</section>
<section id="optimal-policy-breakdown" class="slide level3">
<h3>Optimal policy breakdown</h3>
<p>We can also think of it as the policy that maximizes current reward +
discounted value of next state:</p>
<p><span class="math display">\[\pi_{*} = \arg\max_{\pi} r(s,a) + \gamma
V^*_{\pi}(\delta(s,a))\]</span></p>
<aside class="notes">
<p>From here on, we are going to assume a value based RL agent, and we
will discuss strategies for learning the value function.</p>
<p>But first: what type of policy will the RL agent use?</p>
</aside>
</section>
<section id="exploration-and-exploitation" class="slide level3">
<h3>Exploration and exploitation</h3>
<p>If the policy is always to take the best action we know about, we
might miss out on learning about other, better actions!</p>
<ul>
<li><strong>exploration</strong>: take some action to find out about
environment, even if you may miss out on some reward</li>
<li><strong>exploitation</strong>: take the action that maximizes
reward, based on what you know about the environment.</li>
</ul>
</section>
<section id="epsilon-greedy-policy" class="slide level3">
<h3><span class="math inline">\(\epsilon\)</span>-greedy policy</h3>
<ul>
<li>With probability <span class="math inline">\(\epsilon\)</span>,
choose random action</li>
<li>With probability <span class="math inline">\(1-\epsilon\)</span>,
choose optimal</li>
</ul>
<p>Can decay <span class="math inline">\(\epsilon\)</span> over
time.</p>
<aside class="notes">
<p>There are many alternatives, e.g. an upper confidence bound policy,
where we trade off actions that we know are optimal vs actions about
whose effect we are less certain.</p>
<p>In either case: you would still use a greedy policy during inference!
It’s just during training that you would use a policy that includes both
exploitation and exploration. (We call this “off-policy” when we use a
different policy during training and inference.)</p>
<p>Now that we have a policy - we need to learn a value function. And of
course, we want to learn from <em>experience</em>.</p>
</aside>
</section>
<section id="monte-carlo" class="slide level3">
<h3>Monte Carlo</h3>
<p>We could update the value function for a state after the end of a
<em>complete</em> experience, e.g. after observing <span
class="math inline">\(G_t\)</span>.</p>
<p><span class="math display">\[V(S_t) \leftarrow V(S_t) + \alpha[G_t -
V(S_t)]\]</span></p>
<p>This is the Monte Carlo method.</p>
<aside class="notes">
<p>The new value of state <span class="math inline">\(S_t\)</span> is
the previous estimate of the value of state <span
class="math inline">\(S_t\)</span>, plus learning rate times:</p>
<ul>
<li><span class="math inline">\(G_t\)</span>, the return after step
<span class="math inline">\(t\)</span> (observed)</li>
<li>minus previous estimate of value of state <span
class="math inline">\(S_t\)</span> (estimated)</li>
</ul>
<p>However, this only considers the return of an entire experience -
does not consider which states/actions in the experience were useful,
and which were not.</p>
</aside>
</section>
<section id="td-learning" class="slide level3">
<h3>TD learning</h3>
<p>Instead, we could update the value function after a single step,
e.g. after observing <span class="math inline">\(R_{t+1}\)</span> and
<span class="math inline">\(S_{t+1}\)</span> but no more.</p>
<p><span class="math display">\[V(S_t) \leftarrow V(S_t) +
\alpha[R_{t+1} + \gamma V(S_{t+1}) - V(S_t)]\]</span></p>
<p>This is called temporal difference (TD) learning.</p>
<aside class="notes">
<p>The new value of state <span class="math inline">\(S_t\)</span> is
the previous estimate of the value of state <span
class="math inline">\(S_t\)</span>, plus learning rate times:</p>
<ul>
<li>immediate reward (observed)</li>
<li>plus discounted value of next state (next state is observed, its
value is estimated)</li>
<li>minus previous estimate of value of state <span
class="math inline">\(S_t\)</span> (estimated)</li>
</ul>
<p>Here we have essentially broken down <span
class="math inline">\(G_t\)</span> into <span
class="math inline">\(R_{t+1}\)</span> (immediate reward) and <span
class="math inline">\(\gamma V(S_{t+1})\)</span> (discounted future
reward).</p>
</aside>
</section></section>
<section>
<section id="q-learning" class="title-slide slide level2">
<h2>Q learning</h2>
<aside class="notes">
<p>As an example, we will look more closely at Q learning, which</p>
<ul>
<li>is value-based</li>
<li>uses TD learning</li>
<li>and learns the action-value function</li>
</ul>
</aside>
</section>
<section id="q-table" class="slide level3">
<h3>Q table</h3>
<ul>
<li>Each row/column is an action</li>
<li>Each column/row is a state</li>
<li>Table stores current estimate <span
class="math inline">\(\hat{Q}(s,a)\)</span></li>
</ul>
<aside class="notes">
<figure>
<img data-src="../images/rl-q-learning-initial.png" style="width:80.0%"
alt="Example agent, environment, and Q table." />
<figcaption aria-hidden="true">Example agent, environment, and Q
table.</figcaption>
</figure>
</aside>
</section>
<section id="iterative-approximation" class="slide level3">
<h3>Iterative approximation</h3>
<ul>
<li><p>start with zero values</p></li>
<li><p>observe state <span class="math inline">\(S_t\)</span>, then
iteratively:</p>
<ul>
<li>choose action <span class="math inline">\(A_t\)</span> and
execute,</li>
<li>observe immediate reward <span
class="math inline">\(R_{t+1}\)</span> and new state <span
class="math inline">\(S_{t+1}\)</span></li>
<li>update <span class="math inline">\(\hat{Q}(S_t,A_t)\)</span>
using</li>
</ul></li>
</ul>
<p><span class="math display">\[Q(S_t, A_t) \leftarrow Q(S_t, A_t) +
\alpha[R_{t+1} + \gamma \max_{a} Q(S_{t+1}, a) - Q(S_t,
A_t)]\]</span></p>
<ul>
<li>go to new state <span class="math inline">\(S_{t+1}\)</span>.</li>
</ul>
<aside class="notes">
<p>New Q value estimate is the previous estimate, plus learning rate
times:</p>
<ul>
<li>immediate reward (observed)</li>
<li>plus discounted estimate of optimal Q value of next state (optimal,
using greedy policy - not epsilon greedy!)</li>
<li>minus previous estimate of Q value</li>
</ul>
</aside>
</section>
<section id="q-table---after-two-steps" class="slide level3">
<h3>Q table - after two steps</h3>
<aside class="notes">
<figure>
<img data-src="../images/rl-q-learning-after.png" style="width:80.0%"
alt="After two steps." />
<figcaption aria-hidden="true">After two steps.</figcaption>
</figure>
<p>In practice, we want to learn environments with very large state
space where we cannot enumerate a Q table like this. But, in place of a
lookup table, we can learn underlying relationships using e.g. deep
neural networks <span class="math inline">\(\rightarrow\)</span> deep Q
learning.</p>
</aside>
<!--

### Loss function

Bellman error: MSE between current $\hat{Q}$ and next one:

$$L = \frac{1}{2} (r_t + \gamma \max_{a'} \hat{Q}(s,a') - \hat{Q}(s,a))^2$$ 

### Gradient descent rule

$$Q(s,a) \leftarrow Q(s,a) + \eta \left[ r + \gamma \max_{a'} Q(s',a') - Q(s,a) \right]$$
-->
</section></section>
    </div>
  </div>

  <script src="reveal.js-master/dist/reveal.js"></script>

  <!-- reveal.js plugins -->
  <script src="reveal.js-master/plugin/notes/notes.js"></script>
  <script src="reveal.js-master/plugin/search/search.js"></script>
  <script src="reveal.js-master/plugin/zoom/zoom.js"></script>
  <script src="reveal.js-master/plugin/math/math.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
        // Display controls in the bottom right corner
        controls: true,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: true,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'bottom-right',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: false,

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: false,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: true,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'default',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: true,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'slide',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'fade',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [
          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    </body>
</html>
