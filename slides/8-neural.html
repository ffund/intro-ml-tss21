<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="author" content="Fraida Fund">
  <title>Neural networks</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="reveal.js-master/dist/reset.css">
  <link rel="stylesheet" href="reveal.js-master/dist/reveal.css">
  <style>
    .reveal .sourceCode {  /* see #7635 */
      overflow: visible;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="reveal.js-master/dist/theme/white.css" id="theme">
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <h1 class="title">Neural networks</h1>
  <p class="author">Fraida Fund</p>
</section>

<section id="in-this-lecture" class="title-slide slide level2">
<h2>In this lecture</h2>
<ul>
<li>Neural network</li>
<li>Structure of a neural network</li>
<li>Training a neural network</li>
</ul>
<aside class="notes">
<p><strong>Math prerequisites for this lecture</strong>: You should know
about:</p>
<ul>
<li>derivatives and especially the chain rule (Appendix C in Boyd and
Vandenberghe)</li>
</ul>
</aside>
</section>

<section>
<section id="from-linear-to-non-linear"
class="title-slide slide level2">
<h2>From linear to non-linear</h2>

</section>
<section id="representation-as-a-computational-graph"
class="slide level3">
<h3>Representation as a computational graph</h3>
<aside class="notes">
<p>Let’s represent the linear regression and logistic regression models
using a computational graph.</p>
<figure>
<img data-src="../images/7-reg-comp-graph.png" style="width:60.0%"
alt="Regression and classification models." />
<figcaption aria-hidden="true">Regression and classification
models.</figcaption>
</figure>
<p>We can use also do a linear regression or logistic regression with a
basis function transformation applied to the data first. Here, we have
one “transformation” node for each basis function, and then the output
of those “transformation” nodes become the input to the logistic
regression (or linear regression).</p>
<figure>
<img data-src="../images/7-lbf-graph-trans.png" style="width:65.0%"
alt="With a basis function transformation." />
<figcaption aria-hidden="true">With a basis function
transformation.</figcaption>
</figure>
<p>We can also represent the SVM with a linear or non-linear kernel
using a computational graph.</p>
<p>Here, we have one “transformation node” for each training sample!
(The “transformation” is the kernel function, which is computed over the
input sample and the training sample).</p>
<p>Then the weighted sum of those nodes (weighted by <span
class="math inline">\(\alpha_i\)</span>, which is learned by the SVM,
and which is zero for every non-support vector training sample and
non-zero for every support vector training sample) is used to compute
the class label.</p>
<figure>
<img data-src="../images/7-svm-graph.png" style="width:65.0%"
alt="SVM computational graph." />
<figcaption aria-hidden="true">SVM computational graph.</figcaption>
</figure>
<p>In those regression and classification models, we use a fixed basis
function to transform features. We only learned the weights to map the
transformed features to a continuous output (regression) or to a class
label (classification).</p>
<p>Would it be possible to also learn the first part - the mapping of
the features to a transformed feature space?</p>
<figure>
<img data-src="../images/7-lbf-comp-graph.png" style="width:65.0%"
alt="Can we learn this mapping to transformed feature space?" />
<figcaption aria-hidden="true">Can we learn this mapping to transformed
feature space?</figcaption>
</figure>
</aside>
</section>
<section id="example-synthetic-data" class="slide level3">
<h3>Example: synthetic data</h3>
<figure>
<img data-src="../images/two-class-nn-synthetic.png" style="width:30.0%"
alt="Example via Sundeep Rangan" />
<figcaption aria-hidden="true">Example via <a
href="https://github.com/sdrangan/introml/blob/master/unit09_neural/demo1_synthetic.ipynb">Sundeep
Rangan</a></figcaption>
</figure>
</section>
<section id="model-of-example-two-stage-network-1" class="slide level3">
<h3>Model of example two-stage network (1)</h3>
<p>First step (<em>hidden layer</em>):</p>
<ul>
<li>Take <span class="math inline">\(N_H=4\)</span> “logistic
regression” nodes.</li>
<li>Use <span class="math inline">\(\mathbf{x}\)</span> as input to each
node.</li>
<li>At each node <span class="math inline">\(m\)</span>, first compute:
<span class="math inline">\(z_{H,m} = \mathbf{w}_{H,m}^T
\mathbf{x}\)</span></li>
<li>Then, at each node, apply a sigmoid: <span
class="math inline">\(u_{H,m} = g_H(z_{H,m}) =
\frac{1}{1+e^{-z_{H,m}}}\)</span></li>
</ul>
<aside class="notes">
<p>Note: assume a 1s column was added to the data matrix, so we don’t
need a separate intercept term.</p>
<figure>
<img data-src="../images/7-computational-graph-H1.png"
style="width:30.0%" alt="Computation at one hidden node." />
<figcaption aria-hidden="true">Computation at one hidden
node.</figcaption>
</figure>
<figure>
<img data-src="../images/7-computational-graph-H4.png"
style="width:50.0%" alt="Computation at four hidden nodes." />
<figcaption aria-hidden="true">Computation at four hidden
nodes.</figcaption>
</figure>
<p>At this point, we have some representation of the data in <span
class="math inline">\(\mathbb{R}^4\)</span>.</p>
</aside>
</section>
<section id="model-of-example-two-stage-network-2" class="slide level3">
<h3>Model of example two-stage network (2)</h3>
<p>Second step (<em>output layer</em>):</p>
<ul>
<li>At output node, first compute: <span class="math inline">\(z_O =
\mathbf{w}^T_O [1, \mathbf{u}_H]\)</span></li>
<li>Then, compute: <span class="math inline">\(u_O = g_O(z_O) =
\frac{1}{1+e^{-z_{O}}}\)</span></li>
<li>(Not in the graph): apply a threshold to get <span
class="math inline">\(\hat{y}\)</span></li>
</ul>
<aside class="notes">
<p>Notes:</p>
<ul>
<li>we use the output of the previous layer as input to this layer</li>
<li>as with the first layer, we add a 1s column to the input, to take
care of the intercept term.</li>
</ul>
<figure>
<img data-src="../images/7-computational-graph-output.png"
style="width:55.0%" alt="Two-stage network." />
<figcaption aria-hidden="true">Two-stage network.</figcaption>
</figure>
<p>What does the output look like (over the feature space) at each
node?</p>
</aside>
</section>
<section id="example-output-of-each-hidden-node" class="slide level3">
<h3>Example: output of each hidden node</h3>
<figure>
<img data-src="../images/two-class-nn-neurons.png" style="width:60.0%"
alt="Example via Sundeep Rangan" />
<figcaption aria-hidden="true">Example via <a
href="https://github.com/sdrangan/introml/blob/master/unit09_neural/demo1_synthetic.ipynb">Sundeep
Rangan</a></figcaption>
</figure>
</section>
<section id="example-output-of-output-node" class="slide level3">
<h3>Example: output of output node</h3>
<figure>
<img data-src="../images/two-class-nn-avgout.png" style="width:20.0%"
alt="Via Sundeep Rangan" />
<figcaption aria-hidden="true">Via <a
href="https://github.com/sdrangan/introml/blob/master/unit09_neural/demo1_synthetic.ipynb">Sundeep
Rangan</a></figcaption>
</figure>
</section>
<section id="matrix-form-of-two-stage-network" class="slide level3">
<h3>Matrix form of two stage network</h3>
<ul>
<li>Hidden layer: <span class="math inline">\(\mathbf{z}_H =
\mathbf{W}_H^T \mathbf{x}, \quad \mathbf{u}_H =
g_H(\mathbf{z}_H)\)</span></li>
<li>Output layer: <span class="math inline">\(z_O = \mathbf{W}_O^T [1,
\mathbf{u}_H], \quad u_O = g_O(\mathbf{z}_O)\)</span></li>
</ul>
</section></section>
<section>
<section id="neural-networks" class="title-slide slide level2">
<h2>Neural networks</h2>
<!-- 
### Biological inspiration

![A biological neuron accepts inputs via dendrites, "adds" them together within cell body, and once some electrical potential is reached, "fires" via axons. Synaptic weight (the influence the firing of one neuron has on another) changes over time ("learning"). Image via: [http://http://doi.org/10.5772/51275](http://http://doi.org/10.5772/51275) ](../images/biological-neuron.png){width=55%}

-->
</section>
<section id="terminology" class="slide level3">
<h3>Terminology</h3>
<ul>
<li><strong>Hidden variables</strong>: the variables <span
class="math inline">\(\mathbf{z}_H, \mathbf{u}_H\)</span>, which are not
directly observed.</li>
<li><strong>Hidden units</strong>: the nodes that compute the hidden
variables.</li>
<li><strong>Activation function</strong>: the function <span
class="math inline">\(g(z)\)</span></li>
<li><strong>Output units</strong>: the node(s) that compute <span
class="math inline">\(z_{O}\)</span>.</li>
</ul>
</section>
<section id="setting-up-a-neural-network---givens" class="slide level3">
<h3>Setting up a neural network - givens</h3>
<p>For a particular problem, these are “given”:</p>
<ul>
<li>the number of inputs</li>
<li>the number of outputs</li>
<li>the activation function to use at the output</li>
<li>the loss function</li>
</ul>
<aside class="notes">
<p>The number of inputs comes from the data - what is the size of each
training sample?</p>
<p>The number of outputs is dictated by the type of problem -</p>
<ul>
<li>binary classification: we need to predict <span
class="math inline">\(P(y=1)\)</span> which is a single quantity, so we
have one output unit.</li>
<li>multi-class classification: we will predict <span
class="math inline">\(P(y=k)\)</span> for each class <span
class="math inline">\(k\)</span>, so we need an output unit for each
class.</li>
<li>regression: if we need to predict a single target, we will have one
output unit. If we need to predict multiple values in the same problem
(vector output), we will have as many output units as there are values
in the output.</li>
</ul>
<p>Similarly, the activation function at the output unit and the loss
function are dictated by the problem!</p>
</aside>
</section>
<section id="binary-classification" class="slide level3">
<h3>Binary classification…</h3>
<p>For binary classification, <span class="math inline">\(y \in
[0,1]\)</span>:</p>
<ul>
<li>Use sigmoid activation, output will be: <span
class="math inline">\(u_O = P(y=1| x)=\frac{1}{1+e^{-z_O}}\)</span></li>
<li><span class="math inline">\(u_O\)</span> is scalar - need one output
node</li>
<li>Use binary cross entropy loss:</li>
</ul>
<p><span class="math display">\[L(\mathbf{W}) = \sum_{i=1}^n -y_i z_{Oi}
+ \text{ln}(1+e^{z_{Oi}})\]</span></p>
<aside class="notes">
<p>Then you select the predicted label by applying a threshold to the
output <span class="math inline">\(u_O\)</span>.</p>
<p>The mapping from transformed feature space to output is just like a
logistic regression - we haven’t changed <em>that</em> part!</p>
</aside>
</section>
<section id="multi-class-classification" class="slide level3">
<h3>Multi-class classification…</h3>
<p>For multi-class classification, <span
class="math inline">\(y=1,\ldots, K\)</span>:</p>
<ul>
<li>Use softmax activation, output will be: <span
class="math inline">\(u_{O, k} = P(y=k|
x)=\frac{e^{z_{O,k}}}{\sum_{\ell=1}^K e^{z_\ell}}\)</span></li>
<li><span class="math inline">\(u_O\)</span> is vector <span
class="math inline">\([u_0, \ldots, u_K]\)</span> - need <span
class="math inline">\(K\)</span> output nodes</li>
<li>Use categorical cross entropy loss:</li>
</ul>
<p><span class="math display">\[L(\mathbf{W}) = \sum_{i=1}^n \left[ \ln
\left(\sum_k e^{z_{Oik}}\right) - \sum_k  r_{ik}
z_{Oik}\right]\]</span></p>
<aside class="notes">
<p>Then you can select predicted label by <span
class="math inline">\(\hat{y} = \operatorname*{argmax}_k
u_{O,k}\)</span></p>
</aside>
</section>
<section id="regression-with-one-output" class="slide level3">
<h3>Regression with one output…</h3>
<p>For regression, <span class="math inline">\(y \in R^{1}\)</span>:</p>
<ul>
<li>Use linear activation, output will be: <span
class="math inline">\(u_O = z_O\)</span></li>
<li><span class="math inline">\(u_O\)</span> is scalar - need one output
node</li>
<li>Use L2 loss:</li>
</ul>
<p><span class="math display">\[L(\mathbf{W}) = \sum_{i=1}^n (y_i -
z_{Oi})^2\]</span></p>
</section>
<section id="regression-with-multiple-outputs" class="slide level3">
<h3>Regression with multiple outputs…</h3>
<p>For regression, <span class="math inline">\(y \in R^{K}\)</span>:</p>
<ul>
<li>Use linear activation, output will be: <span
class="math inline">\(u_{O,k} = z_{O,k}\)</span></li>
<li><span class="math inline">\(u_O\)</span> is vector <span
class="math inline">\([u_0, \ldots, u_K]\)</span> - need <span
class="math inline">\(K\)</span> output nodes</li>
<li>Use vector L2 loss:</li>
</ul>
<p><span class="math display">\[L(\mathbf{W}) = \sum_{i=1}^n
\sum_{k=1}^K (y_{ik} - z_{Oik})^2\]</span></p>
</section>
<section id="setting-up-a-neural-network---decisions"
class="slide level3">
<h3>Setting up a neural network - decisions</h3>
<p>We still need to decide:</p>
<ul>
<li>the number of hidden units</li>
<li>the activation function to use at hidden units</li>
</ul>
</section>
<section id="dimension-1" class="slide level3">
<h3>Dimension (1)</h3>
<ul>
<li><span class="math inline">\(N_I\)</span> = input dimension, number
of features</li>
<li><span class="math inline">\(N_H\)</span> = number of hidden units,
you decide!</li>
<li><span class="math inline">\(N_O\)</span> = output dimension, number
of outputs</li>
</ul>
</section>
<section id="dimension-2" class="slide level3">
<h3>Dimension (2)</h3>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">Parameter</th>
<th style="text-align: left;">Symbol</th>
<th style="text-align: left;">Number of parameters</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Hidden layer: weights</td>
<td style="text-align: left;"><span
class="math inline">\(W_H\)</span></td>
<td style="text-align: left;"><span class="math inline">\(N_H (N_I +
1)\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">Output layer: weights</td>
<td style="text-align: left;"><span
class="math inline">\(W_O\)</span></td>
<td style="text-align: left;"><span class="math inline">\(N_O (N_H +
1)\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Total</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><span
class="math inline">\(N_H(N_I+1)+N_O(N_H+1)\)</span></td>
</tr>
</tbody>
</table>
</section>
<section id="activation-functions-at-hidden-layer-identity"
class="slide level3">
<h3>Activation functions at hidden layer: identity?</h3>
<ul>
<li>Suppose we use <span class="math inline">\(g(z) = z\)</span>
(identity function) as activation function throughout the network.</li>
<li>The network can only achieve linear decision boundary!</li>
<li>To get non-linear decision boundary, need non-linear activation
functions.</li>
</ul>
<aside class="notes">
<p>Universal approximation theorem: under certain conditions, with
enough (finite) hidden nodes, can approximate <em>any</em> continuous
real-valued function, to any degree of precision. But only with
non-linear decision boundary! (See <a
href="http://neuralnetworksanddeeplearning.com/chap4.html">this post</a>
for a convincing demonstration.)</p>
<p>(The more hidden units we have, the more complex a function we can
represent.)</p>
<p>By scaling, shifting, and adding a bunch of “step” or “step-like”
functions, you can approximate a complicated function. What step-like
function can you use?</p>
</aside>
</section>
<section id="activation-functions-at-hidden-layer-binary-step"
class="slide level3">
<h3>Activation functions at hidden layer: binary step</h3>
<ul>
<li>Not differentiable at <span class="math inline">\(x=0\)</span>, has
<span class="math inline">\(0\)</span> derivative everywhere else.</li>
<li>Not useful for gradient-based optimization methods.</li>
</ul>
</section>
<section id="activation-functions-at-hidden-layer-some-choices"
class="slide level3">
<h3>Activation functions at hidden layer: some choices</h3>
<figure>
<img data-src="../images/activation-functions.png" style="width:50.0%"
alt="Most common activation functions" />
<figcaption aria-hidden="true">Most common activation
functions</figcaption>
</figure>
<aside class="notes">
<p>What do they have in common?</p>
<ul>
<li>Differentiable (at least from one side)</li>
<li>Non-linear (except for the linear one, which is only used as the
output function for a regression)</li>
</ul>
</aside>
</section></section>
<section>
<section id="neural-network---summary" class="title-slide slide level2">
<h2>Neural network - summary</h2>

</section>
<section id="things-that-are-given" class="slide level3">
<h3>Things that are “given”</h3>
<p>For a particular problem, these are “given”:</p>
<ul>
<li>the number of inputs</li>
<li>the number of outputs</li>
<li>the activation function to use at the output</li>
<li>the loss function</li>
</ul>
</section>
<section id="things-that-we-decide" class="slide level3">
<h3>Things that we decide</h3>
<p>We still need to decide:</p>
<ul>
<li>the number of hidden units</li>
<li>the activation function to use at hidden units</li>
</ul>
</section>
<section id="training-the-network" class="slide level3">
<h3>Training the network</h3>
<ul>
<li>Still need to find the <span
class="math inline">\(\mathbf{W}\)</span> that minimizes <span
class="math inline">\(L(\mathbf{W})\)</span>.</li>
<li>How?</li>
</ul>
</section></section>
<section>
<section id="backpropagation" class="title-slide slide level2">
<h2>Backpropagation</h2>
<!-- 


### Training the two-stage network for binary classification

* From final stage: $z_o = F(\mathbf{x}, \mathbf{W})$ where parameters $\mathbf{W} = (\mathbf{W}_H, \mathbf{W}_o)$
* Given training data $(\mathbf{x}_i, y_i), i = 1, \ldots, n$ 
* Loss function $L(\mathbf{W}) := -\sum_{i=1}^n \text{ln} P(y_i | \mathbf{x}_i, \mathbf{W})$
* Choose parameters to minimize loss: $\hat{\mathbf{W}} = \operatorname*{argmin}_{\mathbf{W}} L(\mathbf{W})$


::: notes

(Using negative log likelihood/binary cross-entropy loss function from the logistic regression lesson. )

How do we choose the parameters in the last step? We'll use *gradient descent* on the computational graph.

:::

-->
</section>
<section id="how-to-compute-gradients" class="slide level3">
<h3>How to compute gradients?</h3>
<ul>
<li>Gradient descent requires computation of the gradient <span
class="math inline">\(\nabla L(\mathbf{W})\)</span></li>
<li>Backpropagation is key to efficient computation of gradients</li>
</ul>
<aside class="notes">
<p>We need to compute the gradient of the loss function with respect to
<em>every</em> weight, and there are <span
class="math inline">\(N_H(N_I+1)+N_O(N_H+1)\)</span> weights!</p>
<p>The key to efficient computation will be:</p>
<ul>
<li>saving all the intermediate (hidden) variables on the forward pass,
to reuse in the computation of gradients</li>
<li>computing the gradients in a <em>backwards</em> pass - going from
output to input, and accumulating local derivatives along the path
(you’ll see!)</li>
</ul>
</aside>
</section>
<section id="composite-functions-and-computational-graphs"
class="slide level3">
<h3>Composite functions and computational graphs</h3>
<p>Suppose we have a composite function <span
class="math inline">\(f(g(h(x)))\)</span></p>
<p>We can represent it as a computational graph, where each connection
is an input and each node performs a function or operation:</p>
<figure>
<img data-src="../images/9-composite-function.png" style="width:50.0%"
alt="Composite function." />
<figcaption aria-hidden="true">Composite function.</figcaption>
</figure>
<!-- 
\begin{tikzpicture}
  \node[circle] (n1) at (1,1) {$x$};
  \node[circle,fill=blue!20] (n2) at (4,1)  {$v=h(x)$};
  \node[circle,fill=blue!20] (n3) at (8,1)  {$u=g(v)$};
  \node[circle,fill=blue!20] (n4) at (11,1) {$f(u)$};
  \draw [->] (n1) -- (n2) node[midway, above] {};
  \draw [->] (n2) -- (n3) node[midway, above] {};
  \draw [->] (n3) -- (n4) node[midway, above] {};
\end{tikzpicture}
-->
</section>
<section id="forward-pass-on-computational-graph" class="slide level3">
<h3>Forward pass on computational graph</h3>
<p>To compute the output <span
class="math inline">\(f(g(h(x)))\)</span>, we do a <em>forward pass</em>
on the computational graph:</p>
<ul>
<li>Compute <span class="math inline">\(v=h(x)\)</span></li>
<li>Compute <span class="math inline">\(u=g(v)\)</span></li>
<li>Compute <span class="math inline">\(f(u)\)</span></li>
</ul>
<figure>
<img data-src="../images/9-composite-forward.png" style="width:50.0%"
alt="Forward pass." />
<figcaption aria-hidden="true">Forward pass.</figcaption>
</figure>
<aside class="notes">
<p>Note that we <em>accumulate</em> results in the forward direction -
at each node, we use the output of the previous node, which depends on
the output of <em>all</em> the previous nodes. But, we don’t need to
repeat the steps of <em>all</em> the previous nodes each time, since the
output is “accumulated” forward.</p>
</aside>
</section>
<section id="derivative-of-composite-function" class="slide level3">
<h3>Derivative of composite function</h3>
<p>Suppose we need to compute the derivative of the composite function
<span class="math inline">\(f(g(h(x)))\)</span> with respect to <span
class="math inline">\(x\)</span>.</p>
<p>We will use the chain rule:</p>
<p><span class="math display">\[\frac{df}{dx} = \frac{df}{du}
\frac{dg}{dv} \frac{dh}{dx}\]</span></p>
</section>
<section id="backward-pass-on-computational-graph" class="slide level3">
<h3>Backward pass on computational graph</h3>
<p>We can compute this chain rule derivative by doing a <em>backward
pass</em> on the computational graph:</p>
<figure>
<img data-src="../images/9-composite-backward.png" style="width:55.0%"
alt="Backward pass." />
<figcaption aria-hidden="true">Backward pass.</figcaption>
</figure>
<aside class="notes">
<p>As in the forward pass, in the backward pass, we do a “local”
operation at each node: the “local” derivative of each node with respect
to its inputs (shown in rectangles on each edge).</p>
<p>As in the forward pass, we <em>accumulate</em> results, but now in
the backward direction. At each node, the derivative of the output with
respect to the value computed at that node is:</p>
<ul>
<li>The product of all the “local” derivatives along the path between
the node and the output.</li>
<li>or equivalently, the product of the derivative at the previous node
in the backward pass, and the “local” derivative along the path from
that node.</li>
</ul>
<p>For example: when we compute <span
class="math inline">\(\frac{df}{dx}\)</span> at the last “stop” along
the backwards pass, we don’t need to compute all the parts of <span
class="math inline">\(\frac{df}{du}\frac{dg}{dv}\frac{du}{dx}\)</span>
<em>again</em>. We just need:</p>
<ul>
<li>compute “local” gradient <span
class="math inline">\(\frac{dh}{dx}\)</span></li>
<li>and multiply it by the “accumulated” gradient computed at the
previous node, <span class="math inline">\(\frac{df}{dv}\)</span></li>
</ul>
<p>This seems obvious… but when we apply it to a neural network, we will
see why it is so important.</p>
</aside>
<!-- 

\begin{tikzpicture}
  \node[circle] (n1) at (1,1) {$x$};
  \node[circle,fill=blue!20] (n2) at (4,1)  {$v=h(x)$};
  \node[circle,fill=blue!20] (n3) at (8,1)  {$u=g(v)$};
  \node[circle,fill=blue!20] (n4) at (11,1) {$f(u)$};
  \draw [<-] (n1) -- (n2) node[midway, above] {$\frac{dh}{dx}$};
  \draw [<-] (n2) -- (n3) node[midway, above] {$\frac{dg}{dv}$};
  \draw [<-] (n3) -- (n4) node[midway, above] {$\frac{df}{du}$};
\end{tikzpicture}

-->
</section>
<section id="neural-network-computational-graph" class="slide level3">
<h3>Neural network computational graph</h3>
<figure>
<img data-src="../images/9-neural-computational.png" style="width:60.0%"
alt="Neural network as a computational graph." />
<figcaption aria-hidden="true">Neural network as a computational
graph.</figcaption>
</figure>
<aside class="notes">
<p>What about when we have multiple inputs, multiple hidden units?</p>
</aside>
<!-- 
\begin{tikzpicture}
  \node[circle, fill=green!20] (n1) at (1,1) {$x_i$};
  \node[circle,fill=purple!20] (n2) at (3,1)  {$z_{H,i}$};
  \node[circle,fill=purple!20] (n3) at (5,1)  {$u_{H,i}$};
  \node[circle,fill=purple!20] (n4) at (7,1) {$z_{O,i}$};
  \node[circle,fill=purple!20] (n5) at (9,1) {$u_{O,i}$};
  \node[circle,fill=orange!20] (n6) at (12,1) {$L(y_i, u_{O,i})$};
  \node[circle,fill=blue!20] (n7) at (1,4) {$W_H$};
  \node[circle,fill=blue!20] (n8) at (5,4) {$W_O$};


  \draw [->] (n1) -- (n2) node[midway, above] {};
  \draw [->] (n2) -- (n3) node[midway, above] {};
  \draw [->] (n3) -- (n4) node[midway, above] {};
  \draw [->] (n4) -- (n5) node[midway, above] {};
  \draw [->] (n5) -- (n6) node[midway, above] {};
  \draw [->] (n7) -- (n2) node[midway, above] {};
  \draw [->] (n8) -- (n4) node[midway, above] {};

\end{tikzpicture}
-->
</section>
<section id="backpropagation-error-definition" class="slide level3">
<h3>Backpropagation error: definition</h3>
<p>Denote the backpropagation error of node <span
class="math inline">\(j\)</span> as</p>
<p><span class="math display">\[\delta_j = \frac{\partial L}{\partial
z_j}\]</span></p>
<p>the derivative of the loss function, with respect to the input to the
activation function at that node.</p>
<aside class="notes">
<p>Spoiler: this <span class="math inline">\(\delta_j\)</span> is going
to be the “accumulated” part of the derivative.</p>
</aside>
</section>
<section id="output-unit-backpropagation-error-accumulated"
class="slide level3">
<h3>Output unit: backpropagation error (accumulated)</h3>
<p>Generally, for output unit <span
class="math inline">\(j\)</span>:</p>
<p><span class="math display">\[\delta_j = \frac{\partial L}{\partial
z_j} = \frac{\partial L}{\partial u_j}\frac{\partial u_j}{\partial
z_j}\]</span></p>
<aside class="notes">
<figure>
<img data-src="../images/9-output-backprop.png" style="width:50.0%"
alt="Computing backpropagation error at output unit." />
<figcaption aria-hidden="true">Computing backpropagation error at output
unit.</figcaption>
</figure>
<p>For example, in a regression network:</p>
<p><span class="math display">\[L =  \frac{1}{2}\sum_n (y_n -
z_{O,n})^2\]</span></p>
<p>Then <span class="math inline">\(\delta_O = \frac{\partial
L}{\partial z_O} = - (y - z_{O})\)</span>.</p>
</aside>
<!-- 
\begin{tikzpicture}
  \node[circle, fill=purple!20,minimum size=1cm] (n1) at (1,1) {};
  \node[circle,fill=purple!20,minimum size=1cm] (n2) at (1,5)  {};
  \node[label=$j$,circle,fill=purple!20,minimum size=1cm] (n3) at (5,3)  {};

  \draw [->] (n1) -- (n3) node[midway, above] {};
  \draw [->] (n2) -- (n3) node[midway, above] {$ u_i w_{j,i}$};
\end{tikzpicture}

-->
</section>
<section id="output-unit-derivative-vs-input-weights-local"
class="slide level3">
<h3>Output unit: derivative vs input weights (local)</h3>
<ul>
<li>At a node <span class="math inline">\(j\)</span>, <span
class="math inline">\(z_j = \sum_i w_{j,i} u_{i} = w_{j,i} u_i +
\ldots\)</span> (sum over inputs to the node)</li>
<li>When taking <span class="math inline">\(\frac{\partial z_j}{\partial
w_{j,i}}\)</span> the only term left is <span
class="math inline">\(w_{j,i} u_i\)</span></li>
<li>So <span class="math inline">\(\frac{\partial z_j}{\partial w_{j,i}
} = u_i\)</span></li>
</ul>
<aside class="notes">
<figure>
<img data-src="../images/9-output-weights.png" style="width:55.0%"
alt="Computing gradient with respect to weight at output unit." />
<figcaption aria-hidden="true">Computing gradient with respect to weight
at output unit.</figcaption>
</figure>
<p>The derivative of the loss with respect to a weight <span
class="math inline">\(w_{j,i}\)</span> input to the node, <span
class="math inline">\(\frac{\partial L}{\partial w_{j,i}}\)</span>, is
the product of:</p>
<ul>
<li><span class="math inline">\(\delta_j = \frac{\partial L}{\partial
z_j}\)</span> (the “accumulated” part)</li>
<li><span class="math inline">\(u_i = \frac{\partial z_j}{\partial
w_{j,i}}\)</span> (the “local” part)</li>
</ul>
<p>so finally, <span class="math inline">\(\frac{\partial L}{\partial
w_{j,i}} = \delta_j u_i\)</span>.</p>
<p>(We save the computations of all the <span
class="math inline">\(u_i\)</span> values from the forward pass, so that
we can reuse them for backpropagation.)</p>
</aside>
</section>
<section id="hidden-unit-backpropagation-error-accumulated"
class="slide level3">
<h3>Hidden unit: backpropagation error (accumulated)</h3>
<p>Sum the accumulated gradient along output paths:</p>
<p><span class="math display">\[
\begin{aligned}
\delta_j \quad &amp;  =   \frac{\partial L}{\partial z_j} \\
         \quad &amp;  =  \sum_k \frac{\partial L}{\partial
z_k}\frac{\partial z_k}{\partial z_j} \\
         \quad &amp;  =  \sum_k \delta_k \frac{\partial z_k}{\partial
z_j} \\
         \quad &amp;  =  \sum_k \delta_k w_{k,j}g_j&#39;(z_j)
\end{aligned}
\]</span></p>
<aside class="notes">
<figure>
<img data-src="../images/9-hidden-backprop.png" style="width:60.0%"
alt="Computing backpropagation error at hidden unit." />
<figcaption aria-hidden="true">Computing backpropagation error at hidden
unit.</figcaption>
</figure>
<p>Note: since we move from the output end of the network toward its
input, we have already accumulated <span
class="math inline">\(\delta_k\)</span> when we “visited” node <span
class="math inline">\(k\)</span>. So we the “new” computation is just
<span class="math inline">\(\frac{\partial z_k}{\partial
z_j}\)</span>.</p>
<p>We compute the next “accumulated” gradient using the previous
“accumulated” gradient and a “local” derivative.</p>
<p>Since</p>
<p><span class="math display">\[z_k = \sum_l w_{k,l} u_l\]</span></p>
<p>(sum over inputs to node <span class="math inline">\(k\)</span>), but
for the derivative with respect to <span
class="math inline">\(z_j\)</span> the only term left is <span
class="math inline">\(w_{k,j} u_j\)</span>. So,</p>
<p><span class="math display">\[
\begin{aligned}
\frac{\partial z_k}{\partial z_j} \quad
&amp;  =  \frac{\partial}{\partial z_j} w_{k,j} u_j \\
                                  \quad &amp; = \frac{\partial}{\partial
z_j} w_{k,j} g_j(z_j) \\
                                  \quad &amp;  =  w_{k,j}g_j&#39;(z_j)
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(g_j&#39;()\)</span> is the
derivative of the activation function. (We save <span
class="math inline">\(z_j\)</span> from the forward pass, so we can
reuse it here to compute <span
class="math inline">\(g_j&#39;(z_j)\)</span>.)</p>
</aside>
</section>
<section id="hidden-unit-derivative-vs-input-weights-local"
class="slide level3">
<h3>Hidden unit: derivative vs input weights (local)</h3>
<p>Same as output unit - <span class="math inline">\(\frac{\partial
z_j}{\partial w_{j,i} } = u_i\)</span></p>
<aside class="notes">
<figure>
<img data-src="../images/9-hidden-weights.png" style="width:65.0%"
alt="Computing gradient with respect to weight at hidden unit." />
<figcaption aria-hidden="true">Computing gradient with respect to weight
at hidden unit.</figcaption>
</figure>
<p>As at output unit, the derivative of the loss with respect to a
weight <span class="math inline">\(w_{j,i}\)</span> input to the node,
<span class="math inline">\(\frac{\partial L}{\partial
w_{j,i}}\)</span>, is the product of:</p>
<ul>
<li><span class="math inline">\(\delta_j = \frac{\partial L}{\partial
z_j}\)</span> (the “accumulated” part)</li>
<li><span class="math inline">\(u_i = \frac{\partial z_j}{\partial
w_{j,i}}\)</span> (the “local” part)</li>
</ul>
<p>so for a hidden unit, too, <span class="math inline">\(\frac{\partial
L}{\partial w_{j,i}} = \delta_j u_i\)</span></p>
</aside>
<!-- 
### Backpropagation error: unit with inputs and outputs illustration



\begin{tikzpicture}
  \node[circle, fill=purple!20,minimum size=1cm] (n1) at (1,1) {};
  \node[label=$i$,circle,fill=purple!20,minimum size=1cm] (n2) at (1,5)  {};
  \node[label=$j$,circle,fill=purple!20,minimum size=1cm] (n3) at (5,3)  {};
  \node[circle,fill=purple!20,minimum size=1cm] (n4) at (9,1)  {};
  \node[label=$k$,circle,fill=purple!20,minimum size=1cm] (n5) at (9,5)  {};

  \draw [->] (n1) -- (n3) node[midway, above] {};
  \draw [->] (n2) -- (n3) node[midway, above] {$ u_i w_{j,i}$};
  \draw [->] (n3) -- (n4) node[midway, above] {};
  \draw [->] (n3) -- (n5) node[midway, above] {$ u_j w_{k,j}$};
\end{tikzpicture}

-->
</section>
<section id="backpropagation-gradient-descent-algorithm-1"
class="slide level3">
<h3>Backpropagation + gradient descent algorithm (1)</h3>
<ol type="1">
<li>Start with random (small) weights. Apply input <span
class="math inline">\(x_n\)</span> to network and propagate values
forward using <span class="math inline">\(z_j = \sum_i w_{j,i}
u_i\)</span> and <span class="math inline">\(u_j = g(z_j)\)</span>. (Sum
is over all inputs to node <span class="math inline">\(j\)</span>.)</li>
<li>Evaluate <span class="math inline">\(\delta_j\)</span> for all
output units.</li>
</ol>
</section>
<section id="backpropagation-gradient-descent-algorithm-2"
class="slide level3">
<h3>Backpropagation + gradient descent algorithm (2)</h3>
<ol start="3" type="1">
<li>Backpropagate the <span class="math inline">\(\delta\)</span>s to
get <span class="math inline">\(\delta_j\)</span> for each hidden unit.
(Sum is over all outputs of node <span
class="math inline">\(j\)</span>.)</li>
</ol>
<p><span class="math display">\[\delta_j = g&#39;(z_j) \sum_k w_{k,j}
\delta_k\]</span></p>
</section>
<section id="backpropagation-gradient-descent-algorithm-3"
class="slide level3">
<h3>Backpropagation + gradient descent algorithm (3)</h3>
<ol start="4" type="1">
<li>Use <span class="math inline">\(\frac{\partial L_n}{\partial
w_{j,i}} = \delta_j u_i\)</span> to evaluate derivatives.</li>
<li>Update weights using gradient descent.</li>
</ol>
<!-- 

### Derivatives for common loss functions

Squared/L2 loss: 

$$L = \sum_i (y_i - z_{O,i})^2, \quad \frac{\partial L}{\partial z_{O,i}} = \sum_i -(y_i - z_{O,i})$$

Binary cross entropy loss: 

$$L = \sum_i -y_i z_{O,i} + \text{ln} (1 + e^{y_i z_{O,i}}), \quad \frac{\partial L}{\partial z_{O,i}} = y_i - \frac{ e^{y_i z_{O,i}} }{1 + e^{y_i z_{O,i}} }$$

### Derivatives for common activation functions

* Sigmoid activation: $g'(x) = \sigma(x) (1-\sigma(x))$
* Tanh activation: $g'(x) = \frac{1}{\text{cosh}^2(x)}$

-->
</section></section>
<section>
<section id="why-is-backpropagation-so-important"
class="title-slide slide level2">
<h2>Why is backpropagation so important?</h2>
<p>Example: <span class="math inline">\(e = (a + b) \times
(b+1)\)</span></p>
<figure>
<img data-src="../images/tree-eval-derivs.png" style="width:40.0%"
alt="Derivatives on a computational graph" />
<figcaption aria-hidden="true">Derivatives on a computational
graph</figcaption>
</figure>
<aside class="notes">
<p>Example via <a
href="https://colah.github.io/posts/2015-08-Backprop">https://colah.github.io/posts/2015-08-Backprop/</a>.</p>
</aside>
</section>
<section id="forward-mode-differentiation" class="slide level3">
<h3>Forward-mode differentiation</h3>
<figure>
<img data-src="../images/tree-forwradmode.png" style="width:40.0%"
alt="Forward-mode differentiation." />
<figcaption aria-hidden="true">Forward-mode
differentiation.</figcaption>
</figure>
<aside class="notes">
<p>With forward-mode differentiation, we take the derivative of the
outupt with respect to one input (e.g. <span
class="math inline">\(\frac{de}{db}\)</span>), by starting at the
<em>input</em> and “accumulating” the gradients toward the output.</p>
<p>However, if we want to take derivatives with respect to a
<em>different</em> input (e.g. input <span
class="math inline">\(a\)</span>), these accumulated gradients don’t
help - we need to compute all of the derivatives again.</p>
</aside>
</section>
<section id="reverse-mode-differentiation" class="slide level3">
<h3>Reverse-mode differentiation</h3>
<figure>
<img data-src="../images/tree-backprop.png" style="width:40.0%"
alt="Reverse-mode differentiation." />
<figcaption aria-hidden="true">Reverse-mode
differentiation.</figcaption>
</figure>
<aside class="notes">
<p>With reverse mode differentiation, we take the derivative of the
outupt with respect to one input (e.g. <span
class="math inline">\(\frac{de}{db}\)</span>), by starting at the
<em>output</em> and “accumulating” the gradients toward the input.</p>
<p>If we want to take derivatives with respect to a <em>different</em>
input (e.g. input <span class="math inline">\(a\)</span>), we already
have most of the accumulated gradients - we would just need to compute
one more local derivative near that input (<span
class="math inline">\(\frac{dc}{da}\)</span>).</p>
<p>For a problem where you need derivative of one output (loss) with
respect to many inputs (many weights), reverse mode differentiation is
very efficient because the accumulated gradients (<span
class="math inline">\(\delta\)</span> values) are computed once and then
reused many times.</p>
</aside>
</section></section>
    </div>
  </div>

  <script src="reveal.js-master/dist/reveal.js"></script>

  <!-- reveal.js plugins -->
  <script src="reveal.js-master/plugin/notes/notes.js"></script>
  <script src="reveal.js-master/plugin/search/search.js"></script>
  <script src="reveal.js-master/plugin/zoom/zoom.js"></script>
  <script src="reveal.js-master/plugin/math/math.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
        // Display controls in the bottom right corner
        controls: true,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: true,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'bottom-right',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: false,

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: false,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: true,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'default',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: true,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'slide',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'fade',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [
          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    </body>
</html>
