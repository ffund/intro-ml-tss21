<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="author" content="Fraida Fund">
  <title>Convolutional neural networks</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="reveal.js-master/dist/reset.css">
  <link rel="stylesheet" href="reveal.js-master/dist/reveal.css">
  <style>
    .reveal .sourceCode {  /* see #7635 */
      overflow: visible;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="reveal.js-master/dist/theme/white.css" id="theme">
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <h1 class="title">Convolutional neural networks</h1>
  <p class="author">Fraida Fund</p>
</section>

<section>
<section id="motivation" class="title-slide slide level2">
<h2>Motivation</h2>
<aside class="notes">
<p>People are good at recognizing objects in images.</p>
<p>Computers are bad at it! Why?</p>
</aside>
</section>
<section id="scene-conditions" class="slide level3">
<h3>Scene conditions</h3>
<figure>
<img data-src="../images/scene-conditions.png" style="width:50.0%"
alt="Difficult scene conditions: background clutter, occlusion…" />
<figcaption aria-hidden="true">Difficult scene conditions: background
clutter, occlusion…</figcaption>
</figure>
<!-- 
### Similarity and variability

![Must identify inter-class similarity, while accommodating intra-class variability.](../images/chairs.png){ width=50% }

-->
</section>
<section id="dimension" class="slide level3">
<h3>Dimension</h3>
<ul>
<li>Huge number of classes</li>
<li>Images can have millions of pixels</li>
</ul>
<p>For example, CIFAR-10: tiny images of size 32x32x3. One
<em>fully-connected</em> neuron in a first hidden layer of a regular NN
would have 3072 weights!</p>
</section>
<section id="object-can-be-anywhere-within-image" class="slide level3">
<h3>Object can be anywhere within image</h3>
<figure>
<img data-src="../images/mnist-sample.jpg" style="width:70.0%"
alt="MNIST sample. Find the “3” in the form?" />
<figcaption aria-hidden="true">MNIST sample. Find the “3” in the
form?</figcaption>
</figure>
</section></section>
<section>
<section id="convolutional-neural-networks"
class="title-slide slide level2">
<h2>Convolutional neural networks</h2>

</section>
<section id="key-idea" class="slide level3">
<h3>Key idea</h3>
<ul>
<li>Neuron is connected to a small part of image at a time (<em>locally
connected</em>)</li>
<li>By having multiple locally connected neurons covering the entire
image, we effectively “scan” the image</li>
</ul>
</section>
<section id="locally-connected-neurons-illustration"
class="slide level3">
<h3>Locally connected neurons: illustration</h3>
<figure>
<img data-src="../images/local-einstein.png" style="width:60.0%"
alt="Example: 200x200 image. Fully connected network with 400,000 hidden units, 16 billion parameters. Locally connected network with 400,000 hidden units in 10x10 fields, 40 million parameters." />
<figcaption aria-hidden="true">Example: 200x200 image. Fully connected
network with 400,000 hidden units, 16 billion parameters. Locally
connected network with 400,000 hidden units in 10x10 fields, 40 million
parameters.</figcaption>
</figure>
</section>
<section id="spatial-arrangement-conventional-networks"
class="slide level3">
<h3>Spatial arrangement: conventional networks</h3>
<figure>
<img data-src="../images/neural_net2.jpeg" style="width:50.0%"
alt="Conventional neural network: neurons don’t have spatial arrangement." />
<figcaption aria-hidden="true">Conventional neural network: neurons
don’t have spatial arrangement.</figcaption>
</figure>
</section>
<section id="spatial-arrangement-convolutional-networks"
class="slide level3">
<h3>Spatial arrangement: convolutional networks</h3>
<figure>
<img data-src="../images/cnn.jpeg" style="width:50.0%"
alt="CNN: input and output of each layer is a tensor, a multidimensional array with width, height, and depth. Preserves spatial relationships." />
<figcaption aria-hidden="true">CNN: input and output of each layer is a
<em>tensor</em>, a multidimensional array with width, height, and depth.
Preserves spatial relationships.</figcaption>
</figure>
</section></section>
<section>
<section id="layers-in-cnn" class="title-slide slide level2">
<h2>Layers in CNN</h2>

</section>
<section id="different-layer-types" class="slide level3">
<h3>Different layer types</h3>
<ul>
<li>Convolutional Layer</li>
<li>Pooling Layer</li>
<li>Fully-Connected Layer</li>
</ul>
<p>Each layer accepts an input 3D volume, transforms it to an output 3D
volume.</p>
</section>
<section id="convolutional-layer" class="slide level3">
<h3>Convolutional layer</h3>
<ul>
<li>Layer has a set of learnable “filters”</li>
<li>Each filter has small width and height, but full depth</li>
<li>During forward pass, filter “slides” across width and height of
input, and computes dot product</li>
<li>Effectively performs “convolution”</li>
</ul>
</section>
<section id="convolution-example" class="slide level3">
<h3>Convolution example</h3>
<figure>
<img data-src="../images/convolution-example.png" style="width:60.0%"
alt="Animated demo at https://cs231n.github.io/assets/conv-demo/index.html" />
<figcaption aria-hidden="true">Animated demo at <a
href="https://cs231n.github.io/assets/conv-demo/index.html">https://cs231n.github.io/assets/conv-demo/index.html</a></figcaption>
</figure>
</section>
<section id="feature-localization-via-convolution" class="slide level3">
<h3>Feature localization via “convolution”</h3>
<ul>
<li>Given large image <span class="math inline">\(X\)</span> with
dimensions <span class="math inline">\(N_1 \times N_2\)</span>,</li>
<li>small filter <span class="math inline">\(W\)</span> with dimensions
<span class="math inline">\(K_1 \times K_2\)</span></li>
</ul>
<p>At each offset <span class="math inline">\((j_1, j_2)\)</span> we
compute</p>
<p><span class="math display">\[Z[j_1, j_2] = \sum_{k_1=0}^{K_1-1}
\sum_{k_2=0}^{K_2-1} W[k_1, k_2]X[j_1 + k_1, j_2 + k_2]\]</span></p>
<p>which is large if “matching” feature is present.</p>
</section>
<section id="feature-localization-via-convolution-illustration"
class="slide level3">
<h3>Feature localization via convolution (illustration)</h3>
<figure>
<img data-src="../images/convolution-as-localization.png"
style="width:60.0%" alt="Finding features with convolution." />
<figcaption aria-hidden="true">Finding features with
convolution.</figcaption>
</figure>
</section>
<section id="local-connectivity" class="slide level3">
<h3>Local connectivity</h3>
<ul>
<li><p>Each filter is fully connected along depth axis, but only locally
connected along width and height.</p></li>
<li><p>Example: For CIFAR-10 (32x32x3), a 5x5 filter will have weights
to a (5x5x3) region in input volume.</p></li>
<li><p>Parameter dimensions: 75 weights and 1 bias. (much
smaller!)</p></li>
</ul>
</section>
<section id="size-of-output-volume" class="slide level3">
<h3>Size of output volume</h3>
<p>Size of output volume is determined by</p>
<ul>
<li>Input volume size <span class="math inline">\(W\)</span></li>
<li>depth</li>
<li>filter field size <span class="math inline">\(F\)</span></li>
<li>stride <span class="math inline">\(S\)</span></li>
<li>zero padding <span class="math inline">\(P\)</span></li>
</ul>
</section>
<section id="size-of-output-volume-depth" class="slide level3">
<h3>Size of output volume: depth</h3>
<p>Output depth is a hyperparameter: corresponds to number of filters
that should “look” at the same region of input at a time.</p>
<figure>
<img data-src="../images/depthcol.jpeg" style="width:20.0%"
alt="Example: this output depth is 5." />
<figcaption aria-hidden="true">Example: this output depth is
5.</figcaption>
</figure>
</section>
<section id="size-of-output-volume-stride" class="slide level3">
<h3>Size of output volume: stride</h3>
<p>How many pixels do we slide the filter each time? This is called the
<em>stride</em>.</p>
<figure>
<img data-src="../images/stride.jpeg" style="width:50.0%"
alt="In this example there is one spatial dimension (x-axis), one neuron with F = 3, W = 5, and P = 1. Left: S=1. Right: S=2." />
<figcaption aria-hidden="true">In this example there is one spatial
dimension (x-axis), one neuron with F = 3, W = 5, and P = 1. Left: S=1.
Right: S=2.</figcaption>
</figure>
</section>
<section id="size-of-output-volume-zero-padding" class="slide level3">
<h3>Size of output volume: zero-padding</h3>
<p>Use zero padding on border-</p>
<ul>
<li>Without padding, size would shrink in each layer.</li>
<li>Without padding, neurons “touch” the edges less often than the
middle</li>
</ul>
<p>To have output width and height the same as input, use <span
class="math inline">\(P=\frac{F-1}{2}\)</span>.</p>
</section>
<section id="summary-of-convolutional-layer" class="slide level3">
<h3>Summary of convolutional layer</h3>
<ul>
<li>Accepts input volume <span class="math inline">\(W_1 \times H_1
\times D_1\)</span></li>
<li>Four hyperparameters: number of filters <span
class="math inline">\(K\)</span>, filter size <span
class="math inline">\(F\)</span>, stride <span
class="math inline">\(S\)</span>, amount of zero padding <span
class="math inline">\(P\)</span></li>
<li>Produces volume of size</li>
</ul>
<p><span class="math display">\[W_2 = \frac{W_1 - F + 2P}{S} + 1 , H_2
=  \frac{H_1 - F + 2P}{S} + 1 \]</span> <span class="math display">\[D_2
= K \]</span></p>
<ul>
<li>With parameter sharing: <span class="math inline">\(F \cdot F \cdot
D_1\)</span> weights per filter, for <span class="math inline">\(F \cdot
F \cdot D_1 \cdot K\)</span> weights and <span
class="math inline">\(K\)</span> biases</li>
</ul>
<aside class="notes">
<ul>
<li>Common setting: <span class="math inline">\(F=3, S=1,
P=1\)</span>.</li>
</ul>
</aside>
</section>
<section id="parameter-sharing" class="slide level3">
<h3>Parameter sharing</h3>
<p>Basic insight:</p>
<ul>
<li>A particular filter with a set of weights represents a feature to
look for</li>
<li>If it is useful to look for a feature at position <span
class="math inline">\(x,y\)</span>, it is probably useful to look for
the same feature at <span
class="math inline">\(x&#39;,y&#39;\)</span></li>
<li>“Depth slice” = all the shifted versions of a filter. All neurons
within a depth slice can share the same weights.</li>
</ul>
<aside class="notes">
<p>Greatly reduces number of parameters.</p>
</aside>
</section>
<section id="example-alexnet-filters" class="slide level3">
<h3>Example: AlexNet filters</h3>
<figure>
<img data-src="../images/weights.jpeg" style="width:40.0%"
alt="Each of the 96 filters shown here is of size 11x11x3, and each one is shared by the 55x55 neurons in one depth slice." />
<figcaption aria-hidden="true">Each of the 96 filters shown here is of
size 11x11x3, and each one is shared by the 55x55 neurons in one depth
slice.</figcaption>
</figure>
</section>
<section id="relu-activation" class="slide level3">
<h3>ReLU activation</h3>
<ul>
<li>Convolutional typically followed by ReLU activation function</li>
</ul>
</section>
<section id="pooling-layer" class="slide level3">
<h3>Pooling layer</h3>
<ul>
<li>Reduces spatial size of image (reduce computation, prevent
overfitting)</li>
<li>Typical example: 2x2 filter size, stride of 2, downsamples by a
factor of 2 along width and height</li>
<li>Works independently on each depth slice</li>
<li>Typically uses MAX operation</li>
</ul>
</section>
<section id="pooling-illustration" class="slide level3">
<h3>Pooling: illustration</h3>
<figure>
<img data-src="../images/pool.jpeg" style="width:30.0%"
alt="Input volume of size 224x224x64 is pooled with filter size 2, stride 2 into output volume of size 112x112x64 (with same depth)." />
<figcaption aria-hidden="true">Input volume of size 224x224x64 is pooled
with filter size 2, stride 2 into output volume of size 112x112x64 (with
same depth).</figcaption>
</figure>
</section>
<section id="pooling-illustration-of-max-operation"
class="slide level3">
<h3>Pooling: illustration of max operation</h3>
<figure>
<img data-src="../images/maxpool.jpeg" style="width:30.0%"
alt="Each max is taken over a 2x2 square." />
<figcaption aria-hidden="true">Each max is taken over a 2x2
square.</figcaption>
</figure>
</section>
<section id="summary-of-pooling-layer" class="slide level3">
<h3>Summary of pooling layer</h3>
<ul>
<li>Accepts input volume <span class="math inline">\(W_1 \times H_1
\times D_1\)</span></li>
<li>Two hyperparameters: filter size <span
class="math inline">\(F\)</span>, stride <span
class="math inline">\(S\)</span></li>
<li>Produces volume of size</li>
</ul>
<p><span class="math display">\[W_2 = \frac{W_1 - F}{S} + 1 , H_2
=  \frac{H_1 - F}{S} + 1 , D_2 = D_1 \]</span></p>
<ul>
<li>No parameters</li>
</ul>
</section>
<section id="fully-connected-layer" class="slide level3">
<h3>Fully connected layer</h3>
<ul>
<li>Reshape into matrix</li>
<li>Output with matrix multiplication</li>
</ul>
<p><span class="math display">\[Z[i,k] = \sum_j W[j,k]U[i,j] + b[k],
\quad k=0,\ldots,N_O\]</span></p>
</section>
<section id="typical-architecture" class="slide level3">
<h3>Typical architecture</h3>
<ul>
<li>Input</li>
<li>Some number of convolutional + ReLU layers</li>
<li>Occasional pooling layers</li>
<li>Some number of fully connected + ReLU layers</li>
<li>Fully connected output layer</li>
</ul>
</section>
<section id="example" class="slide level3">
<h3>Example</h3>
<figure>
<img data-src="../images/convnet.jpeg" style="width:80.0%"
alt="Example of a convolutional network architecture. Live demo link." />
<figcaption aria-hidden="true">Example of a convolutional network
architecture. <a href="http://cs231n.stanford.edu/">Live demo
link.</a></figcaption>
</figure>
</section>
<section id="reference" class="slide level3">
<h3>Reference</h3>
<p>Source of most images here, and excellent set of notes on
convolutional neural networks:</p>
<p><a
href="https://cs231n.github.io/convolutional-networks/">https://cs231n.github.io/convolutional-networks/</a></p>
<!--

More stuff:

https://colab.research.google.com/gist/artificialsoph/b71c1c25b5ea86cb7ad3ab38afcbfb55/conv-data-generator.ipynb
https://soph.info/slash-data

-->
</section></section>
    </div>
  </div>

  <script src="reveal.js-master/dist/reveal.js"></script>

  <!-- reveal.js plugins -->
  <script src="reveal.js-master/plugin/notes/notes.js"></script>
  <script src="reveal.js-master/plugin/search/search.js"></script>
  <script src="reveal.js-master/plugin/zoom/zoom.js"></script>
  <script src="reveal.js-master/plugin/math/math.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
        // Display controls in the bottom right corner
        controls: true,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: true,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'bottom-right',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: false,

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: false,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: true,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'default',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: true,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'slide',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'fade',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [
          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    </body>
</html>
