<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="author" content="Fraida Fund">
  <meta name="dcterms.date" content="2020-02-03">
  <title>Assessing model performance</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@^4//dist/reset.css">
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@^4//dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@^4//dist/theme/white.css" id="theme">
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <h1 class="title">Assessing model performance</h1>
  <p class="author">Fraida Fund</p>
  <p class="date">February 3, 2020</p>
</section>

<section id="in-this-lecture" class="title-slide slide level2 cell markdown">
<h2>In this lecture</h2>
<ul>
<li>Performance metrics for classification</li>
<li>Case study: COMPAS</li>
<li>Fairness metrics for classification</li>
</ul>
</section>

<section id="evaluating-model-performance" class="title-slide slide level2 cell markdown">
<h2>Evaluating model performance</h2>
<ul>
<li>Suppose we have a series of data points <span class="math inline">\(\{(\mathbf{x_1},y_1),(\mathbf{x_2},y_2),\ldots,(\mathbf{x_n},y_n)\}\)</span> and there is some (unknown) relationship between <span class="math inline">\(\mathbf{x_i}\)</span> and <span class="math inline">\(y_i\)</span>.</li>
<li>We also have a black box  that, given some input <span class="math inline">\(\mathbf{x_i}\)</span>, will each produce as its output an estimate of <span class="math inline">\(y_i\)</span>, denoted <span class="math inline">\(\hat{y_i}\)</span>.</li>
<li>The question we will consider in this lecture - without knowing any details of the model - is <em>how can we judge the performance of the estimator</em>?</li>
</ul>
</section>

<section id="classifier-performance-metrics" class="title-slide slide level2 cell markdown">
<h2>Classifier performance metrics</h2>

</section>

<section id="binary-classifier-performance-metrics" class="slide level3 cell markdown">
<h3>Binary classifier performance metrics</h3>
<p>Suppose in our example, the output variable <span class="math inline">\(y\)</span> is constrained to be either a <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span>. The estimator is a <em>binary classifier</em>.</p>
<ul>
<li>a <span class="math inline">\(1\)</span> label is considered a <em>positive</em> label.</li>
<li>a <span class="math inline">\(0\)</span> label is considered a <em>negative</em> label.</li>
</ul>
<p><span class="math inline">\(y\)</span> is the actual outcome and <span class="math inline">\(\hat{y}\)</span> is the predicted outcome.</p>
</section>
<section id="error-types" class="slide level3 cell markdown">
<h3>Error types</h3>
<p>A binary classifier may make two types of errors:</p>
<ul>
<li>Type 1 error (also called <em>false positive</em> or <em>false alarm</em>): Outputs <span class="math inline">\(\hat{y}=1\)</span> when <span class="math inline">\(y=0\)</span>.</li>
<li>Type 2 error (also called <em>false negative</em> or <em>missed detection</em>): Output <span class="math inline">\(\hat{y}=0\)</span> when <span class="math inline">\(y=1\)</span>.</li>
</ul>
</section>
<section id="confusion-matrix" class="slide level3 cell markdown">
<h3>Confusion matrix</h3>
<p>The number of <em>true positive</em> (TP) outputs, <em>true negative</em> (TN) outputs, false positive (FP) outputs, and false negative (FN) outputs, are often presented together in a <em>confusion matrix</em>:</p>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">Real <span class="math inline">\(\downarrow\)</span> Pred. <span class="math inline">\(\rightarrow\)</span></th>
<th style="text-align: left;">1</th>
<th style="text-align: left;">0</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">1</td>
<td style="text-align: left;">TP</td>
<td style="text-align: left;">FN</td>
</tr>
<tr class="even">
<td style="text-align: left;">0</td>
<td style="text-align: left;">FP</td>
<td style="text-align: left;">TN</td>
</tr>
</tbody>
</table>
<p><span class="math inline">\(P = TP+FN\)</span>, <span class="math inline">\(N=FP+TN\)</span></p>
</section>
<section id="accuracy" class="slide level3 cell markdown">
<h3>Accuracy</h3>
<p>A simple performance metric, <em>accuracy</em>, is defined as</p>
<p><span class="math display">\[ \frac{TP + TN}{TP + FP + TN + FN}\]</span></p>
<p>i.e., the portion of samples classified correctly.</p>
</section>
<section id="balanced-accuracy" class="slide level3 cell markdown">
<h3>Balanced accuracy</h3>
<p>With imbalanced classes (<span class="math inline">\(P &gt;&gt; N\)</span> or <span class="math inline">\(P &lt;&lt; N\)</span>), we get good accuracy by “predicting” all <span class="math inline">\(1\)</span> or all <span class="math inline">\(0\)</span>!</p>
<p>Balanced accuracy is more appropriate for highly imbalanced classes -</p>
<p><span class="math display">\[ \frac{1}{2} \left( \frac{TP}{P} + \frac{TN}{N} \right) \]</span></p>
<p>gives the proportion of correct predictions in each class, averaged across classes.</p>
</section>
<section id="more-binary-classifier-metrics-1" class="slide level3 cell markdown">
<h3>More binary classifier metrics (1)</h3>
<ul>
<li><em>True Positive Rate (TPR)</em> also called <em>recall</em> or <em>sensitivity</em>:</li>
</ul>
<p><span class="math display">\[ TPR = \frac{TP}{P} = \frac{TP}{TP + FN} = P(\hat{y}=1 | y = 1)\]</span></p>
<ul>
<li><em>True Negative Rate (TNR)</em> also called <em>specificity</em>:</li>
</ul>
<p><span class="math display">\[ TNR = \frac{TN}{N} = \frac{TN}{FP + TN} = P(\hat{y}=0 | y = 0)\]</span></p>
</section>
<section id="more-binary-classifier-metrics-2" class="slide level3 cell markdown">
<h3>More binary classifier metrics (2)</h3>
<ul>
<li><em>Positive Predictive Value (PPV)</em> also called <em>precision</em>:</li>
</ul>
<p><span class="math display">\[ PPV = \frac{TP}{TP + FP} = P(y=1 | \hat{y} = 1)\]</span></p>
<ul>
<li><em>Negative Predictive Value (NPV)</em>:</li>
</ul>
<p><span class="math display">\[ NPV = \frac{TN}{TN + FN} = P(y=0 | \hat{y} = 0)\]</span></p>
</section>
<section id="more-binary-classifier-metrics-3" class="slide level3 cell markdown">
<h3>More binary classifier metrics (3)</h3>
<ul>
<li><em>False Positive Rate (FPR)</em>:</li>
</ul>
<p><span class="math display">\[ FPR = \frac{FP}{N} = \frac{FP}{FP+TN} = 1 - TNR = P(\hat{y}=1 | y = 0)\]</span></p>
<ul>
<li><em>False Discovery Rate (FDR)</em>:</li>
</ul>
<p><span class="math display">\[ FDR = \frac{FP}{FP+TP} = 1 - PPV = P(y = 0 | \hat{y} = 1)\]</span></p>
</section>
<section id="more-binary-classifier-metrics-4" class="slide level3 cell markdown">
<h3>More binary classifier metrics (4)</h3>
<ul>
<li><em>False Negative Rate (FNR)</em>:</li>
</ul>
<p><span class="math display">\[ FNR = \frac{FN}{FN+TP}  = 1 - TPR = P(\hat{y}=0 | y = 1)\]</span></p>
<ul>
<li><em>False Omission Rate (FOR)</em>:</li>
</ul>
<p><span class="math display">\[ FOR = \frac{FN}{FN+TN}  = 1 - TPR = P(y=1 | \hat{y} = 0)\]</span></p>
</section>
<section id="summary-of-binary-classifier-metrics" class="slide level3 cell markdown">
<h3>Summary of binary classifier metrics</h3>
<figure>
<img data-src="images/ConfusionMatrix.svg" alt="Selected classifier metrics" /><figcaption aria-hidden="true">Selected classifier metrics</figcaption>
</figure>
</section>
<section id="f1-score" class="slide level3 cell markdown">
<h3>F1 score</h3>
<p>Combines precision (<span class="math inline">\(\frac{TP}{TP + FP}\)</span>) and recall (<span class="math inline">\(\frac{TP}{TP + FN}\)</span>) in one metric:</p>
<p><span class="math display">\[ F_1 =  2  \left( \frac{ \textrm{precision} \times  \textrm{recall}}{\textrm{precision} + \textrm{recall}} \right) \]</span></p>
</section>
<section id="which-metric" class="slide level3 cell markdown">
<h3>Which metric?</h3>
<p>Consider</p>
<ul>
<li>class balance</li>
<li>relative cost of each kind of error</li>
</ul>
</section>
<section id="example-identifying-key-metrics" class="slide level3 cell markdown">
<h3>Example: identifying key metrics</h3>
<p>Imagine a classifier for non-invasive prenatal testing that analyzes blood samples of pregnant women, to:</p>
<ul>
<li>Identify whether the fetus is a boy or a girl.</li>
<li>Identify women that should undergo more invasive diagnostic tests for possible fetal health problems.</li>
</ul>
</section>
<section id="soft-decisions-and-thresholds" class="slide level3 cell markdown">
<h3>Soft decisions and thresholds</h3>
<p>Some classifiers give <em>soft</em> decisions:</p>
<ul>
<li><strong>Hard decision</strong>: output is either a <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span></li>
<li><strong>Soft decision</strong>: output is a probability, <span class="math inline">\(P(y=1|\mathbf{x})\)</span></li>
</ul>
<p>We get a “hard” label from a “soft” classifier by setting a threshold: <span class="math inline">\(\hat{y}=1\)</span> if we estimate <span class="math inline">\(P(y=1|\mathbf{x})&gt;t\)</span> for some threshold <span class="math inline">\(t\)</span>.</p>
</section>
<section id="soft-decisions-and-performance-metrics" class="slide level3 cell markdown">
<h3>Soft decisions and performance metrics</h3>
<p>With a threshold, we can get a confusion matrix and compute the other performance metrics - but these all depend on choice of <span class="math inline">\(t\)</span>.</p>
</section>
<aside class="notes">
<div class="cell code">
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true"></a>df <span class="op">=</span> pd.DataFrame({<span class="st">&#39;x&#39;</span>: [<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">6</span>,<span class="dv">7</span>,<span class="dv">8</span>,<span class="dv">9</span>,<span class="dv">10</span>], </span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true"></a>    <span class="st">&#39;True y&#39;</span>: [<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>], </span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true"></a>    <span class="st">&#39;Probability Estimate&#39;</span>: [<span class="fl">0.1</span>, <span class="fl">0.24</span>, <span class="fl">0.16</span>, <span class="fl">0.52</span>, <span class="fl">0.44</span>, <span class="fl">0.45</span>, <span class="fl">0.61</span>, <span class="fl">0.81</span>, <span class="fl">0.73</span>, <span class="fl">0.9</span>]})</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true"></a>sns.scatterplot(data<span class="op">=</span>df, x<span class="op">=</span><span class="st">&#39;x&#39;</span>, y<span class="op">=</span><span class="st">&#39;Probability Estimate&#39;</span>, hue<span class="op">=</span><span class="st">&#39;True y&#39;</span>)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true"></a>plt.axhline(y<span class="op">=</span><span class="fl">0.3</span>, xmin<span class="op">=</span><span class="dv">0</span>, xmax<span class="op">=</span><span class="dv">1</span>, color<span class="op">=</span><span class="st">&#39;gray&#39;</span>)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true"></a>plt.axhline(y<span class="op">=</span><span class="fl">0.5</span>, xmin<span class="op">=</span><span class="dv">0</span>, xmax<span class="op">=</span><span class="dv">1</span>, color<span class="op">=</span><span class="st">&#39;gray&#39;</span>)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true"></a>plt.axhline(y<span class="op">=</span><span class="fl">0.7</span>, xmin<span class="op">=</span><span class="dv">0</span>, xmax<span class="op">=</span><span class="dv">1</span>, color<span class="op">=</span><span class="st">&#39;gray&#39;</span>)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true"></a>plt.savefig(<span class="st">&#39;images/threshold.svg&#39;</span>)</span></code></pre></div>
</div>
</aside>
<section id="metrics-depend-on-threshold" class="slide level3 cell markdown">
<h3>Metrics depend on threshold</h3>
<figure>
<img data-src="images/threshold.svg" style="width:50.0%" alt="We could set t to maximize overall accuracy, set it higher to decrease FPR (but also decrease TPR), or set it lower to increase TPR (but also include FPR)." /><figcaption aria-hidden="true">We could set <span class="math inline">\(t\)</span> to maximize overall accuracy, set it higher to decrease FPR (but also decrease TPR), or set it lower to increase TPR (but also include FPR).</figcaption>
</figure>
</section>
<section id="roc-curve" class="slide level3 cell markdown">
<h3>ROC curve</h3>
<p>The <em>ROC curve</em> shows tradeoff between FPR and TPR for a specific <em>classifier</em> with varying <span class="math inline">\(t\)</span></p>
<ul>
<li>Each point shows the FPR and TPR of the classifier for a different value of <span class="math inline">\(t\)</span></li>
<li>Plot FPR on x-axis, TPR on y-axis</li>
</ul>
<p>(<em>ROC</em> stands for receiver operating characteristic" - the term is from radar applications.)</p>
</section>
<section id="roc-curve-example" class="slide level3 cell markdown">
<h3>ROC curve example</h3>
<figure>
<img data-src="images/roc.png" style="width:50.0%" alt="ROC curve - via bu.edu" /><figcaption aria-hidden="true">ROC curve - via bu.edu</figcaption>
</figure>
</section>
<section id="auc" class="slide level3 cell markdown">
<h3>AUC</h3>
<p><em>Area under the [ROC] curve</em> (AUC) is a performance metric for the overall classifier, independent of <span class="math inline">\(t\)</span></p>
<ul>
<li>Higher AUC is better</li>
<li>Higher AUC means for a given FPR, it has higher TPR</li>
</ul>
</section>
<section id="multi-class-classifier-performance-metrics" class="slide level3 cell markdown">
<h3>Multi-class classifier performance metrics</h3>
<p>Output variable <span class="math inline">\(y \in {1,2,\cdots,K}\)</span></p>
<ul>
<li>Accuracy: number of correct labels, divided by number of samples</li>
<li>Balanced accuracy: direct extension of two-class version</li>
<li>Other metrics: pairwise comparisons between one class and all others</li>
</ul>
<p>Soft classifier: probability for each class.</p>
</section>
<section id="multi-class-confusion-matrix" class="slide level3 cell markdown">
<h3>Multi-class confusion matrix</h3>
<figure>
<img data-src="images/multiclass.jpg" style="width:50.0%" alt="Example via Cross Validated" /><figcaption aria-hidden="true">Example via Cross Validated</figcaption>
</figure>
</section>
<section id="using-scikit-learn-to-compute-metrics" class="title-slide slide level2 cell markdown">
<h2>Using <code>scikit-learn</code> to compute metrics</h2>
<p>The <code>scikit-learn</code> library in Python includes functions to compute many performance metrics.</p>
<p>For reference, you can find these at: <a href="https://scikit-learn.org/stable/modules/classes.html#sklearn-metrics-metrics">scikit-learn metrics</a>.</p>
</section>

<section id="function-definitions" class="slide level3 cell markdown">
<h3>Function definitions</h3>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true"></a></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true"></a>sklearn.metrics.accuracy_score(y_true, y_pred, </span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true"></a>    normalize<span class="op">=</span><span class="va">True</span>, sample_weight<span class="op">=</span><span class="va">None</span>, ...)</span></code></pre></div>
</section>
<section id="function-calls" class="slide level3 cell markdown">
<h3>Function calls</h3>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true"></a><span class="im">from</span> sklearn <span class="im">import</span> metrics</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true"></a><span class="co"># assuming you have the vectors y_true and y_pred...</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true"></a>acc <span class="op">=</span> metrics.accuracy(y_true, y_pred)</span></code></pre></div>
</section>
<section id="what-causes-poor-performance" class="slide level3 cell markdown">
<h3>What causes poor performance?</h3>
<ul>
<li>Data (garbage in, garbage out)</li>
<li>Variability in observations, not explained by features</li>
<li>Incomplete coverage of the domain</li>
<li>Model error: too simple, too complicated</li>
</ul>
</section>
<section id="evaluating-models---not-just-performance" class="title-slide slide level2 cell markdown">
<h2>Evaluating models - not just performance</h2>
<ul>
<li>Cost/time for training and prediction</li>
<li>Interpretability</li>
<li>Fairness/bias</li>
</ul>
</section>

<section id="bias-in-model-output" class="slide level3 cell markdown">
<h3>Bias in model output</h3>
<p>Many potential <em>fairness</em> issues when ML models are used to make important decisions:</p>
<ul>
<li>ML used for graduate admissions</li>
<li>ML used for hiring</li>
<li>ML used to decide which patients should be admitted to hospital</li>
<li>Even ML used to decide which ads to show people…</li>
</ul>
</section>
<section id="causes-of-bias" class="slide level3 cell markdown">
<h3>Causes of bias</h3>
<ul>
<li>Models trained with less data for minority group, are less accurate for that group</li>
<li>Sampling issues: Street Bump example</li>
<li>Inherent bias in society reflected in training data, carries through to ML predictions</li>
<li>Target variable based on human judgment</li>
<li>Lack of transparency exacerbates problem!</li>
</ul>
</section>
<section id="fairness-metrics" class="title-slide slide level2 cell markdown">
<h2>Fairness metrics</h2>
<p>Suppose samples come from two groups: <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span></p>
<p>How can we tell whether the classifier treats both groups <em>fairly</em>?</p>
</section>

<section id="group-fairness" class="slide level3 cell markdown">
<h3>Group fairness</h3>
<p>(also called <em>statistical parity</em>). For groups <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>,</p>
<p><span class="math display">\[P(\hat{y}=1 | G = a) = P(\hat{y}=1 | G = b)\]</span></p>
<p>i.e. equal probability of positive classification.</p>
<p>Related: <em>Conditional statistical parity</em> (controlling for factor F)</p>
<p><span class="math display">\[P(\hat{y}=1 | G = a, F=f) = P(\hat{y}=1 | G = b, F=f)\]</span></p>
</section>
<section id="balance-for-positivenegative-class" class="slide level3 cell markdown">
<h3>Balance for positive/negative class</h3>
<p>This is similar to <em>group fairness</em>, but it is for classifiers that produce soft output - applies to every probability <span class="math inline">\(S\)</span> produced by the classifier.</p>
<p>The expected value of probability assigned by the classifier should be the same for both groups -</p>
<p>For positive class balance,</p>
<p><span class="math display">\[E(S|y=1, G=a) = E(S|y=1, G=b)\]</span></p>
<p>For negative class balance,</p>
<p><span class="math display">\[E(S|y=0, G=a) = E(S|y=0, G=b)\]</span></p>
</section>
<section id="predictive-parity" class="slide level3 cell markdown">
<h3>Predictive parity</h3>
<p>(also called <em>outcome test</em>)</p>
<p><span class="math display">\[P(y = 1 | \hat{y} = 1, G = a) = P(y = 1 | \hat{y} = 1, G = b)  \]</span></p>
<p>Groups have equal PPV. Also implies equal FDR:</p>
<p><span class="math display">\[P(y = 0 | \hat{y} = 1, G = a) = P(y = 0 | \hat{y} = 1, G = b)  \]</span></p>
<p>The prediction should carry similar meaning (w.r.t. probability of positive outcome) for both groups.</p>
</section>
<section id="calibration" class="slide level3 cell markdown">
<h3>Calibration</h3>
<p>(also called <em>test fairness</em>, <em>matching conditional frequencies</em>).</p>
<p>This is similar to <em>predictive parity</em>, but it is for classifiers that produce soft output - applies to every probability <span class="math inline">\(S\)</span> produced by the classifier.</p>
<p><span class="math display">\[P(y = 1 | S = s, G = a) = P(y = 1 | S = s, G = b) \]</span></p>
<p><em>Well-calibration</em> extends this definition to add that the probability of positive outcome should actually be <span class="math inline">\(s\)</span>:</p>
<p><span class="math display">\[P(y = 1 | S = s, G = a) = P(y = 1 | S = s, G = b) = s\]</span></p>
</section>
<section id="false-positive-error-rate-balance" class="slide level3 cell markdown">
<h3>False positive error rate balance</h3>
<p>(also called <em>predictive equality</em>)</p>
<p><span class="math display">\[P(\hat{y} = 1 | y = 0, G = a) = P(\hat{y} = 1 | y = 0, G = b)\]</span></p>
<p>Groups have equal FPR. Also implies equal TNR:</p>
<p><span class="math display">\[P(\hat{y} = 0 | y = 0, G = a) = P(\hat{y} = 0 | y = 0, G = b)\]</span></p>
</section>
<section id="false-negative-error-rate-balance" class="slide level3 cell markdown">
<h3>False negative error rate balance</h3>
<p>(also called <em>equal opportunity</em>)</p>
<p><span class="math display">\[P(\hat{y} = 0 | y = 1, G = a) = P(\hat{y} = 0 | y = 1, G = b)\]</span></p>
<p>Groups have equal FNR. Also implies equal TPR:</p>
<p><span class="math display">\[P(\hat{y} = 1 | y = 1, G = a) = P(\hat{y} = 1 | y = 1, G = b)\]</span></p>
<p>This is equivalent to group fairness <strong>only</strong> if the prevalence of positive result is the same among both groups.</p>
</section>
<section id="equalized-odds" class="slide level3 cell markdown">
<h3>Equalized odds</h3>
<p>(also called <em>disparate mistreatment</em>)</p>
<p><span class="math display">\[P(\hat{y} = 0 | y = i, G = a) = P(\hat{y} = 0 | y = i, G = b), i \in 0,1\]</span></p>
<p>Both groups should have equal TPR <em>and</em> FPR</p>
</section>
<section id="satisfying-multiple-fairness-metrics" class="slide level3 cell markdown">
<h3>Satisfying multiple fairness metrics</h3>
<p>If the prevalence of (actual) positive result <span class="math inline">\(p\)</span> is <strong>different</strong> between groups, then it is not possible to satisfy FP and FN <em>error rate balance</em> and <em>predictive parity</em> at the same time.</p>
</section>
<section id="conditional-use-accuracy-equality" class="slide level3 cell markdown">
<h3>Conditional use accuracy equality</h3>
<p>Groups have equal PPV <em>and</em> NPV</p>
<p><span class="math display">\[P(y = 1 | \hat{y} = 1, G = a) = P(y = 1 | \hat{y} = 1, G = b)\]</span></p>
<p>AND</p>
<p><span class="math display">\[P(y = 0 | \hat{y} = 0, G = a) = P(y = 0 | \hat{y} = 0, G = b)\]</span></p>
</section>
<section id="overall-accuracy-equality" class="slide level3 cell markdown">
<h3>Overall accuracy equality</h3>
<p>Groups have equal overall accuracy</p>
<p><span class="math display">\[P(\hat{y} = y | G = a) = P((\hat{y} = y | G = b)\]</span></p>
</section>
<section id="treatment-equality" class="slide level3 cell markdown">
<h3>Treatment equality</h3>
<p>Groups have equal ratio of FN to FP, <span class="math inline">\(\frac{FN}{FP}\)</span></p>
</section>
<section id="causal-discrimination" class="slide level3 cell markdown">
<h3>Causal discrimination</h3>
<p>Two samples that are identical w.r.t all features except group membership, should have same classification.</p>
</section>
<section id="fairness-through-unawareness" class="slide level3 cell markdown">
<h3>Fairness through unawareness</h3>
<ul>
<li>Features related to group membership are not used in classification.</li>
<li>Samples that are identical w.r.t all features except group membership, should have same classification.</li>
</ul>
</section>
<section id="summary---model-fairness" class="title-slide slide level2 cell markdown">
<h2>Summary - model fairness</h2>
<ul>
<li>A model can be biased with respect to age, race, gender, if those features are not used as input to the model.</li>
<li>There are many measures of fairness, sometimes it is impossible to satisfy some combination of these simultaneously.</li>
<li>People are not necessarily more fair.</li>
</ul>
</section>
    </div>
  </div>

  <script src="https://unpkg.com/reveal.js@^4//dist/reveal.js"></script>

  // reveal.js plugins
  <script src="https://unpkg.com/reveal.js@^4//plugin/notes/notes.js"></script>
  <script src="https://unpkg.com/reveal.js@^4//plugin/search/search.js"></script>
  <script src="https://unpkg.com/reveal.js@^4//plugin/zoom/zoom.js"></script>
  <script src="https://unpkg.com/reveal.js@^4//plugin/math/math.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
        // Push each slide change to the browser history
        history: true,
        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [
          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    </body>
</html>
