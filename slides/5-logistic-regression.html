<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="author" content="Fraida Fund">
  <title>Logistic Regression for Classification</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="reveal.js-master/dist/reset.css">
  <link rel="stylesheet" href="reveal.js-master/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <link rel="stylesheet" href="reveal.js-master/dist/theme/white.css" id="theme">
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <h1 class="title">Logistic Regression for Classification</h1>
  <p class="author">Fraida Fund</p>
</section>

<section id="in-this-lecture" class="title-slide slide level2">
<h2>In this lecture</h2>
<ul>
<li>Linear classifiers</li>
<li>Logistic regression</li>
<li>Fitting logistic regression</li>
<li>Naive Bayes classifier</li>
</ul>
</section>

<section id="classification" class="title-slide slide level2">
<h2>Classification</h2>
<p>Suppose we have a series of data points <span class="math inline">\(\{(\mathbf{x_1},y_1),(\mathbf{x_2},y_2),\ldots,(\mathbf{x_n},y_n)\}\)</span> and there is some (unknown) relationship between <span class="math inline">\(\mathbf{x_i}\)</span> and <span class="math inline">\(y_i\)</span>.</p>
<ul>
<li><p><strong>Classification</strong>: The output variable <span class="math inline">\(y\)</span> is constrained to be <span class="math inline">\(\in {1,2,\cdots,K}\)</span></p></li>
<li><p><strong>Binary classification</strong>: The output variable <span class="math inline">\(y\)</span> is constrained to be <span class="math inline">\(\in {0, 1}\)</span></p></li>
</ul>
</section>

<section>
<section id="linear-classifiers" class="title-slide slide level2">
<h2>Linear classifiers</h2>

</section>
<section id="binary-classification-with-linear-decision-boundary" class="slide level3">
<h3>Binary classification with linear decision boundary</h3>
<aside class="notes">
<ul>
<li>Plot training data points</li>
<li>Draw a line (<strong>decision boundary</strong>) separating 0 class and 1 class</li>
<li>If a new data point is in the <strong>decision region</strong> corresponding to class 0, then <span class="math inline">\(\hat{y} = 0\)</span>.</li>
<li>If it is in the decision region corresponding to class 1, then <span class="math inline">\(\hat{y} = 1\)</span>.</li>
</ul>
<figure>
<img data-src="../images/4-linear-classifier.png" style="width:40.0%" alt="Binary classification problem with linear decision boundary." /><figcaption aria-hidden="true">Binary classification problem with linear decision boundary.</figcaption>
</figure>
</aside>
</section>
<section id="linear-classification-rule" class="slide level3">
<h3>Linear classification rule</h3>
<ul>
<li>Given a <strong>weight vector</strong>: <span class="math inline">\(\mathbf{w} = (w_0, \cdots, w_d)\)</span></li>
<li>Compute linear combination <span class="math inline">\(z = w_0 + \sum_{j=1}^d w_d x_d\)</span></li>
<li>Predict class: <span class="math display">\[  \hat{y} = 
  \begin{cases}
  1, z &gt; 0 \\
  0, z \leq 0
  \end{cases}
 \]</span></li>
</ul>
</section>
<section id="multi-class-classification-illustration" class="slide level3">
<h3>Multi-class classification: illustration</h3>
<figure>
<img data-src="../images/hyperplane.png" style="width:50.0%" alt="Each hyperplane H_i separates the examples of C_i from the examples of all other classes." /><figcaption aria-hidden="true">Each hyperplane <span class="math inline">\(H_i\)</span> separates the examples of <span class="math inline">\(C_i\)</span> from the examples of all other classes.</figcaption>
</figure>
</section>
<section id="linear-separability" class="slide level3">
<h3>Linear separability</h3>
<p>Given training data</p>
<p><span class="math display">\[(\mathbf{x}_i, y_i), i=1,\cdots,N\]</span></p>
<p>The problem is <strong>perfectly linearly separable</strong> if there exists a <strong>separating hyperplane</strong> <span class="math inline">\(H_i\)</span> such that all <span class="math inline">\(\mathbf{x} \in C_i\)</span> lie on its positive side, and all <span class="math inline">\(\mathbf{x} \in C_j, j \neq i\)</span> lie on its negative side.</p>
</section>
<section id="non-uniqueness-of-separating-hyperplane" class="slide level3">
<h3>Non-uniqueness of separating hyperplane</h3>
<aside class="notes">
<p>When a separating hyperplane exists, it is not unique (there are in fact infinitely many such hyperplanes.)</p>
<figure>
<img data-src="../images/4-linear-classifier-non-unique.png" style="width:40.0%" alt="Several separating hyperplanes." /><figcaption aria-hidden="true">Several separating hyperplanes.</figcaption>
</figure>
</aside>
</section>
<section id="non-existence-of-perfectly-separating-hyperplane" class="slide level3">
<h3>Non-existence of perfectly separating hyperplane</h3>
<aside class="notes">
<p>Many datasets <em>not</em> linearly separable - some points will be misclassified by <em>any</em> possible hyperplane.</p>
<figure>
<img data-src="../images/4-linear-classifier-non-sep.png" style="width:40.0%" alt="This data is not separable." /><figcaption aria-hidden="true">This data is not separable.</figcaption>
</figure>
</aside>
</section>
<section id="choosing-a-hyperplane" class="slide level3">
<h3>Choosing a hyperplane</h3>
<p>Which hyperplane to choose?</p>
<p>We will try to find the hyperplane that minimizes loss according to some <strong>loss function</strong>.</p>
<p>Will revisit several times this semester.</p>
</section></section>
<section>
<section id="logistic-regression" class="title-slide slide level2">
<h2>Logistic regression</h2>

</section>
<section id="probabilistic-model-for-binary-classification" class="slide level3">
<h3>Probabilistic model for binary classification</h3>
<p>Instead of looking for a model <span class="math inline">\(f\)</span> so that</p>
<p><span class="math display">\[y_i \approx f(x_i)\]</span></p>
<p>we will look for an <span class="math inline">\(f\)</span> so that</p>
<p><span class="math display">\[ P(y_i = 1 | x_i) = f(x_i), P(y_i = 0 | x_i) = 1 - f(x_i)\]</span></p>
<aside class="notes">
<p>We need a function that takes a real value and maps it to range <span class="math inline">\([0,1]\)</span>. What function should we use?</p>
</aside>
</section>
<section id="logisticsigmoid-function" class="slide level3">
<h3>Logistic/sigmoid function</h3>
<figure>
<img data-src="../images/sigmoid.png" style="width:30.0%" alt="\sigma(z) = \frac{1}{1 + e^{-z}} is a classic “S”-shaped function." /><figcaption aria-hidden="true"><span class="math inline">\(\sigma(z) = \frac{1}{1 + e^{-z}}\)</span> is a classic “S”-shaped function.</figcaption>
</figure>
<aside class="notes">
<p>Note the intuitive relationship behind this function’s output and the distance from the linear separator (the argument that is input to the function).</p>
<figure>
<img data-src="../images/4-logistic-sigmoid-distance.png" style="width:50.0%" alt="Output is close to 0 or 1 if the argument to the \sigma has large magnitude (point is far from separating hyperplane, but closer to 0.5 if the argument is small (point is near separating hyperplane)." /><figcaption aria-hidden="true">Output is close to 0 or 1 if the argument to the <span class="math inline">\(\sigma\)</span> has large magnitude (point is far from separating hyperplane, but closer to 0.5 if the argument is small (point is near separating hyperplane).</figcaption>
</figure>
</aside>
</section>
<section id="logistic-function-for-binary-classification" class="slide level3">
<h3>Logistic function for binary classification</h3>
<p>Let <span class="math inline">\(z = w_0 + \sum_{j=1}^d w_d x_d\)</span>, then</p>
<p><span class="math display">\[ P(y=1|\mathbf{x}) = \frac{1}{1 + e^{-z}}, \quad  P(y=0|\mathbf{x}) = \frac{e^{-z}}{1 + e^{-z}} \]</span></p>
<p>(note: <span class="math inline">\(P(y=1) + P(y=0) = 1\)</span>)</p>
</section>
<section id="logistic-function-with-threshold" class="slide level3">
<h3>Logistic function with threshold</h3>
<p>Choose a threshold <span class="math inline">\(t\)</span>, then</p>
<p><span class="math display">\[ \hat{y} = 
\begin{cases}
1, \quad P(y=1|\mathbf{x}) \geq t \\
0, \quad P(y=1|\mathbf{x}) &lt; t
\end{cases}
\]</span></p>
</section>
<section id="logistic-model-as-a-soft-classifier" class="slide level3">
<h3>Logistic model as a “soft” classifier</h3>
<figure>
<img data-src="../images/sigmoid-shape.png" style="width:30.0%" alt="Plot of P(y=1|x) = \frac{1}{1+e^{-z}}, z=w_1 x. As w_1 \to \infty the logistic model becomes a “hard” rule." /><figcaption aria-hidden="true">Plot of <span class="math inline">\(P(y=1|x) = \frac{1}{1+e^{-z}}, z=w_1 x\)</span>. As <span class="math inline">\(w_1 \to \infty\)</span> the logistic model becomes a “hard” rule.</figcaption>
</figure>
</section>
<section id="logistic-classifier-properties-1" class="slide level3">
<h3>Logistic classifier properties (1)</h3>
<ul>
<li>Class probabilities depend on distance from separating hyperplane</li>
<li>Points far from separating hyperplane have probability <span class="math inline">\(\approx 0\)</span> or <span class="math inline">\(\approx 1\)</span></li>
<li>When <span class="math inline">\(|| \mathbf{w}||\)</span> is larger, class probabilities go towards extremes (0,1) more quickly</li>
</ul>
</section>
<section id="logistic-classifier-properties-2" class="slide level3">
<h3>Logistic classifier properties (2)</h3>
<ul>
<li>Unlike linear regression, weights do <em>not</em> correspond to change in output associated with one-unit change in input.</li>
<li>Sign of weight <em>does</em> tell us about relationship between a given feature and target variable.</li>
</ul>
</section>
<section id="logistic-regression---illustration" class="slide level3">
<h3>Logistic regression - illustration</h3>
<figure>
<img data-src="../images/logistic-regression-contour-plot.png" style="width:60.0%" alt="Logistic regression, illustrated with contour plot." /><figcaption aria-hidden="true">Logistic regression, illustrated with contour plot.</figcaption>
</figure>
</section>
<section id="multi-class-logistic-regression" class="slide level3">
<h3>Multi-class logistic regression</h3>
<p>Suppose <span class="math inline">\(y \in 1, \ldots, K\)</span>. We use:</p>
<ul>
<li><span class="math inline">\(\mathbf{W} \in R^{K\times d}\)</span> (parameter matrix)</li>
<li><span class="math inline">\(\mathbf{z} = \mathbf{Wx}\)</span> (<span class="math inline">\(K\)</span> linear functions)</li>
</ul>
<aside class="notes">
<p>Assume we have stacked a 1s column so that the intercept is rolled into the parameter matrix.</p>
</aside>
</section>
<section id="softmax-function" class="slide level3">
<h3>Softmax function</h3>
<p><span class="math display">\[ g_k(\mathbf{z}) = \frac{e^{z_k}}{\sum_{\ell=1}^K e^{z_\ell}}\]</span></p>
<ul>
<li>Takes as input a vector of <span class="math inline">\(K\)</span> numbers</li>
<li>Outputs <span class="math inline">\(K\)</span> probabilities proportional to the exponentials of the input numbers.</li>
</ul>
</section>
<section id="softmax-function-as-a-pmf" class="slide level3">
<h3>Softmax function as a PMF</h3>
<p>Acts like a probability mass function:</p>
<ul>
<li><span class="math inline">\(g_k(\mathbf{z}) \in [0,1]\)</span> for each <span class="math inline">\(k\)</span></li>
<li><span class="math inline">\(\sum_{k=1}^K g_k(\mathbf{z}) = 1\)</span></li>
<li>larger input corresponds to larger “probability”</li>
</ul>
</section>
<section id="softmax-function-for-multi-class-logistic-regression-1" class="slide level3">
<h3>Softmax function for multi-class logistic regression (1)</h3>
<p>Class probabilities are given by</p>
<p><span class="math display">\[P(y=k | \mathbf{x}) = \frac{e^{z_k}}{\sum_{\ell=1}^K e^{z_\ell}}\]</span></p>
</section>
<section id="softmax-function-for-multi-class-logistic-regression-2" class="slide level3">
<h3>Softmax function for multi-class logistic regression (2)</h3>
<p>When <span class="math inline">\(z_k \gg z_{\ell}\)</span> for all <span class="math inline">\(\ell \neq k\)</span>:</p>
<ul>
<li><span class="math inline">\(g_k(\mathbf{z}) \approx 1\)</span></li>
<li><span class="math inline">\(g_\ell(\mathbf{z}) \approx 0\)</span> for all <span class="math inline">\(\ell \neq k\)</span></li>
</ul>
<p>Assign highest probability to class <span class="math inline">\(k\)</span> when <span class="math inline">\(z_k\)</span> is largest.</p>
</section></section>
<section>
<section id="fitting-logistic-regression-model" class="title-slide slide level2">
<h2>Fitting logistic regression model</h2>
<aside class="notes">
<p>We know that to fit weights, we need</p>
<ul>
<li>a loss function,</li>
<li>and a training algorithm to find the weights that minimize the loss function.</li>
</ul>
</aside>
</section>
<section id="learning-logistic-model-parameters" class="slide level3">
<h3>Learning logistic model parameters</h3>
<p>Weights <span class="math inline">\(\mathbf{W}\)</span> are the unknown <strong>model parameters</strong>:</p>
<p><span class="math display">\[ \mathbf{z} = \mathbf{W x}, \mathbf{W} \in R^{K \times d}\]</span></p>
<p><span class="math display">\[ P(y=k | \mathbf{x}) = g_k(\mathbf{z}) = g_k(\mathbf{Wx})\]</span></p>
<p>Given training data <span class="math inline">\((\mathbf{x}_i, y_i), i=1,\ldots,n\)</span>, we must learn <span class="math inline">\(\mathbf{W}\)</span>.</p>
</section>
<section id="maximum-likelihood-estimation-1" class="slide level3">
<h3>Maximum likelihood estimation (1)</h3>
<p>Let <span class="math inline">\(P(\mathbf{y}| \mathbf{X}, \mathbf{W})\)</span> be the probability of observing class labels <span class="math inline">\(\mathbf{y} = (y_1, \ldots, y_n)^T\)</span></p>
<p>given inputs <span class="math inline">\(\mathbf{X} = (\mathbf{x}_1, \ldots, \mathbf{x}_n)^T\)</span> and weights <span class="math inline">\(\mathbf{W}\)</span>.</p>
<p>The <strong>maximum likelihood estimate</strong> is</p>
<p><span class="math display">\[ \mathbf{\hat{W}} = \operatorname*{argmax}_W P(\mathbf{y}| \mathbf{X}, \mathbf{W})\]</span></p>
<aside class="notes">
<p>It is the estimate of parameters for which these observations are most likely.</p>
</aside>
</section>
<section id="maximum-likelihood-estimation-2" class="slide level3">
<h3>Maximum likelihood estimation (2)</h3>
<p>Assume outputs <span class="math inline">\(y_i\)</span> are independent of one another,</p>
<p><span class="math display">\[ P(\mathbf{y}| \mathbf{X}, \mathbf{W}) = \prod_{i=1}^n P(y_i| \mathbf{x_i}, \mathbf{W})\]</span></p>
<aside class="notes">
<p>We take the log of both sides, because then the product turns into a sum…</p>
</aside>
</section>
<section id="maximum-likelihood-estimation-3" class="slide level3">
<h3>Maximum likelihood estimation (3)</h3>
<p>Define the <strong>negative log likelihood</strong>:</p>
<p><span class="math display">\[
\begin{aligned}
L(\mathbf{W}) &amp;= -\ln P(\mathbf{y}| \mathbf{X}, \mathbf{W}) \\
&amp;= - \sum_{i=1}^n \ln P(y_i| \mathbf{x_i}, \mathbf{W})
\end{aligned}
\]</span></p>
<p>(the term in the sum is also called cross-entropy)</p>
<aside class="notes">
<p>Note that maximizing the likelihood is the same as minimizing the negative log likelihood.</p>
</aside>
</section>
<section id="maximum-likelihood-estimation-4" class="slide level3">
<h3>Maximum likelihood estimation (4)</h3>
<p>Now we can re-write max likelihood estimator with a loss function to minimize:</p>
<p><span class="math display">\[ \mathbf{\hat{W}} = \operatorname*{argmax}_W P(\mathbf{y}| \mathbf{X}, \mathbf{W}) = \operatorname*{argmin}_W L(\mathbf{W})\]</span></p>
<aside class="notes">
<p>The next step will be to plug in our sigmoid function.</p>
</aside>
</section>
<section id="binary-cross-entropy-loss-1" class="slide level3">
<h3>Binary cross-entropy loss (1)</h3>
<p>For binary classification with class labels <span class="math inline">\(0, 1\)</span>:</p>
<p><span class="math display">\[\begin{equation} 
\begin{aligned}
\ln P(y_i | \mathbf{x_i}, \mathbf{w})  &amp; = y_i \ln P(y_i = 1| \mathbf{x_i}, \mathbf{w}) + (1 − y_i) \ln P(y_i = 0| \mathbf{x_i}, \mathbf{w}) \\
 &amp; = y_i \ln \sigma(z_i)  + (1 − y_i) \ln (1-\sigma(z_i)) \\
 &amp; = y_i (\ln \sigma(z_i) - \ln \sigma(-z_i)) + \ln \sigma(-z_i) \\
 &amp; = y_i \ln \frac{\sigma(z_i)}{\sigma(-z_i)} + \ln \sigma(-z_i) \\
 &amp; = y_i \ln \frac{1+e^{z_i}}{1+e^{-z_i}} + \ln \sigma(-z_i) \\
 &amp; = y_i \ln \frac{e^{z_i}(e^{-z_i}+1)}{1+e^{-z_i}} + \ln \sigma(-z_i)  \\
 &amp; =  y_i z_i - \ln (1+e^{z_i}) 
\end{aligned}
\end{equation}\]</span></p>
<p>(Note: <span class="math inline">\(\sigma(-z) = 1-\sigma(z)\)</span>)</p>
</section>
<section id="binary-cross-entropy-loss-2" class="slide level3">
<h3>Binary cross-entropy loss (2)</h3>
<p>Binary cross-entropy loss function (negative log likelihood):</p>
<p><span class="math display">\[\sum_{i=1}^n \ln (1+e^{z_i}) - y_i z_i\]</span></p>
</section>
<section id="cross-entropy-loss-for-multi-class-classification-1" class="slide level3">
<h3>Cross-entropy loss for multi-class classification (1)</h3>
<p>Define “one-hot” vector - for a sample from class <span class="math inline">\(k\)</span>, all entries in the vector are <span class="math inline">\(0\)</span> except for the <span class="math inline">\(k\)</span>th entry which is <span class="math inline">\(1\)</span>:</p>
<p><span class="math display">\[r_{ik} = 
\begin{cases}
1 \quad y_i = k \\
0 \quad y_i \neq k
\end{cases}
\]</span></p>
<p><span class="math display">\[i = 1,\ldots , n, \quad k=1, \ldots, K\]</span></p>
</section>
<section id="cross-entropy-loss-for-multi-class-classification-2" class="slide level3">
<h3>Cross-entropy loss for multi-class classification (2)</h3>
<p>Then,</p>
<p><span class="math display">\[ \ln P(y_i | \mathbf{x_i}, \mathbf{W}) = \sum_{k=1}^K r_{ik} \ln P(y_i = k| \mathbf{x_i}, \mathbf{W})\]</span></p>
<p>Cross-entropy loss function is</p>
<p><span class="math display">\[ \sum_{i=1}^n \left[ \ln \left(\sum_k e^{z_{ik}}\right) - \sum_k z_{ik} r_{ik} \right]\]</span></p>
</section>
<section id="minimizing-cross-entropy-loss" class="slide level3">
<h3>Minimizing cross-entropy loss</h3>
<p>To minimize, we would take the partial derivative:</p>
<p><span class="math display">\[ \frac{\partial L(W)}{\partial W_{kj}} = 0 \]</span></p>
<p>for all <span class="math inline">\(W_{kj}\)</span></p>
<p><strong>But</strong>, there is no closed-form expression - can only estimate weights via numerical optimization (e.g. gradient descent)</p>
</section>
<section id="transformations-bias-variance" class="slide level3">
<h3>Transformations, bias, variance</h3>
<ul>
<li>Can use basis functions to map problem to transformed feature space (if “natural” decision boundary is non-linear)</li>
<li>Variance increases with <span class="math inline">\(d\)</span> and decreases with <span class="math inline">\(n\)</span></li>
<li>Can add a regularization penalty to loss function</li>
</ul>
</section></section>
<section id="recipe-for-logistic-regression-binary-classifier" class="title-slide slide level2">
<h2>“Recipe” for logistic regression (binary classifier)</h2>
<aside class="notes">
<ul>
<li>Choose a <strong>model</strong>: <span class="math display">\[P(y = 1 | x, w) = \sigma\left(w_0 + \sum_{i=1}^d w_d x_d\right)\]</span></li>
</ul>
<p><span class="math display">\[ \hat{y} = 
\begin{cases}
1, \quad P(y=1|\mathbf{x}) \geq t \\
0, \quad P(y=1|\mathbf{x}) &lt; t
\end{cases}
\]</span></p>
<ul>
<li>Get <strong>data</strong> - for supervised learning, we need <strong>labeled</strong> examples: <span class="math inline">\((x_i, y_i), i=1,2,\cdots,n\)</span></li>
<li>Choose a <strong>loss function</strong> that will measure how well model fits data: binary cross-entropy</li>
</ul>
<p><span class="math display">\[\sum_{i=1}^n \ln (1+e^{z_i}) - y_i z_i\]</span></p>
<ul>
<li>Find model <strong>parameters</strong> that minimize loss: use numerical optimization to find weight vector <span class="math inline">\(w\)</span></li>
<li>Use model to <strong>predict</strong> <span class="math inline">\(\hat{y}\)</span> for new, unlabeled samples.</li>
</ul>
</aside>
</section>

<section id="recipe-for-logistic-regression-multi-class-classifier" class="title-slide slide level2">
<h2>“Recipe” for logistic regression (multi-class classifier)</h2>
<aside class="notes">
<ul>
<li>Choose a <strong>model</strong>: find probability of belonging to each class, then choose the class for which the probability is highest.</li>
</ul>
<p><span class="math display">\[P(y=k | \mathbf{x}) = \frac{e^{z_k}}{\sum_{\ell=1}^K e^{z_\ell}} \text{ where } \mathbf{z} = \mathbf{Wx}\]</span></p>
<ul>
<li>Get <strong>data</strong> - for supervised learning, we need <strong>labeled</strong> examples: <span class="math inline">\((x_i, y_i), i=1,2,\cdots,n\)</span></li>
<li>Choose a <strong>loss function</strong> that will measure how well model fits data: cross-entropy</li>
</ul>
<p><span class="math display">\[ \sum_{i=1}^n \left[ \ln \left(\sum_k e^{z_{ik}}\right) - \sum_k z_{ik} r_{ik} \right] \text{ where }\]</span></p>
<p><span class="math display">\[r_{ik} = 
\begin{cases}
1 \quad y_i = k \\
0 \quad y_i \neq k
\end{cases}
\]</span></p>
<ul>
<li>Find model <strong>parameters</strong> that minimize loss: use numerical optimization to find weight vector <span class="math inline">\(w\)</span></li>
<li>Use model to <strong>predict</strong> <span class="math inline">\(\hat{y}\)</span> for new, unlabeled samples.</li>
</ul>
</aside>
</section>

<section>
<section id="naive-bayes-classifier" class="title-slide slide level2">
<h2>Naive Bayes classifier</h2>
<aside class="notes">
<p>A quick look at a different type of model!</p>
</aside>
</section>
<section id="probabilistic-models-1" class="slide level3">
<h3>Probabilistic models (1)</h3>
<p>For logistic regression, minimizing the cross-entropy loss finds the parameters for which</p>
<p><span class="math inline">\(P(\mathbf{y}| \mathbf{X}, \mathbf{W})\)</span></p>
<p>is maximized.</p>
</section>
<section id="probabilistic-models-2" class="slide level3">
<h3>Probabilistic models (2)</h3>
<p>For linear regression, assuming normally distributed stochastic error, minimizing the <strong>squared error</strong> loss finds the parameters for which</p>
<p><span class="math inline">\(P(\mathbf{y}| \mathbf{X}, \mathbf{w})\)</span></p>
<p>is maximized.</p>
<aside class="notes">
<p>Surprise! We’ve been doing maximum likelihood estimation all along.</p>
</aside>
</section>
<section id="probabilistic-models-3" class="slide level3">
<h3>Probabilistic models (3)</h3>
<p>ML models that try to</p>
<ul>
<li>get a good fit for <span class="math inline">\(P(y|X)\)</span>: <strong>discriminative</strong> models.</li>
<li>fit <span class="math inline">\(P(X, y)\)</span> or <span class="math inline">\(P(X|y) P(y)\)</span>: <strong>generative</strong> models.</li>
</ul>
<aside class="notes">
<p>Linear regression and logistic regression are both considered discriminative models; they say “given that we have this data, what’s the most likely label?” (e.g. learning a mapping from an input to a target variable).</p>
<p>Generative models try to learn “what does data for each class look like” and then apply Bayes rule.</p>
</aside>
</section>
<section id="bayes-rule" class="slide level3">
<h3>Bayes rule</h3>
<p>For a sample <span class="math inline">\(\mathbf{x}_i\)</span>, <span class="math inline">\(y_k\)</span> is label of class <span class="math inline">\(k\)</span>:</p>
<p><span class="math display">\[P(y_k | \mathbf{x}_i) = \frac{P(\mathbf{x}_i|y_k) P(y_k)}{P(\mathbf{x}_i)}\]</span></p>
<aside class="notes">
<ul>
<li><span class="math inline">\(P(y_k | \mathbf{x}_i)\)</span>: posterior probability. “What is the probability that this sample belongs to class <span class="math inline">\(k\)</span>, given its observed feature values are <span class="math inline">\(\mathbf{x}_i\)</span>?”</li>
<li><span class="math inline">\(P(\mathbf{x}_i | y_k)\)</span>: conditional probability: “What is the probability of observing the feature values <span class="math inline">\(\mathbf{x}_i\)</span> in a sample, given that the sample belongs to class <span class="math inline">\(k\)</span>?”</li>
<li><span class="math inline">\(P(y_k)\)</span>: prior probability</li>
<li><span class="math inline">\(P(\mathbf{x}_i)\)</span>: evidence</li>
</ul>
</aside>
<!--
http://stanford.edu/~jurafsky/slp3/slides/7_NB.pdf
https://sebastianraschka.com/faq/docs/naive-naive-bayes.html
https://sebastianraschka.com/Articles/2014_naive_bayes_1.html
https://sebastianraschka.com/faq/docs/naive-bayes-vs-logistic-regression.html
-->
</section>
<section id="class-conditional-probability-1" class="slide level3">
<h3>Class conditional probability (1)</h3>
<p>“Naive” assumption conditional independence of features:</p>
<p><span class="math display">\[
\begin{aligned}
P(\mathbf{x}_i | y_k) &amp;= P(x_{i,1} | y_k) P(x_{i,2} | y_k) \ldots P(x_{i,d} | y_k)  \\
                    &amp;= \prod_{j=1}^d P(x_{i,j}|y_k)
\end{aligned}
\]</span></p>
<aside class="notes">
<p>This is called “naive” because this assumption is probably not true in most realistic situations.</p>
<!--
For example, given the two words “peanut” and “butter” in a text document, intuition tells us that this assumption is obviously violated: If a document contains the word “peanut” it will be more likely that it also contains the word “butter” (or “allergy”).
-->
<p>(But the classifier may still work OK!)</p>
<p>Also assumes samples are i.i.d.</p>
</aside>
</section>
<section id="class-conditional-probability-2" class="slide level3">
<h3>Class conditional probability (2)</h3>
<p>Example: for binary/categorical features, we could compute</p>
<p><span class="math display">\[\hat{P}(x_{i,j}| y_k) = \frac{N_{x_{i,j}, y_k}}{N_{y_k}}\]</span></p>
<aside class="notes">
<ul>
<li><span class="math inline">\(N_{x_{i,j}, y_k}\)</span> is the number of samples belonging to class <span class="math inline">\(k\)</span> that have feature <span class="math inline">\(j\)</span>.</li>
<li><span class="math inline">\(N_{y_k}\)</span> is the total number of samples belonging to class <span class="math inline">\(k\)</span>.</li>
</ul>
<p>Example: for cat photo classifier,</p>
<p><span class="math display">\[
\hat{P}(\mathbf{x}_i = \text{[has tail, has pointy ears, has fur, purrs when petted, likes to eat fish]}| y = \text{cat})\]</span></p>
<p><span class="math display">\[ \rightarrow P(\frac{N_{\text{tail, cat}}}{N_{\text{cat}}}) P(\frac{N_{\text{pointy ears, cat}}}{N_{\text{cat}}}) P(\frac{N_{\text{fur, cat}}}{N_{\text{cat}}}) P(\frac{N_{\text{purrs, cat}}}{N_{\text{cat}}}) P(\frac{N_{\text{eats fish, cat}}}{N_{\text{cat}}})\]</span></p>
<p><span class="math display">\[\rightarrow \frac{20}{20} \frac{18}{20} \frac{17}{20} \frac{5}{20} \frac{15}{20}\]</span></p>
</aside>
</section>
<section id="prior-probability" class="slide level3">
<h3>Prior probability</h3>
<p>Can estimate prior probability as</p>
<p><span class="math display">\[\hat{P}(y_k) = \frac{N_{y_k}}{N}\]</span></p>
<aside class="notes">
<p>Prior probabilities: probability of encountering a particular class <span class="math inline">\(k\)</span>.</p>
<p>Example: <span class="math inline">\(\frac{20}{1500}\)</span> photos are cats.</p>
</aside>
</section>
<section id="evidence" class="slide level3">
<h3>Evidence</h3>
<p>We don’t actually need <span class="math inline">\(P(\mathbf{x}_i)\)</span> to make decisions, since it is the same for every class.</p>
</section>
<section id="naive-bayes-decision-boundary" class="slide level3">
<h3>Naive bayes decision boundary</h3>
<aside class="notes">
<figure>
<img data-src="../images/5-naive-bayes-decision.png" style="width:80.0%" alt="Naive bayes decision boundary." /><figcaption aria-hidden="true">Naive bayes decision boundary.</figcaption>
</figure>
</aside>
</section>
<section id="why-generative-model" class="slide level3">
<h3>Why generative model?</h3>
<aside class="notes">
<p>The generative model solves a more general problem than the discriminative model!</p>
<p>But, only the generative model can be used to <strong>generate</strong> new samples similar to the training data.</p>
<p>Example: “generate a new sample that is probably a cat.”</p>
</aside>
</section></section>
    </div>
  </div>

  <script src="reveal.js-master/dist/reveal.js"></script>

  // reveal.js plugins
  <script src="reveal.js-master/plugin/notes/notes.js"></script>
  <script src="reveal.js-master/plugin/search/search.js"></script>
  <script src="reveal.js-master/plugin/zoom/zoom.js"></script>
  <script src="reveal.js-master/plugin/math/math.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
        // Push each slide change to the browser history
        history: true,
        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [
          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    </body>
</html>
