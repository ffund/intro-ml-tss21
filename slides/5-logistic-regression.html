<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="author" content="Fraida Fund">
  <title>Logistic Regression for Classification</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="reveal.js-master/dist/reset.css">
  <link rel="stylesheet" href="reveal.js-master/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <link rel="stylesheet" href="reveal.js-master/dist/theme/white.css" id="theme">
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <h1 class="title">Logistic Regression for Classification</h1>
  <p class="author">Fraida Fund</p>
</section>

<section id="in-this-lecture" class="title-slide slide level2">
<h2>In this lecture</h2>
<ul>
<li>Linear classifiers</li>
<li>Logistic regression</li>
<li>Fitting logistic regression</li>
</ul>
</section>

<section id="classification" class="title-slide slide level2">
<h2>Classification</h2>
<p>Suppose we have a series of data points <span class="math inline">\(\{(\mathbf{x_1},y_1),(\mathbf{x_2},y_2),\ldots,(\mathbf{x_n},y_n)\}\)</span> and there is some (unknown) relationship between <span class="math inline">\(\mathbf{x_i}\)</span> and <span class="math inline">\(y_i\)</span>.</p>
<ul>
<li><p><strong>Classification</strong>: The output variable <span class="math inline">\(y\)</span> is constrained to be <span class="math inline">\(\in {1,2,\cdots,K}\)</span></p></li>
<li><p><strong>Binary classification</strong>: The output variable <span class="math inline">\(y\)</span> is constrained to be <span class="math inline">\(\in {0, 1}\)</span></p></li>
</ul>
</section>

<section>
<section id="linear-classifiers" class="title-slide slide level2">
<h2>Linear classifiers</h2>

</section>
<section id="binary-classification-with-linear-decision-boundary" class="slide level3">
<h3>Binary classification with linear decision boundary</h3>
<aside class="notes">
<ul>
<li>Plot training data points</li>
<li>Draw a line (<strong>decision boundary</strong>) separating 0 class and 1 class</li>
<li>If a new data point is in the <strong>decision region</strong> corresponding to class 0, then <span class="math inline">\(\hat{y} = 0\)</span>.</li>
<li>If it is in the decision region corresponding to class 1, then <span class="math inline">\(\hat{y} = 1\)</span>.</li>
</ul>
<figure>
<img data-src="../images/4-linear-classifier.png" style="width:40.0%" alt="Binary classification problem with linear decision boundary." /><figcaption aria-hidden="true">Binary classification problem with linear decision boundary.</figcaption>
</figure>
</aside>
</section>
<section id="linear-classification-rule" class="slide level3">
<h3>Linear classification rule</h3>
<ul>
<li>Given a <strong>weight vector</strong>: <span class="math inline">\(\mathbf{w} = (w_0, \cdots, w_d)\)</span></li>
<li>Compute linear combination <span class="math inline">\(z = w_0 + \sum_{j=1}^d w_d x_d\)</span></li>
<li>Predict class: <span class="math display">\[  \hat{y} = 
  \begin{cases}
  1, z &gt; 0 \\
  0, z \leq 0
  \end{cases}
 \]</span></li>
</ul>
</section>
<section id="multi-class-classification-illustration" class="slide level3">
<h3>Multi-class classification: illustration</h3>
<figure>
<img data-src="../images/hyperplane.png" style="width:50.0%" alt="Each hyperplane H_i separates the examples of C_i from the examples of all other classes." /><figcaption aria-hidden="true">Each hyperplane <span class="math inline">\(H_i\)</span> separates the examples of <span class="math inline">\(C_i\)</span> from the examples of all other classes.</figcaption>
</figure>
</section>
<section id="linear-separability" class="slide level3">
<h3>Linear separability</h3>
<p>Given training data</p>
<p><span class="math display">\[(\mathbf{x}_i, y_i), i=1,\cdots,N\]</span></p>
<p>The problem is <strong>perfectly linearly separable</strong> if there exists a <strong>separating hyperplane</strong> <span class="math inline">\(H_i\)</span> such that all <span class="math inline">\(\mathbf{x} \in C_i\)</span> lie on its positive side, and all <span class="math inline">\(\mathbf{x} \in C_j, j \neq i\)</span> lie on its negative side.</p>
</section>
<section id="non-uniqueness-of-separating-hyperplane" class="slide level3">
<h3>Non-uniqueness of separating hyperplane</h3>
<aside class="notes">
<p>When a separating hyperplane exists, it is not unique (there are in fact infinitely many such hyperplanes.)</p>
<figure>
<img data-src="../images/4-linear-classifier-non-unique.png" style="width:40.0%" alt="Several separating hyperplanes." /><figcaption aria-hidden="true">Several separating hyperplanes.</figcaption>
</figure>
</aside>
</section>
<section id="non-existence-of-perfectly-separating-hyperplane" class="slide level3">
<h3>Non-existence of perfectly separating hyperplane</h3>
<aside class="notes">
<p>Many datasets <em>not</em> linearly separable - some points will be misclassified by <em>any</em> possible hyperplane.</p>
<figure>
<img data-src="../images/4-linear-classifier-non-sep.png" style="width:40.0%" alt="This data is not separable." /><figcaption aria-hidden="true">This data is not separable.</figcaption>
</figure>
</aside>
</section>
<section id="choosing-a-hyperplane" class="slide level3">
<h3>Choosing a hyperplane</h3>
<p>Which hyperplane to choose?</p>
<p>We will try to find the hyperplane that minimizes loss according to some <strong>loss function</strong>.</p>
<p>Will revisit several times this semester.</p>
</section></section>
<section>
<section id="logistic-regression" class="title-slide slide level2">
<h2>Logistic regression</h2>

</section>
<section id="probabilistic-model-for-binary-classification" class="slide level3">
<h3>Probabilistic model for binary classification</h3>
<p>Instead of looking for a model <span class="math inline">\(f\)</span> so that</p>
<p><span class="math display">\[y_i \approx f(x_i)\]</span></p>
<p>we will look for an <span class="math inline">\(f\)</span> so that</p>
<p><span class="math display">\[ P(y_i = 1 | x_i) = f(x_i), P(y_i = 0 | x_i) = 1 - f(x_i)\]</span></p>
<aside class="notes">
<p>We need a function that takes a real value and maps it to range <span class="math inline">\([0,1]\)</span>. What function should we use?</p>
</aside>
</section>
<section id="logisticsigmoid-function" class="slide level3">
<h3>Logistic/sigmoid function</h3>
<figure>
<img data-src="../images/sigmoid.png" style="width:30.0%" alt="\sigma(z) = \frac{1}{1 + e^{-z}} is a classic “S”-shaped function." /><figcaption aria-hidden="true"><span class="math inline">\(\sigma(z) = \frac{1}{1 + e^{-z}}\)</span> is a classic “S”-shaped function.</figcaption>
</figure>
<aside class="notes">
<p>Why the sigmoid/logistic function? Among its other nice properties, its derivative is <span class="math inline">\(\sigma (1-\sigma)\)</span>, which makes for an easy update rule for gradient descent.</p>
<p>Also note the intuitive relationship behind this function’s output and the distance from the linear separator (the argument that is input to the function).</p>
<figure>
<img data-src="../images/4-logistic-sigmoid-distance.png" style="width:50.0%" alt="Output is close to 0 or 1 if the argument to the \sigma has large magnitude (point is far from separating hyperplane, but closer to 0.5 if the argument is small (point is near separating hyperplane)." /><figcaption aria-hidden="true">Output is close to 0 or 1 if the argument to the <span class="math inline">\(\sigma\)</span> has large magnitude (point is far from separating hyperplane, but closer to 0.5 if the argument is small (point is near separating hyperplane).</figcaption>
</figure>
</aside>
</section>
<section id="logistic-function-for-binary-classification" class="slide level3">
<h3>Logistic function for binary classification</h3>
<p>Let <span class="math inline">\(z = w_0 + \sum_{j=1}^d w_d x_d\)</span>, then</p>
<p><span class="math display">\[ P(y=1|\mathbf{x}) = \frac{1}{1 + e^{-z}}, \quad  P(y=0|\mathbf{x}) = \frac{e^{-z}}{1 + e^{-z}} \]</span></p>
<p>(note: <span class="math inline">\(P(y=1) + P(y=0) = 1\)</span>)</p>
</section>
<section id="logistic-function-with-threshold" class="slide level3">
<h3>Logistic function with threshold</h3>
<p>Choose a threshold <span class="math inline">\(t\)</span>, then</p>
<p><span class="math display">\[ \hat{y} = 
\begin{cases}
1, \quad P(y=1|\mathbf{x}) \geq t \\
0, \quad P(y=1|\mathbf{x}) &lt; t
\end{cases}
\]</span></p>
</section>
<section id="logistic-model-as-a-soft-classifier" class="slide level3">
<h3>Logistic model as a “soft” classifier</h3>
<figure>
<img data-src="../images/sigmoid-shape.png" style="width:30.0%" alt="Plot of P(y=1|x) = \frac{1}{1+e^{-z}}, z=w_1 x. As w_1 \to \infty the logistic model becomes a “hard” rule." /><figcaption aria-hidden="true">Plot of <span class="math inline">\(P(y=1|x) = \frac{1}{1+e^{-z}}, z=w_1 x\)</span>. As <span class="math inline">\(w_1 \to \infty\)</span> the logistic model becomes a “hard” rule.</figcaption>
</figure>
</section>
<section id="logistic-classifier-properties-1" class="slide level3">
<h3>Logistic classifier properties (1)</h3>
<ul>
<li>Class probabilities depend on distance from separating hyperplane</li>
<li>Points far from separating hyperplane have probability <span class="math inline">\(\approx 0\)</span> or <span class="math inline">\(\approx 1\)</span></li>
<li>When <span class="math inline">\(|| \mathbf{w}||\)</span> is larger, class probabilities go towards extremes (0,1) more quickly</li>
</ul>
</section>
<section id="logistic-classifier-properties-2" class="slide level3">
<h3>Logistic classifier properties (2)</h3>
<ul>
<li>Unlike linear regression, weights do <em>not</em> correspond to change in output associated with one-unit change in input.</li>
<li>Sign of weight <em>does</em> tell us about relationship between a given feature and target variable.</li>
<li>Can add a regularization penalty to cost function to reduce variance, just like with linear regression.</li>
</ul>
</section>
<section id="logistic-regression---illustration" class="slide level3">
<h3>Logistic regression - illustration</h3>
<figure>
<img data-src="../images/logistic-regression-contour-plot.png" style="width:60.0%" alt="Logistic regression, illustrated with contour plot." /><figcaption aria-hidden="true">Logistic regression, illustrated with contour plot.</figcaption>
</figure>
</section>
<section id="multi-class-logistic-regression" class="slide level3">
<h3>Multi-class logistic regression</h3>
<p>Suppose <span class="math inline">\(y \in 1, \ldots, K\)</span>. We use:</p>
<ul>
<li><span class="math inline">\(\mathbf{W} \in R^{K\times d}\)</span> (parameter matrix)</li>
<li><span class="math inline">\(\mathbf{z} = \mathbf{Wx}\)</span> (<span class="math inline">\(K\)</span> linear functions)</li>
</ul>
<aside class="notes">
<p>Assume we have stacked a 1s column so that the intercept is rolled into the parameter matrix.</p>
</aside>
</section>
<section id="softmax-function" class="slide level3">
<h3>Softmax function</h3>
<p><span class="math display">\[ g_k(\mathbf{z}) = \frac{e^{z_k}}{\sum_{\ell=1}^K e^{z_\ell}}\]</span></p>
<ul>
<li>Takes as input a vector of <span class="math inline">\(K\)</span> numbers</li>
<li>Outputs <span class="math inline">\(K\)</span> probabilities proportional to the exponentials of the input numbers.</li>
</ul>
</section>
<section id="softmax-function-as-a-pmf" class="slide level3">
<h3>Softmax function as a PMF</h3>
<p>Acts like a probability mass function:</p>
<ul>
<li><span class="math inline">\(g_k(\mathbf{z}) \in [0,1]\)</span> for each <span class="math inline">\(k\)</span></li>
<li><span class="math inline">\(\sum_{k=1}^K g_k(\mathbf{z}) = 1\)</span></li>
<li>larger input corresponds to larger “probability”</li>
</ul>
</section>
<section id="softmax-function-for-multi-class-logistic-regression-1" class="slide level3">
<h3>Softmax function for multi-class logistic regression (1)</h3>
<p>Class probabilities are given by</p>
<p><span class="math display">\[P(y=k | \mathbf{x}) = \frac{e^{z_k}}{\sum_{\ell=1}^K e^{z_\ell}}\]</span></p>
</section>
<section id="softmax-function-for-multi-class-logistic-regression-2" class="slide level3">
<h3>Softmax function for multi-class logistic regression (2)</h3>
<p>When <span class="math inline">\(z_k \gg z_{\ell}\)</span> for all <span class="math inline">\(\ell \neq k\)</span>:</p>
<ul>
<li><span class="math inline">\(g_k(\mathbf{z}) \approx 1\)</span></li>
<li><span class="math inline">\(g_\ell(\mathbf{z}) \approx 0\)</span> for all <span class="math inline">\(\ell \neq k\)</span></li>
</ul>
<p>Assign highest probability to class <span class="math inline">\(k\)</span> when <span class="math inline">\(z_k\)</span> is largest.</p>
</section></section>
<section>
<section id="fitting-logistic-regression-model" class="title-slide slide level2">
<h2>Fitting logistic regression model</h2>
<aside class="notes">
<p>We know that to fit weights, we need</p>
<ul>
<li>a loss function,</li>
<li>and a training algorithm to find the weights that minimize the loss function.</li>
</ul>
</aside>
</section>
<section id="learning-logistic-model-parameters" class="slide level3">
<h3>Learning logistic model parameters</h3>
<p>Weights <span class="math inline">\(\mathbf{W}\)</span> are the unknown <strong>model parameters</strong>:</p>
<p><span class="math display">\[ \mathbf{z} = \mathbf{W x}, \mathbf{W} \in R^{K \times d}\]</span></p>
<p><span class="math display">\[ P(y=k | \mathbf{x}) = g_k(\mathbf{z}) = g_k(\mathbf{Wx})\]</span></p>
<p>Given training data <span class="math inline">\((\mathbf{x}_i, y_i), i=1,\ldots,n\)</span>, we must learn <span class="math inline">\(\mathbf{W}\)</span>.</p>
</section>
<section id="maximum-likelihood-estimation-1" class="slide level3">
<h3>Maximum likelihood estimation (1)</h3>
<p>Let <span class="math inline">\(P(\mathbf{y}| \mathbf{X}, \mathbf{W})\)</span> be the probability of observing class labels <span class="math inline">\(\mathbf{y} = (y_1, \ldots, y_n)^T\)</span></p>
<p>given inputs <span class="math inline">\(\mathbf{X} = (\mathbf{x}_1, \ldots, \mathbf{x}_n)^T\)</span> and weights <span class="math inline">\(\mathbf{W}\)</span>.</p>
<p>The <strong>maximum likelihood estimate</strong> is</p>
<p><span class="math display">\[ \mathbf{\hat{W}} = \operatorname*{argmax}_W P(\mathbf{y}| \mathbf{X}, \mathbf{W})\]</span></p>
<aside class="notes">
<p>It is the estimate of parameters for which these observations are most likely.</p>
</aside>
</section>
<section id="maximum-likelihood-estimation-2" class="slide level3">
<h3>Maximum likelihood estimation (2)</h3>
<p>Assume outputs <span class="math inline">\(y_i\)</span> are independent of one another,</p>
<p><span class="math display">\[ P(\mathbf{y}| \mathbf{X}, \mathbf{W}) = \prod_{i=1}^n P(y_i| \mathbf{x_i}, \mathbf{W})\]</span></p>
<aside class="notes">
<p>We take the log of both sides, because then the product turns into a sum…</p>
</aside>
</section>
<section id="maximum-likelihood-estimation-3" class="slide level3">
<h3>Maximum likelihood estimation (3)</h3>
<p>Define the <strong>negative log likelihood</strong>:</p>
<p><span class="math display">\[
\begin{aligned}
L(\mathbf{W}) &amp;= -\ln P(\mathbf{y}| \mathbf{X}, \mathbf{W}) \\
&amp;= - \sum_{i=1}^n \ln P(y_i| \mathbf{x_i}, \mathbf{W})
\end{aligned}
\]</span></p>
<p>(the term in the sum is also called cross-entropy)</p>
<aside class="notes">
<p>Note that maximizing the likelihood is the same as minimizing the negative log likelihood.</p>
</aside>
</section>
<section id="maximum-likelihood-estimation-4" class="slide level3">
<h3>Maximum likelihood estimation (4)</h3>
<p>Now we can re-write max likelihood estimator with a loss function to minimize:</p>
<p><span class="math display">\[ \mathbf{\hat{W}} = \operatorname*{argmax}_W P(\mathbf{y}| \mathbf{X}, \mathbf{W}) = \operatorname*{argmin}_W L(\mathbf{W})\]</span></p>
<aside class="notes">
<p>The next step will be to plug in our sigmoid function.</p>
</aside>
</section>
<section id="binary-cross-entropy-loss-1" class="slide level3">
<h3>Binary cross-entropy loss (1)</h3>
<p>For binary classification with class labels <span class="math inline">\(0, 1\)</span>:</p>
<p><span class="math display">\[\begin{equation} 
\begin{aligned}
\ln P(y_i | \mathbf{x_i}, \mathbf{w})  &amp; = y_i \ln P(y_i = 1| \mathbf{x_i}, \mathbf{w}) + (1 − y_i) \ln P(y_i = 0| \mathbf{x_i}, \mathbf{w}) \\
 &amp; = y_i \ln \sigma(z_i)  + (1 − y_i) \ln (1-\sigma(z_i)) \\
 &amp; = y_i (\ln \sigma(z_i) - \ln \sigma(-z_i)) + \ln \sigma(-z_i) \\
 &amp; = y_i \ln \frac{\sigma(z_i)}{\sigma(-z_i)} + \ln \sigma(-z_i) \\
 &amp; = y_i \ln \frac{1+e^{z_i}}{1+e^{-z_i}} + \ln \sigma(-z_i) \\
 &amp; = y_i \ln \frac{e^{z_i}(e^{-z_i}+1)}{1+e^{-z_i}} + \ln \sigma(-z_i)  \\
 &amp; =  y_i z_i - \ln (1+e^{z_i}) 
\end{aligned}
\end{equation}\]</span></p>
<p>(Note: <span class="math inline">\(\sigma(-z) = 1-\sigma(z)\)</span>)</p>
</section>
<section id="binary-cross-entropy-loss-2" class="slide level3">
<h3>Binary cross-entropy loss (2)</h3>
<p>Binary cross-entropy loss function (negative log likelihood):</p>
<p><span class="math display">\[\sum_{i=1}^n \ln (1+e^{z_i}) - y_i z_i\]</span></p>
</section>
<section id="cross-entropy-loss-for-multi-class-classification-1" class="slide level3">
<h3>Cross-entropy loss for multi-class classification (1)</h3>
<p>Define “one-hot” vector - for a sample from class <span class="math inline">\(k\)</span>, all entries in the vector are <span class="math inline">\(0\)</span> except for the <span class="math inline">\(k\)</span>th entry which is <span class="math inline">\(1\)</span>:</p>
<p><span class="math display">\[r_{ik} = 
\begin{cases}
1 \quad y_i = k \\
0 \quad y_i \neq k
\end{cases}
\]</span></p>
<p><span class="math display">\[i = 1,\ldots , n, \quad k=1, \ldots, K\]</span></p>
</section>
<section id="cross-entropy-loss-for-multi-class-classification-2" class="slide level3">
<h3>Cross-entropy loss for multi-class classification (2)</h3>
<p>Then,</p>
<p><span class="math display">\[ \ln P(y_i | \mathbf{x_i}, \mathbf{W}) = \sum_{k=1}^K r_{ik} \ln P(y_i = k| \mathbf{x_i}, \mathbf{W})\]</span></p>
<p>Cross-entropy loss function is</p>
<p><span class="math display">\[ \sum_{i=1}^n \left[ \ln \left(\sum_k e^{z_{ik}}\right) - \sum_k z_{ik} r_{ik} \right]\]</span></p>
</section>
<section id="minimizing-cross-entropy-loss" class="slide level3">
<h3>Minimizing cross-entropy loss</h3>
<p>To minimize, we would take the partial derivative:</p>
<p><span class="math display">\[ \frac{\partial L(W)}{\partial W_{kj}} = 0 \]</span></p>
<p>for all <span class="math inline">\(W_{kj}\)</span></p>
<p><strong>But</strong>, there is no closed-form expression - can only estimate weights via numerical optimization (e.g. gradient descent)</p>
</section></section>
<section id="recipe-for-logistic-regression-binary-classifier" class="title-slide slide level2">
<h2>“Recipe” for logistic regression (binary classifier)</h2>
<ul>
<li>Choose a <strong>model</strong>: <span class="math display">\[P(y = 1 | x, w) = \sigma\left(w_0 + \sum_{i=1}^d w_d x_d\right)\]</span></li>
</ul>
<p><span class="math display">\[ \hat{y} = 
\begin{cases}
1, \quad P(y=1|\mathbf{x}) \geq t \\
0, \quad P(y=1|\mathbf{x}) &lt; t
\end{cases}
\]</span></p>
<ul>
<li>Get <strong>data</strong> - for supervised learning, we need <strong>labeled</strong> examples: <span class="math inline">\((x_i, y_i), i=1,2,\cdots,n\)</span></li>
<li>Choose a <strong>loss function</strong> that will measure how well model fits data: binary cross-entropy</li>
</ul>
<p><span class="math display">\[\sum_{i=1}^n \ln (1+e^{z_i}) - y_i z_i\]</span></p>
<ul>
<li>Find model <strong>parameters</strong> that minimize loss: use numerical optimization to find weight vector <span class="math inline">\(w\)</span></li>
<li>Use model to <strong>predict</strong> <span class="math inline">\(\hat{y}\)</span> for new, unlabeled samples.</li>
</ul>
</section>

<section id="recipe-for-logistic-regression-multi-class-classifier" class="title-slide slide level2">
<h2>“Recipe” for logistic regression (multi-class classifier)</h2>
<ul>
<li>Choose a <strong>model</strong>: find probability of belonging to each class, then choose the class for which the probability is highest.</li>
</ul>
<p><span class="math display">\[P(y=k | \mathbf{x}) = \frac{e^{z_k}}{\sum_{\ell=1}^K e^{z_\ell}} \text{ where } \mathbf{z} = \mathbf{Wx}\]</span></p>
<ul>
<li>Get <strong>data</strong> - for supervised learning, we need <strong>labeled</strong> examples: <span class="math inline">\((x_i, y_i), i=1,2,\cdots,n\)</span></li>
<li>Choose a <strong>loss function</strong> that will measure how well model fits data: cross-entropy</li>
</ul>
<p><span class="math display">\[ \sum_{i=1}^n \left[ \ln \left(\sum_k e^{z_{ik}}\right) - \sum_k z_{ik} r_{ik} \right] \text{ where }\]</span></p>
<p><span class="math display">\[r_{ik} = 
\begin{cases}
1 \quad y_i = k \\
0 \quad y_i \neq k
\end{cases}
\]</span></p>
<ul>
<li>Find model <strong>parameters</strong> that minimize loss: use numerical optimization to find weight vector <span class="math inline">\(w\)</span></li>
<li>Use model to <strong>predict</strong> <span class="math inline">\(\hat{y}\)</span> for new, unlabeled samples.</li>
</ul>
</section>
    </div>
  </div>

  <script src="reveal.js-master/dist/reveal.js"></script>

  // reveal.js plugins
  <script src="reveal.js-master/plugin/notes/notes.js"></script>
  <script src="reveal.js-master/plugin/search/search.js"></script>
  <script src="reveal.js-master/plugin/zoom/zoom.js"></script>
  <script src="reveal.js-master/plugin/math/math.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
        // Push each slide change to the browser history
        history: true,
        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [
          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    </body>
</html>
