<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="author" content="Fraida Fund">
  <title>Decision trees</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="reveal.js-master/dist/reset.css">
  <link rel="stylesheet" href="reveal.js-master/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <link rel="stylesheet" href="reveal.js-master/dist/theme/white.css" id="theme">
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <h1 class="title">Decision trees</h1>
  <p class="author">Fraida Fund</p>
</section>

<section id="in-this-lecture" class="title-slide slide level2">
<h2>In this lecture</h2>
<ul>
<li>Decision trees</li>
<li>Training decision trees</li>
<li>Bias and variance of decision trees</li>
</ul>
</section>

<section>
<section id="recap" class="title-slide slide level2">
<h2>Recap</h2>

</section>
<section id="models-for-regression" class="slide level3">
<h3>Models for regression</h3>
<aside class="notes">
<table>
<colgroup>
<col style="width: 12%" />
<col style="width: 19%" />
<col style="width: 10%" />
<col style="width: 21%" />
<col style="width: 18%" />
<col style="width: 18%" />
</colgroup>
<thead>
<tr class="header">
<th>Model</th>
<th>Function shape</th>
<th>Loss fn.</th>
<th>Training</th>
<th>Prediction</th>
<th>⇩ complexity</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Linear regression</td>
<td>Linear (or transformed)</td>
<td></td>
<td></td>
<td></td>
<td>Regularization</td>
</tr>
<tr class="even">
<td>KNN</td>
<td>Arbitrarily complicated</td>
<td>NA</td>
<td>Non-parametric, store training data</td>
<td></td>
<td>Increase K</td>
</tr>
</tbody>
</table>
</aside>
</section>
<section id="models-for-classification" class="slide level3">
<h3>Models for classification</h3>
<aside class="notes">
<table>
<colgroup>
<col style="width: 11%" />
<col style="width: 18%" />
<col style="width: 11%" />
<col style="width: 21%" />
<col style="width: 18%" />
<col style="width: 18%" />
</colgroup>
<thead>
<tr class="header">
<th>Model</th>
<th>Function shape</th>
<th>Loss fn.</th>
<th>Training</th>
<th></th>
<th>⇩ complexity</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Logistic regression</td>
<td>Linear (or transformed)</td>
<td></td>
<td>No closed form soln., use solver</td>
<td></td>
<td>Regularization</td>
</tr>
<tr class="even">
<td>KNN</td>
<td>Arbitrarily complicated</td>
<td>NA</td>
<td>Non-parametric, store training data</td>
<td></td>
<td>Increase K</td>
</tr>
</tbody>
</table>
</aside>
</section>
<section id="flexible-decisions-with-cheap-prediction" class="slide level3">
<h3>Flexible decisions with cheap prediction?</h3>
<p>KNN was very flexible, but prediction is <strong>slow</strong>.</p>
<p>Next: flexible decisions, non-parametric approach, fast prediction</p>
</section></section>
<section>
<section id="decision-tree" class="title-slide slide level2">
<h2>Decision tree</h2>

</section>
<section id="tree-terminology" class="slide level3">
<h3>Tree terminology</h3>
<aside class="notes">
<p>TODO: add illustration</p>
<figure>
<img data-src="../images/tree-terminology.png" style="width:50.0%" alt="A binary tree." /><figcaption aria-hidden="true">A binary tree.</figcaption>
</figure>
</aside>
</section>
<section id="note-on-notation" class="slide level3">
<h3>Note on notation</h3>
<p>Following notation of ISLR, Chapter 8:</p>
<ul>
<li><span class="math inline">\(X_j\)</span> is feature <span class="math inline">\(j\)</span></li>
<li><span class="math inline">\(x_i\)</span> is sample <span class="math inline">\(i\)</span></li>
</ul>
</section>
<section id="stratification-of-feature-space-1" class="slide level3">
<h3>Stratification of feature space (1)</h3>
<ul>
<li>Given set of possible predictors, <span class="math inline">\(X_1, \ldots, X_p\)</span></li>
<li>Training: Divide predictor space (set of possible values of <span class="math inline">\(X\)</span>) into <span class="math inline">\(J\)</span> non-overlapping regions: <span class="math inline">\(R_1, \ldots, R_J\)</span>, by splitting sequentially on one feature at a time.</li>
</ul>
</section>
<section id="stratification-of-feature-space-2" class="slide level3">
<h3>Stratification of feature space (2)</h3>
<ul>
<li>Prediction: For each observation that falls in region <span class="math inline">\(R_j\)</span>, predict
<ul>
<li>mean of labels of training points in <span class="math inline">\(R_j\)</span> (regression)</li>
<li>mode of labels of training points in <span class="math inline">\(R_j\)</span> (classification)</li>
</ul></li>
</ul>
</section>
<section id="tree-representation" class="slide level3">
<h3>Tree representation</h3>
<ul>
<li>At node that is not a leaf: test one feature <span class="math inline">\(X_i\)</span></li>
<li>Branch from node depending on value of <span class="math inline">\(X_i\)</span></li>
<li>Each leaf node: predict <span class="math inline">\(\hat{y}_{R_m}\)</span></li>
</ul>
</section>
<section id="tree-characterization" class="slide level3">
<h3>Tree characterization</h3>
<ul>
<li>size of tree <span class="math inline">\(|T|\)</span> (number of leaf nodes)</li>
<li>depth (max length from root node to a leaf node)</li>
</ul>
</section>
<section id="stratification-of-feature-space---illustration" class="slide level3">
<h3>Stratification of feature space - illustration</h3>
<figure>
<img data-src="../images/8.3.svg" style="width:50.0%" alt="ISLR, Fig. 8.3." /><figcaption aria-hidden="true">ISLR, Fig. 8.3.</figcaption>
</figure>
<aside class="notes">
<p>The stratification on the top left cannot be produced by a decision tree using recursive binary splitting. The other three subfigures represent a single stratification.</p>
</aside>
</section></section>
<section>
<section id="training-a-decision-tree" class="title-slide slide level2">
<h2>Training a decision tree</h2>

</section>
<section id="basic-idea-1" class="slide level3">
<h3>Basic idea (1)</h3>
<ul>
<li>Goal: find the high-dimensional rectangles that minimize error</li>
<li>Computationally expensive to consider every possible partition</li>
</ul>
</section>
<section id="basic-idea-2" class="slide level3">
<h3>Basic idea (2)</h3>
<ul>
<li>Instead: recursive binary splitting (top-down, greedy approach)</li>
<li>Greedy: at each step, make the best decision at that step, without looking ahead and making a decision that might yield better results at future steps</li>
</ul>
</section>
<section id="recursive-binary-splitting" class="slide level3">
<h3>Recursive binary splitting</h3>
<p>For any feature <span class="math inline">\(j\)</span> and <em>cutpoint</em> <span class="math inline">\(s\)</span>, define the regions</p>
<p><span class="math display">\[R_1(j, s) = \{X|X_j &lt; s\}, \quad R_2(j, s) = \{X|X_j \geq s\}\]</span></p>
<p>where <span class="math inline">\(\{X|X_j &lt; s\}\)</span> is the region of predictor space in which <span class="math inline">\(X_j\)</span> takes on a value less than <span class="math inline">\(s\)</span>.</p>
</section>
<section id="recursive-binary-splitting-steps" class="slide level3">
<h3>Recursive binary splitting steps</h3>
<p>Start at root of the tree, considering all training samples.</p>
<ol type="1">
<li>At the current node,</li>
<li>Find feature <span class="math inline">\(X_j\)</span> and cutpoint <span class="math inline">\(s\)</span> that minimizes some loss function (?)</li>
<li>Split training samples at that node into two leaf nodes</li>
<li>Stop when no training error (?)</li>
<li>Otherwise, repeat at leaf nodes</li>
</ol>
</section>
<section id="loss-function-for-regression-tree" class="slide level3">
<h3>Loss function for regression tree</h3>
<p>For regression: look for feature <span class="math inline">\(j\)</span> and cutpoint <span class="math inline">\(s\)</span> that leads to the greatest possible reduction in squared error:</p>
<p><span class="math display">\[\sum_{i: x_i \in R_1(j,s)} (y_i - \hat{y}_{R_1})^2 \quad + \sum_{i: x_i \in R_2(j,s)} (y_i - \hat{y}_{R_2})^2\]</span></p>
<p>(where <span class="math inline">\(\hat{y}_{R_j}\)</span> is the prediction for the samples in <span class="math inline">\(R_j\)</span>.)</p>
</section>
<section id="loss-function-for-classification-tree" class="slide level3">
<h3>Loss function for classification tree</h3>
<p>For classification, find a split that minimizes some measure of node <em>impurity</em>:</p>
<ul>
<li>A node whose samples all belong to the same class - most <em>pure</em></li>
<li>A node whose samples are evenly distributed among all classes - highly <em>impure</em></li>
</ul>
</section>
<section id="classification-error-rate" class="slide level3">
<h3>Classification error rate</h3>
<p>For classification: one possible way is to split on <em>0-1 loss</em> or <em>misclassification rate</em>:</p>
<p><span class="math display">\[\sum_{x_i \in  R_m} 1 (y_i \neq \hat{y}_{R_m})\]</span></p>
<p>Not used often.</p>
</section>
<section id="gini-index" class="slide level3">
<h3>GINI index</h3>
<p>The GINI index is:</p>
<p><span class="math display">\[ \sum_{k=1}^K \hat{p}_{mk} (1 - \hat{p}_{mk})\]</span></p>
<p>where <span class="math inline">\(\hat{p}_{mk}\)</span> is the proportion of training samples in <span class="math inline">\(R_m\)</span> belonging to class <span class="math inline">\(k\)</span>.</p>
<aside class="notes">
<p>You can see that this is small when all values of <span class="math inline">\(\hat{p}_{mk}\)</span> are around 0 or 1.</p>
</aside>
</section>
<section id="entropy" class="slide level3">
<h3>Entropy</h3>
<aside class="notes">
<p>Entropy of a random variable <span class="math inline">\(X\)</span> (from information theory):</p>
<p><span class="math display">\[H(X) = - \sum_{i=1}^N P(X=i) \log_2 P(X=i) \]</span></p>
</aside>
<p>Entropy as a measure of impurity on subset of samples:</p>
<p><span class="math display">\[ - \sum_{k=1}^K \hat{p}_{mk} \log_2 \hat{p}_{mk}\]</span></p>
<p>where <span class="math inline">\(\hat{p}_{mk}\)</span> is the proportion of training samples in <span class="math inline">\(R_m\)</span> belonging to class <span class="math inline">\(k\)</span>.</p>
</section>
<section id="comparison---measures-of-node-impurity" class="slide level3">
<h3>Comparison - measures of node impurity</h3>
<figure>
<img data-src="../images/impurity.png" style="width:50.0%" alt="Measures of node “impurity”." /><figcaption aria-hidden="true">Measures of node “impurity”.</figcaption>
</figure>
</section>
<section id="conditional-entropy" class="slide level3">
<h3>Conditional entropy</h3>
<ul>
<li>Splitting on feature <span class="math inline">\(X\)</span> creates subsets <span class="math inline">\(S_1\)</span> and <span class="math inline">\(S_2\)</span> with different entropies</li>
<li>Conditional entropy:</li>
</ul>
<p><span class="math display">\[\text{Entropy}(S|X) = \sum_v \frac{|S_v|}{|S|} \text{Entropy}(S_v)\]</span></p>
</section>
<section id="information-gain" class="slide level3">
<h3>Information gain</h3>
<ul>
<li>Choose feature to split so as to maximize information gain, the expected reduction in entropy due to splitting on <span class="math inline">\(X\)</span>:</li>
</ul>
<p><span class="math display">\[\text{Gain}(S, X) := \text{Entropy}(S) - \text{Entropy}(S|X)\]</span></p>
</section>
<section id="example-should-i-play-tennis-1" class="slide level3">
<h3>Example: should I play tennis? (1)</h3>
<figure>
<img data-src="../images/play-tennis-dt.png" style="width:60.0%" alt="Via Tom Mitchell." /><figcaption aria-hidden="true">Via Tom Mitchell.</figcaption>
</figure>
</section>
<section id="example-should-i-play-tennis-2" class="slide level3">
<h3>Example: should I play tennis? (2)</h3>
<p>For top node: <span class="math inline">\(S = \{9+, 5-\}, |S| = 14\)</span></p>
<p><span class="math display">\[\text{Entropy}(S) = -\frac{9}{14}\log_2 \frac{9}{14} - \frac{5}{14}\log_2 \frac{5}{14} = 0.94\]</span></p>
</section>
<section id="example-should-i-play-tennis-3" class="slide level3">
<h3>Example: should I play tennis? (3)</h3>
<p>If we split on Wind:</p>
<p>Considering the Weak branch:</p>
<ul>
<li><span class="math inline">\(S_{\text{weak}} = \{6+, 2-\}, |S_{\text{weak}}| = 8\)</span></li>
<li><span class="math inline">\(\text{Entropy}(S_{\text{weak}}) = -\frac{6}{8}\log_2 (\frac{6}{8}) - \frac{2}{8}\log_2 (\frac{2}{8}) = 0.81\)</span></li>
</ul>
<p>Considering the Strong branch:</p>
<ul>
<li><span class="math inline">\(S_{\text{strong}} = \{3+, 3-\}, |S_{\text{strong}}| = 6\)</span></li>
<li><span class="math inline">\(\text{Entropy}(S_{\text{strong}}) = 1\)</span></li>
</ul>
</section>
<section id="example-should-i-play-tennis-4" class="slide level3">
<h3>Example: should I play tennis? (4)</h3>
<p><span class="math inline">\(\text{Entropy}(S) = -\frac{9}{14}\log_2 \frac{9}{14} - \frac{5}{14}\log_2 \frac{5}{14} = 0.94\)</span></p>
<p><span class="math inline">\(\text{Entropy}(S | \text{Wind}) = \frac{8}{14} \text{Entropy}(S_{\text{weak}}) + \frac{6}{14} \text{Entropy}(S_{\text{strong}}) = 0.89\)</span></p>
<p><span class="math inline">\(\text{Gain}(S, \text{Wind}) = 0.94-0.89 = 0.05\)</span></p>
</section>
<section id="example-should-i-play-tennis-5" class="slide level3">
<h3>Example: should I play tennis? (5)</h3>
<ul>
<li><span class="math inline">\(\text{Gain}(S, \text{Outlook}) = 0.246\)</span></li>
<li><span class="math inline">\(\text{Gain}(S, \text{Humidity}) = 0.151\)</span></li>
<li><span class="math inline">\(\text{Gain}(S, \text{Wind}) = 0.048\)</span></li>
<li><span class="math inline">\(\text{Gain}(S, \text{Temperature}) = 0.029\)</span></li>
</ul>
<p><span class="math inline">\(\rightarrow\)</span> Split on Outlook!</p>
</section>
<section id="feature-importance" class="slide level3">
<h3>Feature importance</h3>
<ul>
<li>For each feature <span class="math inline">\(X_j\)</span>, find all nodes where the feature was used as the split variable</li>
<li>Add up information gain due to split (or for GINI index, difference in loss weighted by number of samples.)</li>
<li>This sum reflects feature importance</li>
</ul>
</section></section>
<section>
<section id="bias-and-variance" class="title-slide slide level2">
<h2>Bias and variance</h2>

</section>
<section id="managing-tree-depth" class="slide level3">
<h3>Managing tree depth</h3>
<ul>
<li>If tree is too deep - likely to overfit (high variance)</li>
<li>If tree is not deep enough - likely to have high bias</li>
</ul>
</section>
<section id="stopping-criteria" class="slide level3">
<h3>Stopping criteria</h3>
<p>If we build tree until there is zero error on training set, we have “memorized” training data.</p>
<p>Other stopping criteria:</p>
<ul>
<li>Max depth</li>
<li>Max size (number of leaf nodes)</li>
<li>Min number of samples to split</li>
<li>Min number of samples in leaf node</li>
<li>Min decrease in loss function due to split</li>
</ul>
<p>(Can select depth, etc. by CV)</p>
</section>
<section id="pruning" class="slide level3">
<h3>Pruning</h3>
<ul>
<li><p>Alternative to stopping criteria: build entire tree, then <em>prune</em></p></li>
<li><p>With greedy algorithm - a very good split may descend from a less-good split</p></li>
</ul>
</section>
<section id="pruning-classification-trees" class="slide level3">
<h3>Pruning classification trees</h3>
<p>We usually prune classification trees using classification error rate as loss function, even if tree was built using GINI or entropy.</p>
</section>
<section id="weakest-link-pruning-1" class="slide level3">
<h3>Weakest link pruning (1)</h3>
<p>Prune a large tree from leaves to root:</p>
<ul>
<li>Start with full tree <span class="math inline">\(T_0\)</span></li>
<li>Merge two adjacent leaf nodes into their parent to obtain <span class="math inline">\(T_1\)</span> by minimizing:</li>
</ul>
<p><span class="math display">\[\frac{Err(T_1)-Err(T_0)}{|T_0| - |T_1|}\]</span></p>
</section>
<section id="weakest-link-pruning-2" class="slide level3">
<h3>Weakest link pruning (2)</h3>
<ul>
<li>Iterate to produce a sequence of trees <span class="math inline">\(T_0, T_1, \ldots, T_m\)</span> where <span class="math inline">\(T_m\)</span> is a tree of minimum size.</li>
<li>Select optimal tree by CV</li>
</ul>
</section>
<section id="cost-complexity-pruning" class="slide level3">
<h3>Cost complexity pruning</h3>
<p>Equivalent to: Minimize</p>
<p><span class="math display">\[ \sum_{m=1}^{|T|} \sum_{x_i}^{R_m} (y_i - \hat{y}_{R_m})^2 + \alpha |T|\]</span></p>
<p>Choose <span class="math inline">\(\alpha\)</span> by CV, 1-SE rule (<span class="math inline">\(\uparrow \alpha, \downarrow |T|\)</span>).</p>
</section></section>
<section>
<section id="summary---so-far" class="title-slide slide level2">
<h2>Summary - so far</h2>

</section>
<section id="the-good-and-the-bad-1" class="slide level3">
<h3>The good and the bad (1)</h3>
<p>Good:</p>
<ul>
<li>Easy to interpret, close to human decision-making</li>
<li>Can derive feature importance</li>
<li>Easily handles mixed types, different ranges</li>
<li>Can find interactions that linear classifiers can’t</li>
</ul>
</section>
<section id="the-good-and-the-bad-2" class="slide level3">
<h3>The good and the bad (2)</h3>
<p>Bad:</p>
<ul>
<li>Need deep tree to overcome bias</li>
<li>Deep trees have large variance</li>
<li>Non-robust: Small change in data can cause large change in estimated tree</li>
</ul>
</section></section>
    </div>
  </div>

  <script src="reveal.js-master/dist/reveal.js"></script>

  // reveal.js plugins
  <script src="reveal.js-master/plugin/notes/notes.js"></script>
  <script src="reveal.js-master/plugin/search/search.js"></script>
  <script src="reveal.js-master/plugin/zoom/zoom.js"></script>
  <script src="reveal.js-master/plugin/math/math.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
        // Push each slide change to the browser history
        history: true,
        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [
          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    </body>
</html>
